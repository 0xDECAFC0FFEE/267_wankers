{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Failed to connect to remote Jupyter notebook.\r\nCheck that the Jupyter Server URI setting has a valid running server specified.\r\nhttp://127.0.0.1:8888/\r\nr: request to http://127.0.0.1:8888/login? failed, reason: connect ECONNREFUSED 127.0.0.1:8888",
     "output_type": "error",
     "traceback": [
      "Error: Failed to connect to remote Jupyter notebook.",
      "Check that the Jupyter Server URI setting has a valid running server specified.",
      "http://127.0.0.1:8888/",
      "r: request to http://127.0.0.1:8888/login? failed, reason: connect ECONNREFUSED 127.0.0.1:8888",
      "at /Users/LucasTong/.vscode/extensions/ms-python.python-2020.2.64397/out/client/extension.js:1:817741",
      "at processTicksAndRejections (internal/process/task_queues.js:89:5)"
     ]
    }
   ],
   "source": [
    "# implementation of probabilistic matrix factorisation\n",
    "# very roughly based off neural net in https://towardsdatascience.com/collaborative-filtering-and-embeddings-part-2-919da17ecefb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieid_mid_lookup = get_movieid_mid_lookup(recompute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving dataset from dataset/train_ratings_binary.csv\n",
      "retrieving dataset from dataset/val_ratings_binary.csv\n",
      "retrieving dataset from dataset/test_ratings.csv\n"
     ]
    }
   ],
   "source": [
    "user_Xs, movie_Xs, ys = get_dataset(train_set, include_ys=True, recompute=False)\n",
    "user_val_Xs, movie_val_Xs, val_ys = get_dataset(val_set, include_ys=True, recompute=False)\n",
    "user_test_Xs, movie_test_Xs = get_dataset(test_set, include_ys=False, recompute=False)\n",
    "movie_genres_one_hot = get_movie_genres_one_hot(recompute=False)\n",
    "\n",
    "train_genres = get_dataset_genres(train_set, dataset_includes_ys=True, recompute=False)\n",
    "val_genres = get_dataset_genres(val_set, dataset_includes_ys=True, recompute=False)\n",
    "test_genres = get_dataset_genres(test_set, dataset_includes_ys=False, recompute=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no_genre_count = 0\n",
    "total = 0\n",
    "\n",
    "with open(test_set, newline=\"\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for rating in tqdm(reader):\n",
    "        if movieid_mid_lookup[int(float(rating[\"movieId\"]))] not in movie_genres_one_hot:\n",
    "            no_genre_count += 1\n",
    "        total += 1\n",
    "\n",
    "print(f\"{no_genre_count}/{total} entries in the test data doesn't have genre info ({no_genre_count/total}%)\")\n",
    "# all movies in test data accounted for in genre information dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # no memory - implicitly calculating user movie matrix from now on\n",
    "\n",
    "# movie_embeddings = tf.Variable(tf.random_normal([5, NUM_MOVIES], stddev=0.03, dtype=tf.float64))\n",
    "# user_embeddings = tf.Variable(tf.random_normal([NUM_USERS, 5], stddev=0.03, dtype=tf.float64))\n",
    "# movie_bias = tf.Variable(tf.random_normal([1, NUM_MOVIES], stddev=0.03, dtype=tf.float64))\n",
    "# user_bias = tf.Variable(tf.random_normal([NUM_USERS, 1], stddev=0.03, dtype=tf.float64))\n",
    "\n",
    "# user_movie_score = tf.tensordot(user_embeddings, movie_embeddings, axes = 1)+.14*tf.tile(movie_bias, [NUM_USERS, 1]) +.87*tf.tile(user_bias, [1, NUM_MOVIES])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 188117/11946576 [00:00<00:06, 1881162.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11946576/11946576 [00:05<00:00, 2204574.33it/s]\n",
      "  8%|▊         | 339324/3999236 [00:00<00:02, 1690138.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3999236/3999236 [00:01<00:00, 2072262.99it/s]\n",
      "  4%|▍         | 162860/4054451 [00:00<00:02, 1628593.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4054451/4054451 [00:01<00:00, 2060383.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projected down to 1128 dims from 1128 dimensions (reduced by 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# use pca to reduce dimensions with preserved_variance=0.96875 => dim=768 (reduction by 32%), val mse=0.031122472 on scaled data\n",
    "preserved_variance = 1\n",
    "\n",
    "train_tags = get_tags(movie_Xs, preserved_variance, recompute=False)\n",
    "val_tags = get_tags(movie_val_Xs, preserved_variance, recompute=False)\n",
    "test_tags = get_tags(movie_test_Xs, preserved_variance, recompute=False)\n",
    "\n",
    "NUM_PROJ_TAGS = train_tags[0].shape[0]\n",
    "print(f\"projected down to {NUM_PROJ_TAGS} dims from {NUM_TAGS} dimensions (reduced by {1-NUM_PROJ_TAGS/NUM_TAGS})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_embedding_rows shape (?, 40)\n",
      "(?, 40) (?, 40)\n",
      "input layer shape (?, 100)\n"
     ]
    }
   ],
   "source": [
    "# THE MODEL\n",
    "\n",
    "embedding_dim = 40\n",
    "tag_embedded_dim = 40\n",
    "\n",
    "movie_genre_embeddings = tf.placeholder(dtype=tf.float64, shape=[None, 20], name=\"movie_genre_placeholder\")\n",
    "movie_embeddings = tf.Variable(tf.contrib.layers.xavier_initializer(dtype=tf.float64)([NUM_MOVIES, embedding_dim]))\n",
    "user_embeddings = tf.Variable(tf.contrib.layers.xavier_initializer(dtype=tf.float64)([NUM_USERS, embedding_dim]))\n",
    "# movie_bias = tf.Variable(tf.random_normal([NUM_MOVIES], stddev=0.03, dtype=tf.float64))\n",
    "# user_bias = tf.Variable(tf.random_normal([NUM_USERS], stddev=0.03, dtype=tf.float64))\n",
    "\n",
    "user_slice_idxs = tf.placeholder(dtype=tf.int64, shape=[None, 1], name=\"uids\") # columns vectors to do tensor slicing\n",
    "movie_slice_idxs = tf.placeholder(dtype=tf.int64, shape=[None, 1], name=\"mids\") # columns vectors to do tensor slicing\n",
    "# user_bias_idxs = tf.placeholder(dtype=tf.int64, shape=[None, 1]) # columns vectors to do tensor slicing\n",
    "# movie_bias_idxs = tf.placeholder(dtype=tf.int64, shape=[None, 1]) # columns vectors to do tensor slicing\n",
    "\n",
    "user_embedding_columns = tf.reshape(tf.gather_nd(user_embeddings, user_slice_idxs), [-1, embedding_dim])\n",
    "movie_embedding_rows = tf.reshape(tf.gather_nd(movie_embeddings, movie_slice_idxs), [-1, embedding_dim])\n",
    "print(\"movie_embedding_rows shape\", movie_embedding_rows.shape)\n",
    "\n",
    "# user_slice_bias = tf.reshape(tf.gather_nd(user_bias, user_slice_idxs), [-1, 1])\n",
    "# movie_slice_bias = tf.reshape(tf.gather_nd(movie_bias, movie_slice_idxs), [-1, 1])\n",
    "\n",
    "# tags = tf.placeholder(dtype=tf.float64, shape=[None, NUM_PROJ_TAGS], name=\"tags_placeholder\")\n",
    "# tag_W1 = tf.Variable(tf.contrib.layers.xavier_initializer()(shape=[NUM_PROJ_TAGS, 150], dtype=tf.float64))\n",
    "# tag_b1 = tf.Variable(initial_value=np.zeros(shape=[150], dtype=np.float64))\n",
    "# tag_l1 = tf.nn.relu(tf.matmul(tags, tag_W1) + tag_b1)\n",
    "\n",
    "# tag_W2 = tf.Variable(tf.contrib.layers.xavier_initializer()(shape=[150, 40], dtype=tf.float64))\n",
    "# tag_b2 = tf.Variable(initial_value=np.zeros(shape=[40], dtype=np.float64))\n",
    "# tag_embedding = tf.nn.relu(tf.matmul(tag_l1, tag_W2) + tag_b2)\n",
    "\n",
    "mult_input = movie_embedding_rows * user_embedding_columns\n",
    "\n",
    "input_layer = tf.concat((\n",
    "#     tag_embedding,\n",
    "#     mult_input,\n",
    "    movie_embedding_rows,\n",
    "    movie_genre_embeddings,\n",
    "    user_embedding_columns,\n",
    "#     user_slice_bias,\n",
    "#     movie_slice_bias\n",
    "), axis=1)\n",
    "print(movie_embedding_rows.shape, user_embedding_columns.shape)\n",
    "print(\"input layer shape\", input_layer.shape)\n",
    "\n",
    "W1 = tf.Variable(tf.contrib.layers.xavier_initializer()(shape=[int(input_layer.shape[1]), 60], dtype=tf.float64))\n",
    "b1 = tf.Variable(initial_value=np.zeros(shape=[60], dtype=np.float64))\n",
    "l1 = tf.nn.relu(tf.matmul(input_layer, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.contrib.layers.xavier_initializer()(shape=[60, 20], dtype=tf.float64))\n",
    "b2 = tf.Variable(initial_value=np.zeros(shape=[20], dtype=np.float64))\n",
    "l2 = tf.nn.relu(tf.matmul(l1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.contrib.layers.xavier_initializer()(shape=[int(l2.shape[1]), 2], dtype=tf.float64))\n",
    "b3 = tf.Variable(initial_value=np.zeros(shape=[2], dtype=np.float64))\n",
    "l3 = tf.matmul(l2, W3) + b3\n",
    "pred_y = tf.nn.sigmoid(l3)\n",
    "\n",
    "# embedding_pred_vectors = tf.reshape(tf.reduce_sum(user_embedding_columns * tf.concat((movie_embedding_rows, movie_genre_embeddings), axis=1), axis=1), (-1, 1))\n",
    "# pred_y = embedding_pred_vectors + .14*movie_slice_bias + .87*user_slice_bias\n",
    "# print(embedding_pred_vectors.shape)\n",
    "# print(pred_y.shape)\n",
    "\n",
    "y_true = tf.placeholder(dtype=tf.float64, shape=[None, 2])\n",
    "\n",
    "all_weights = [W1, b1, W2, b2, W3, b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addbefdb738b4a9881255efcae0c1054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=150), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "training\n",
      "train loss 0.5626707439188547 l2 0.10788484704104648 mse 0.5626707439188547\n",
      "train loss 0.5624289544504717 l2 0.10787336547975494 mse 0.5624289544504717\n",
      "train loss 0.5621783268009097 l2 0.10790256007240444 mse 0.5621783268009097\n",
      "train loss 0.561966955585947 l2 0.10797017924758821 mse 0.561966955585947\n",
      "train loss 0.5617658033255887 l2 0.108064715691589 mse 0.5617658033255887\n",
      "train loss 0.5615813676383288 l2 0.1081818770525185 mse 0.5615813676383288\n",
      "train loss 0.5614022684123153 l2 0.10831577020222855 mse 0.5614022684123153\n",
      "train loss 0.5612603272336408 l2 0.10846175923906669 mse 0.5612603272336408\n",
      "train loss 0.5610484179423859 l2 0.1086221057368853 mse 0.5610484179423859\n",
      "train loss 0.5608713461595168 l2 0.10879693059667324 mse 0.5608713461595168\n",
      "train loss 0.5607350483155041 l2 0.10898367828777077 mse 0.5607350483155041\n",
      "train loss 0.5605347900574934 l2 0.10918180398835287 mse 0.5605347900574934\n",
      "train loss 0.5603435311912012 l2 0.10939049062832425 mse 0.5603435311912012\n",
      "train loss 0.5601262746132813 l2 0.10960797580354364 mse 0.5601262746132813\n",
      "train loss 0.5599370572994748 l2 0.109834691191531 mse 0.5599370572994748\n",
      "train loss 0.5596140981681711 l2 0.11007075345124014 mse 0.5596140981681711\n",
      "val loss 0.560181820548086\n",
      "val acc 0.5781706805999945 val auc 0.5997712533740766\n",
      "NEW BEST AUC: 0.5997712533740766 @ epoch 0\n",
      "NEW BEST TRAIN LOSS: 0.5596140981681711 @ epoch 0\n",
      "epoch 1\n",
      "training\n",
      "train loss 0.5593382556301354 l2 0.11031611260867391 mse 0.5593382556301354\n",
      "train loss 0.5591181403098318 l2 0.11059852040701318 mse 0.5591181403098318\n",
      "train loss 0.5589544869208948 l2 0.11090277260710092 mse 0.5589544869208948\n",
      "train loss 0.5585713078200714 l2 0.11122705004215427 mse 0.5585713078200714\n",
      "train loss 0.5583195692997102 l2 0.1115698670002266 mse 0.5583195692997102\n",
      "train loss 0.55794966259115 l2 0.11192949735200064 mse 0.55794966259115\n",
      "train loss 0.5576597944939438 l2 0.11230382642727975 mse 0.5576597944939438\n",
      "train loss 0.5573300356264769 l2 0.1126934164802084 mse 0.5573300356264769\n",
      "train loss 0.5568323453436387 l2 0.11309768001776735 mse 0.5568323453436387\n",
      "train loss 0.5563178717577213 l2 0.11351769903513058 mse 0.5563178717577213\n",
      "train loss 0.5560818916198388 l2 0.11395351510059656 mse 0.5560818916198388\n",
      "train loss 0.5554856897948568 l2 0.1144051863296345 mse 0.5554856897948568\n",
      "train loss 0.5551397281968147 l2 0.11487316064280609 mse 0.5551397281968147\n",
      "train loss 0.554513583271204 l2 0.11535729633048677 mse 0.554513583271204\n",
      "train loss 0.5540739034823927 l2 0.11585731042274557 mse 0.5540739034823927\n",
      "train loss 0.5532787535858348 l2 0.11637338192564807 mse 0.5532787535858348\n",
      "val loss 0.555424516212883\n",
      "val acc 0.598932896183171 val auc 0.635207322598758\n",
      "NEW BEST AUC: 0.635207322598758 @ epoch 1\n",
      "NEW BEST TRAIN LOSS: 0.5532787535858348 @ epoch 1\n",
      "epoch 2\n",
      "training\n",
      "train loss 0.5525968346082845 l2 0.11690577800266325 mse 0.5525968346082845\n",
      "train loss 0.5520347742907216 l2 0.11745426848092058 mse 0.5520347742907216\n",
      "train loss 0.5517102455045845 l2 0.11801983105368904 mse 0.5517102455045845\n",
      "train loss 0.5506401991813681 l2 0.11860029034186839 mse 0.5506401991813681\n",
      "train loss 0.5501350946611843 l2 0.11919698560770173 mse 0.5501350946611843\n",
      "train loss 0.5492831364199784 l2 0.11980897519744582 mse 0.5492831364199784\n",
      "train loss 0.5484377003557811 l2 0.12043623468991708 mse 0.5484377003557811\n",
      "train loss 0.5478327865050658 l2 0.12108060466580528 mse 0.5478327865050658\n",
      "train loss 0.5466176742363088 l2 0.12174102226689623 mse 0.5466176742363088\n",
      "train loss 0.5455457247144108 l2 0.12241812190067292 mse 0.5455457247144108\n",
      "train loss 0.545305317211715 l2 0.12311150588711696 mse 0.545305317211715\n",
      "train loss 0.5437130855512241 l2 0.12382039069253581 mse 0.5437130855512241\n",
      "train loss 0.543387607856463 l2 0.12454545532190306 mse 0.543387607856463\n",
      "train loss 0.5419253700805672 l2 0.12528550288655615 mse 0.5419253700805672\n",
      "train loss 0.541231702043067 l2 0.12604108282738655 mse 0.541231702043067\n",
      "train loss 0.539799352924521 l2 0.1268109260996666 mse 0.539799352924521\n",
      "val loss 0.5445049909706874\n",
      "val acc 0.6447133902575392 val auc 0.6931274390009565\n",
      "NEW BEST AUC: 0.6931274390009565 @ epoch 2\n",
      "NEW BEST TRAIN LOSS: 0.539799352924521 @ epoch 2\n",
      "epoch 3\n",
      "training\n",
      "train loss 0.5385158049762585 l2 0.12759440684655884 mse 0.5385158049762585\n",
      "train loss 0.5375565470816039 l2 0.1283886995434146 mse 0.5375565470816039\n",
      "train loss 0.5372855733121181 l2 0.12919307891568566 mse 0.5372855733121181\n",
      "train loss 0.5356534144588306 l2 0.1300031884486267 mse 0.5356534144588306\n",
      "train loss 0.5351291309446582 l2 0.1308182774559227 mse 0.5351291309446582\n",
      "train loss 0.5340249090273426 l2 0.13163315001462655 mse 0.5340249090273426\n",
      "train loss 0.5334551795858269 l2 0.13244534585337112 mse 0.5334551795858269\n",
      "train loss 0.5327992949948797 l2 0.13325094436473314 mse 0.5327992949948797\n",
      "train loss 0.5311843879821406 l2 0.13404265301918128 mse 0.5311843879821406\n",
      "train loss 0.5301289350288355 l2 0.1348219161107112 mse 0.5301289350288355\n",
      "train loss 0.531071072114018 l2 0.13558568315636976 mse 0.531071072114018\n",
      "train loss 0.5280885672650938 l2 0.13632225567122427 mse 0.5280885672650938\n",
      "train loss 0.5293189378485188 l2 0.13704025097340763 mse 0.5293189378485188\n",
      "train loss 0.5271141334929282 l2 0.13773187614421953 mse 0.5271141334929282\n",
      "train loss 0.5268830968625438 l2 0.138404168833423 mse 0.5268830968625438\n",
      "train loss 0.5256486101385868 l2 0.13905641244359876 mse 0.5256486101385868\n",
      "val loss 0.5312309580302181\n",
      "val acc 0.6810468299445194 val auc 0.7435648920588281\n",
      "NEW BEST AUC: 0.7435648920588281 @ epoch 3\n",
      "NEW BEST TRAIN LOSS: 0.5256486101385868 @ epoch 3\n",
      "epoch 4\n",
      "training\n",
      "train loss 0.5241708976918974 l2 0.13969046973289578 mse 0.5241708976918974\n",
      "train loss 0.523287626493742 l2 0.14030901190466588 mse 0.523287626493742\n",
      "train loss 0.5233436856395168 l2 0.14091808782129733 mse 0.5233436856395168\n",
      "train loss 0.5220390521286735 l2 0.1415168035816881 mse 0.5220390521286735\n",
      "train loss 0.5212485940765497 l2 0.1421109242743641 mse 0.5212485940765497\n",
      "train loss 0.5204250512276969 l2 0.1427033587515521 mse 0.5204250512276969\n",
      "train loss 0.520059844462691 l2 0.14329614574700567 mse 0.520059844462691\n",
      "train loss 0.5190120105334076 l2 0.14389290464412471 mse 0.5190120105334076\n",
      "train loss 0.5178896170818383 l2 0.14449331272220434 mse 0.5178896170818383\n",
      "train loss 0.5168451650702092 l2 0.1451009743646255 mse 0.5168451650702092\n",
      "train loss 0.5178169021475865 l2 0.14571690045043992 mse 0.5178169021475865\n",
      "train loss 0.5140546888442249 l2 0.14633569539169275 mse 0.5140546888442249\n",
      "train loss 0.516775087061408 l2 0.14696964934955967 mse 0.516775087061408\n",
      "train loss 0.5140081995690695 l2 0.14760205808371768 mse 0.5140081995690695\n",
      "train loss 0.5140367702216474 l2 0.14824031695147194 mse 0.5140367702216474\n",
      "train loss 0.5133411330985014 l2 0.14888021701635196 mse 0.5133411330985014\n",
      "val loss 0.5215945355709687\n",
      "val acc 0.7037514165205554 val auc 0.774796929238972\n",
      "NEW BEST AUC: 0.774796929238972 @ epoch 4\n",
      "NEW BEST TRAIN LOSS: 0.5133411330985014 @ epoch 4\n",
      "epoch 5\n",
      "training\n",
      "train loss 0.5125080807986142 l2 0.14951296420664492 mse 0.5125080807986142\n",
      "train loss 0.5123293396994527 l2 0.15013016454385406 mse 0.5123293396994527\n",
      "train loss 0.5131779045863906 l2 0.15072441830795955 mse 0.5131779045863906\n",
      "train loss 0.5128242972936061 l2 0.15127999950178855 mse 0.5128242972936061\n",
      "train loss 0.5117655084465743 l2 0.15179174167391882 mse 0.5117655084465743\n",
      "train loss 0.5119530636683638 l2 0.15226307828267466 mse 0.5119530636683638\n",
      "train loss 0.5126137853811912 l2 0.15268777356730182 mse 0.5126137853811912\n",
      "train loss 0.5113290827327949 l2 0.15305979863547925 mse 0.5113290827327949\n",
      "train loss 0.5114374159303334 l2 0.15338786958492218 mse 0.5114374159303334\n",
      "train loss 0.5105858017611884 l2 0.15366833377747574 mse 0.5105858017611884\n",
      "train loss 0.5117281616540853 l2 0.15390908716420054 mse 0.5117281616540853\n",
      "train loss 0.5078722549167141 l2 0.15411227239808384 mse 0.5078722549167141\n",
      "train loss 0.511720071980895 l2 0.15429513082079274 mse 0.511720071980895\n",
      "train loss 0.5089120523093261 l2 0.1544443373431045 mse 0.5089120523093261\n",
      "train loss 0.5091059922394151 l2 0.15457392524024802 mse 0.5091059922394151\n",
      "train loss 0.5088829178598514 l2 0.15469012672656451 mse 0.5088829178598514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.5194116092085188\n",
      "val acc 0.7101303849035165 val auc 0.782247938478568\n",
      "NEW BEST AUC: 0.782247938478568 @ epoch 5\n",
      "NEW BEST TRAIN LOSS: 0.5088829178598514 @ epoch 5\n",
      "epoch 6\n",
      "training\n",
      "train loss 0.5086231783922616 l2 0.15479118013558474 mse 0.5086231783922616\n",
      "train loss 0.5083458560892361 l2 0.15487638089459013 mse 0.5083458560892361\n",
      "train loss 0.5094964467825677 l2 0.15495428283231558 mse 0.5094964467825677\n",
      "train loss 0.5093512907914621 l2 0.15501914136766032 mse 0.5093512907914621\n",
      "train loss 0.5083722100816498 l2 0.15507374508758612 mse 0.5083722100816498\n",
      "train loss 0.5087740303445668 l2 0.1551263562394405 mse 0.5087740303445668\n",
      "train loss 0.5096658235119262 l2 0.1551766742170231 mse 0.5096658235119262\n",
      "train loss 0.5084601964421029 l2 0.15522467695311987 mse 0.5084601964421029\n",
      "train loss 0.5088152549903836 l2 0.15527920705881978 mse 0.5088152549903836\n",
      "train loss 0.508333888077524 l2 0.15534260820652115 mse 0.508333888077524\n",
      "train loss 0.5097152366081967 l2 0.15541675246794368 mse 0.5097152366081967\n",
      "train loss 0.5060569884041366 l2 0.15549966114350838 mse 0.5060569884041366\n",
      "train loss 0.5099414297535739 l2 0.15560035804650335 mse 0.5099414297535739\n",
      "train loss 0.5072776248392117 l2 0.15570968135602908 mse 0.5072776248392117\n",
      "train loss 0.5077647637964142 l2 0.1558333197170963 mse 0.5077647637964142\n",
      "train loss 0.5077300135931869 l2 0.15596965527770346 mse 0.5077300135931869\n",
      "val loss 0.5182597082134099\n",
      "val acc 0.7124185719472419 val auc 0.7846946193476912\n",
      "NEW BEST AUC: 0.7846946193476912 @ epoch 6\n",
      "NEW BEST TRAIN LOSS: 0.5077300135931869 @ epoch 6\n",
      "epoch 7\n",
      "training\n",
      "train loss 0.5074935006256056 l2 0.15611278172415796 mse 0.5074935006256056\n",
      "train loss 0.5071970802786769 l2 0.15625881346502954 mse 0.5071970802786769\n",
      "train loss 0.5085134920882122 l2 0.15641491548440234 mse 0.5085134920882122\n",
      "train loss 0.5086798517396054 l2 0.156573064986061 mse 0.5086798517396054\n",
      "train loss 0.5075580904963601 l2 0.15672963798036568 mse 0.5075580904963601\n",
      "train loss 0.5076585327559625 l2 0.15688389406554415 mse 0.5076585327559625\n",
      "train loss 0.5086956097454215 l2 0.15703457745900895 mse 0.5086956097454215\n",
      "train loss 0.5080564439644328 l2 0.15717762423876236 mse 0.5080564439644328\n",
      "train loss 0.5086724592183338 l2 0.15731085073345072 mse 0.5086724592183338\n",
      "train loss 0.5078086150710256 l2 0.15743219341678255 mse 0.5078086150710256\n",
      "train loss 0.5087353030917153 l2 0.1575450012395827 mse 0.5087353030917153\n",
      "train loss 0.5053459137336417 l2 0.15765125834782986 mse 0.5053459137336417\n",
      "train loss 0.5104444906692626 l2 0.15776364014122504 mse 0.5104444906692626\n",
      "train loss 0.5085672154850895 l2 0.15786827050484425 mse 0.5085672154850895\n",
      "train loss 0.5084062444061206 l2 0.15796452669017133 mse 0.5084062444061206\n",
      "train loss 0.5071685306224236 l2 0.15805280780102196 mse 0.5071685306224236\n",
      "val loss 0.518503851577754\n",
      "val acc 0.7121697744269155 val auc 0.7852608226898594\n",
      "NEW BEST AUC: 0.7852608226898594 @ epoch 7\n",
      "NEW BEST TRAIN LOSS: 0.5071685306224236 @ epoch 7\n",
      "epoch 8\n",
      "training\n",
      "train loss 0.5070224437525431 l2 0.1581395603315861 mse 0.5070224437525431\n",
      "train loss 0.5086687434454881 l2 0.15823513447266516 mse 0.5086687434454881\n",
      "train loss 0.5117289340192492 l2 0.15833618371300515 mse 0.5117289340192492\n",
      "train loss 0.511188248821756 l2 0.15842434348119086 mse 0.511188248821756\n",
      "train loss 0.5078765338776029 l2 0.15850337344064094 mse 0.5078765338776029\n",
      "train loss 0.5070911680801687 l2 0.1585924771806939 mse 0.5070911680801687\n",
      "train loss 0.50963241811615 l2 0.1586960970415785 mse 0.50963241811615\n",
      "train loss 0.5103225463688338 l2 0.15880074901810162 mse 0.5103225463688338\n",
      "train loss 0.5103280932256522 l2 0.15888554786932746 mse 0.5103280932256522\n",
      "train loss 0.507803617470777 l2 0.15895784228750368 mse 0.507803617470777\n",
      "train loss 0.5081555052424536 l2 0.15903639901252606 mse 0.5081555052424536\n",
      "train loss 0.5060736619818484 l2 0.15912679702355595 mse 0.5060736619818484\n",
      "train loss 0.5120503953862396 l2 0.15922191985172846 mse 0.5120503953862396\n",
      "train loss 0.5087696597847873 l2 0.15928711972665394 mse 0.5087696597847873\n",
      "train loss 0.5073167345091459 l2 0.15933859523363547 mse 0.5073167345091459\n",
      "train loss 0.506432642350085 l2 0.1593985809159473 mse 0.506432642350085\n",
      "val loss 0.518975681994643\n",
      "val acc 0.710618978224841 val auc 0.7848589159518162\n",
      "NEW BEST TRAIN LOSS: 0.506432642350085 @ epoch 8\n",
      "epoch 9\n",
      "training\n",
      "train loss 0.5093585897223467 l2 0.1594746297528143 mse 0.5093585897223467\n",
      "train loss 0.5118359873058905 l2 0.1595345473888779 mse 0.5118359873058905\n",
      "train loss 0.5111836441295137 l2 0.15955012818573988 mse 0.5111836441295137\n",
      "train loss 0.508943408724644 l2 0.15953918957636457 mse 0.508943408724644\n",
      "train loss 0.5065398167893624 l2 0.15952768998155006 mse 0.5065398167893624\n",
      "train loss 0.5079540899929048 l2 0.15953940091842966 mse 0.5079540899929048\n",
      "train loss 0.5127343448071651 l2 0.15958049533464053 mse 0.5127343448071651\n",
      "train loss 0.5119152350796874 l2 0.15961402560888904 mse 0.5119152350796874\n",
      "train loss 0.5113282163888326 l2 0.15964720553835088 mse 0.5113282163888326\n",
      "train loss 0.5090141633941934 l2 0.15968565082966757 mse 0.5090141633941934\n",
      "train loss 0.5080082051851813 l2 0.15974163073311334 mse 0.5080082051851813\n",
      "train loss 0.5047291682352864 l2 0.1598236594393438 mse 0.5047291682352864\n",
      "train loss 0.5091231148822825 l2 0.15992952209729347 mse 0.5091231148822825\n",
      "train loss 0.5065893128018194 l2 0.16004692648505187 mse 0.5065893128018194\n",
      "train loss 0.506967948614024 l2 0.16018404952601054 mse 0.506967948614024\n",
      "train loss 0.5062754719293302 l2 0.1603435284741817 mse 0.5062754719293302\n",
      "val loss 0.5186918312911786\n",
      "val acc 0.7112253440407118 val auc 0.7863092044212706\n",
      "NEW BEST AUC: 0.7863092044212706 @ epoch 9\n",
      "NEW BEST TRAIN LOSS: 0.5062754719293302 @ epoch 9\n",
      "epoch 10\n",
      "training\n",
      "train loss 0.5063023217102546 l2 0.16052487627877116 mse 0.5063023217102546\n",
      "train loss 0.5060705355256833 l2 0.1607195767593721 mse 0.5060705355256833\n",
      "train loss 0.5075053571144625 l2 0.16093014209502524 mse 0.5075053571144625\n",
      "train loss 0.5077045383883471 l2 0.16114493231850419 mse 0.5077045383883471\n",
      "train loss 0.5069658567232248 l2 0.1613605404026941 mse 0.5069658567232248\n",
      "train loss 0.5087126020235864 l2 0.16157105580084874 mse 0.5087126020235864\n",
      "train loss 0.5078925989675518 l2 0.16174598583317368 mse 0.5078925989675518\n",
      "train loss 0.506283766582643 l2 0.16191281896415768 mse 0.506283766582643\n",
      "train loss 0.5068849618089444 l2 0.1620798417329837 mse 0.5068849618089444\n",
      "train loss 0.5065994132058841 l2 0.16225279769777384 mse 0.5065994132058841\n",
      "train loss 0.5085392471795788 l2 0.1624356908771971 mse 0.5085392471795788\n",
      "train loss 0.5066590254603622 l2 0.1626243090830623 mse 0.5066590254603622\n",
      "train loss 0.5109342855690922 l2 0.16282272879175264 mse 0.5109342855690922\n",
      "train loss 0.5082095473811858 l2 0.1630155296021629 mse 0.5082095473811858\n",
      "train loss 0.508426742123183 l2 0.16320735822974466 mse 0.508426742123183\n",
      "train loss 0.5077070427342276 l2 0.16339272288554493 mse 0.5077070427342276\n",
      "val loss 0.5182946167999908\n",
      "val acc 0.712217783596667 val auc 0.7852794320584826\n",
      "epoch 11\n",
      "training\n",
      "train loss 0.5065103540284367 l2 0.16356541682055428 mse 0.5065103540284367\n",
      "train loss 0.5056850947037667 l2 0.16373828472111884 mse 0.5056850947037667\n",
      "train loss 0.5069978737209115 l2 0.16392153636983497 mse 0.5069978737209115\n",
      "train loss 0.5078859110452243 l2 0.1641175570904476 mse 0.5078859110452243\n",
      "train loss 0.5079391102553441 l2 0.1643326587547465 mse 0.5079391102553441\n",
      "train loss 0.5103535960972156 l2 0.16456768946467107 mse 0.5103535960972156\n",
      "train loss 0.5149142485517356 l2 0.16480686262919414 mse 0.5149142485517356\n",
      "train loss 0.513966596054747 l2 0.16502513200745333 mse 0.513966596054747\n",
      "train loss 0.5132498706215067 l2 0.1652007416152922 mse 0.5132498706215067\n",
      "train loss 0.5110291587532981 l2 0.16533154558604715 mse 0.5110291587532981\n",
      "train loss 0.5105912672682598 l2 0.16541625702228835 mse 0.5105912672682598\n",
      "train loss 0.5039971442128081 l2 0.1654574125693499 mse 0.5039971442128081\n",
      "train loss 0.5081366910202434 l2 0.1655108982587429 mse 0.5081366910202434\n",
      "train loss 0.5072598083735738 l2 0.1655941691701379 mse 0.5072598083735738\n",
      "train loss 0.5099970462289476 l2 0.1657270236136367 mse 0.5099970462289476\n",
      "train loss 0.5114962985792254 l2 0.16591251765171808 mse 0.5114962985792254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.5280819137459914\n",
      "val acc 0.6904063676162147 val auc 0.7810523997847739\n",
      "epoch 12\n",
      "training\n",
      "train loss 0.5157136396984662 l2 0.16614872241357947 mse 0.5157136396984662\n",
      "train loss 0.5193109880454759 l2 0.1664199477618444 mse 0.5193109880454759\n",
      "train loss 0.5212304052999707 l2 0.16669503603661567 mse 0.5212304052999707\n",
      "train loss 0.5207128275709054 l2 0.16694398766178312 mse 0.5207128275709054\n",
      "train loss 0.5181995527733145 l2 0.16714416601976984 mse 0.5181995527733145\n",
      "train loss 0.513687943927039 l2 0.1672877979298661 mse 0.513687943927039\n",
      "train loss 0.5084834677685833 l2 0.16739492006760093 mse 0.5084834677685833\n",
      "train loss 0.5060317347331211 l2 0.16751291352003841 mse 0.5060317347331211\n",
      "train loss 0.5082147829191834 l2 0.16766658733383782 mse 0.5082147829191834\n",
      "train loss 0.5108825825227221 l2 0.16786008288910192 mse 0.5108825825227221\n",
      "train loss 0.5162209439790926 l2 0.16808372294056886 mse 0.5162209439790926\n",
      "train loss 0.5213828690585978 l2 0.16832244985151856 mse 0.5213828690585978\n",
      "train loss 0.5316097231434619 l2 0.1685176253481546 mse 0.5316097231434619\n",
      "train loss 0.5293771871436401 l2 0.168613682718739 mse 0.5293771871436401\n",
      "train loss 0.5272488114724982 l2 0.16860583297660958 mse 0.5272488114724982\n",
      "train loss 0.5241727728596277 l2 0.1685027330767577 mse 0.5241727728596277\n",
      "val loss 0.5197926214081402\n",
      "val acc 0.7085230779078804 val auc 0.7827836890607572\n",
      "epoch 13\n",
      "training\n",
      "train loss 0.513807398993822 l2 0.1683134127053751 mse 0.513807398993822\n",
      "train loss 0.5068140693896084 l2 0.1681119739178867 mse 0.5068140693896084\n",
      "train loss 0.5069916271046245 l2 0.16796375812469674 mse 0.5069916271046245\n",
      "train loss 0.5087971896964473 l2 0.1678871835174083 mse 0.5087971896964473\n",
      "train loss 0.5114607832203351 l2 0.16788421253118616 mse 0.5114607832203351\n",
      "train loss 0.5163326149817294 l2 0.16795105328974508 mse 0.5163326149817294\n",
      "train loss 0.5211227017811552 l2 0.16806835369386036 mse 0.5211227017811552\n",
      "train loss 0.5230115741355067 l2 0.1682071234594696 mse 0.5230115741355067\n",
      "train loss 0.5250565313341191 l2 0.16835935232643656 mse 0.5250565313341191\n",
      "train loss 0.5252544348903962 l2 0.16850750188122254 mse 0.5252544348903962\n",
      "train loss 0.5250349844629012 l2 0.16863889865278597 mse 0.5250349844629012\n",
      "train loss 0.5135126534305692 l2 0.16874701405337833 mse 0.5135126534305692\n",
      "train loss 0.5116687926878277 l2 0.16888789667835824 mse 0.5116687926878277\n",
      "train loss 0.5063632456508371 l2 0.16906923720740338 mse 0.5063632456508371\n",
      "train loss 0.5057583019633405 l2 0.1692960659476398 mse 0.5057583019633405\n",
      "train loss 0.5063370897798893 l2 0.16955770949278676 mse 0.5063370897798893\n",
      "val loss 0.5182565643551634\n",
      "val acc 0.7119702363151362 val auc 0.786568805721712\n",
      "NEW BEST AUC: 0.786568805721712 @ epoch 13\n",
      "epoch 14\n",
      "training\n",
      "train loss 0.5074220300876616 l2 0.16983566507604428 mse 0.5074220300876616\n",
      "train loss 0.5087747766342023 l2 0.17011995695399829 mse 0.5087747766342023\n",
      "train loss 0.5115328658322433 l2 0.1704025500169026 mse 0.5115328658322433\n",
      "train loss 0.5125672749268996 l2 0.17067116661800727 mse 0.5125672749268996\n",
      "train loss 0.5115448689905688 l2 0.17092014726262103 mse 0.5115448689905688\n",
      "train loss 0.5104314727395407 l2 0.17115140992785732 mse 0.5104314727395407\n",
      "train loss 0.5093615468626225 l2 0.1713742998488927 mse 0.5093615468626225\n",
      "train loss 0.5066564941100368 l2 0.17160995968552709 mse 0.5066564941100368\n",
      "train loss 0.5062350417419107 l2 0.17186644019975775 mse 0.5062350417419107\n",
      "train loss 0.5054626620838513 l2 0.17214731433946712 mse 0.5054626620838513\n",
      "train loss 0.506952287006142 l2 0.172449831478902 mse 0.506952287006142\n",
      "train loss 0.5034195868567072 l2 0.1727665307940957 mse 0.5034195868567072\n",
      "train loss 0.5072636932884891 l2 0.17310321054942968 mse 0.5072636932884891\n",
      "train loss 0.5046698532505639 l2 0.17345058596013657 mse 0.5046698532505639\n",
      "train loss 0.5048423336492593 l2 0.17381152351024998 mse 0.5048423336492593\n",
      "train loss 0.5048126952563257 l2 0.17418327835759376 mse 0.5048126952563257\n",
      "val loss 0.5167832613700413\n",
      "val acc 0.7147555183039961 val auc 0.7883872678174129\n",
      "NEW BEST AUC: 0.7883872678174129 @ epoch 14\n",
      "NEW BEST TRAIN LOSS: 0.5048126952563257 @ epoch 14\n",
      "epoch 15\n",
      "training\n",
      "train loss 0.5048312595671853 l2 0.174559929657821 mse 0.5048312595671853\n",
      "train loss 0.5044991748455345 l2 0.17493740113069514 mse 0.5044991748455345\n",
      "train loss 0.5056513207110327 l2 0.1753152324631904 mse 0.5056513207110327\n",
      "train loss 0.505632797937011 l2 0.1756856443157366 mse 0.505632797937011\n",
      "train loss 0.5046384668078957 l2 0.1760463009807787 mse 0.5046384668078957\n",
      "train loss 0.505010191996378 l2 0.17639586750412228 mse 0.505010191996378\n",
      "train loss 0.5059795371215141 l2 0.17673385082501497 mse 0.5059795371215141\n",
      "train loss 0.5049352055535823 l2 0.1770629658699114 mse 0.5049352055535823\n",
      "train loss 0.5053100874168274 l2 0.1773836209411894 mse 0.5053100874168274\n",
      "train loss 0.5047815980998329 l2 0.17769805487289553 mse 0.5047815980998329\n",
      "train loss 0.5060494264000053 l2 0.17800495042656822 mse 0.5060494264000053\n",
      "train loss 0.5024386857978554 l2 0.1782997806117944 mse 0.5024386857978554\n",
      "train loss 0.5064649576919876 l2 0.17859054018138673 mse 0.5064649576919876\n",
      "train loss 0.5038375049317535 l2 0.17887095763393798 mse 0.5038375049317535\n",
      "train loss 0.5040298198146672 l2 0.17914699611748966 mse 0.5040298198146672\n",
      "train loss 0.5040245125906777 l2 0.17941913205845791 mse 0.5040245125906777\n",
      "val loss 0.5168465119159035\n",
      "val acc 0.715333378675327 val auc 0.7893093315619297\n",
      "NEW BEST AUC: 0.7893093315619297 @ epoch 15\n",
      "NEW BEST TRAIN LOSS: 0.5040245125906777 @ epoch 15\n",
      "epoch 16\n",
      "training\n",
      "train loss 0.5040402454315245 l2 0.1796844427585905 mse 0.5040402454315245\n",
      "train loss 0.5038064132424613 l2 0.1799437830065531 mse 0.5038064132424613\n",
      "train loss 0.5050839807920404 l2 0.180199388918011 mse 0.5050839807920404\n",
      "train loss 0.5051054846578356 l2 0.18044649961328885 mse 0.5051054846578356\n",
      "train loss 0.5041002102639334 l2 0.18068599010407596 mse 0.5041002102639334\n",
      "train loss 0.5044803034589902 l2 0.18091954428640578 mse 0.5044803034589902\n",
      "train loss 0.5054575484116748 l2 0.18114773167407117 mse 0.5054575484116748\n",
      "train loss 0.5043952551702507 l2 0.1813738923157211 mse 0.5043952551702507\n",
      "train loss 0.504626838308712 l2 0.18159746837682575 mse 0.504626838308712\n",
      "train loss 0.5040065907108947 l2 0.18182345389862017 mse 0.5040065907108947\n",
      "train loss 0.5051915771083407 l2 0.18205267879038825 mse 0.5051915771083407\n",
      "train loss 0.5016396788172336 l2 0.18228259221016158 mse 0.5016396788172336\n",
      "train loss 0.5057435090470023 l2 0.18252046828773497 mse 0.5057435090470023\n",
      "train loss 0.503180490149618 l2 0.1827606105348721 mse 0.503180490149618\n",
      "train loss 0.503390893509122 l2 0.1830087266915735 mse 0.503390893509122\n",
      "train loss 0.5033971362559616 l2 0.18326585692404856 mse 0.5033971362559616\n",
      "val loss 0.5163579842081909\n",
      "val acc 0.7158172210892281 val auc 0.7901235977959363\n",
      "NEW BEST AUC: 0.7901235977959363 @ epoch 16\n",
      "NEW BEST TRAIN LOSS: 0.5033971362559616 @ epoch 16\n",
      "epoch 17\n",
      "training\n",
      "train loss 0.5034151056671866 l2 0.1835295486702982 mse 0.5034151056671866\n",
      "train loss 0.5031640723688162 l2 0.1838016864679077 mse 0.5031640723688162\n",
      "train loss 0.5043836722734983 l2 0.18408415032177966 mse 0.5043836722734983\n",
      "train loss 0.5043675300639509 l2 0.18437193909153937 mse 0.5043675300639509\n",
      "train loss 0.5033119003361217 l2 0.18466481283838618 mse 0.5033119003361217\n",
      "train loss 0.5036378147610612 l2 0.1849641641806781 mse 0.5036378147610612\n",
      "train loss 0.5046436008207397 l2 0.18526956662045752 mse 0.5046436008207397\n",
      "train loss 0.5036460673802249 l2 0.1855794044509892 mse 0.5036460673802249\n",
      "train loss 0.5038632531094841 l2 0.1858913541787088 mse 0.5038632531094841\n",
      "train loss 0.5032591522801962 l2 0.18620890248059532 mse 0.5032591522801962\n",
      "train loss 0.5044422366176369 l2 0.1865316423480278 mse 0.5044422366176369\n",
      "train loss 0.5009263313032523 l2 0.1868561255653263 mse 0.5009263313032523\n",
      "train loss 0.505019318826067 l2 0.18718735485391144 mse 0.505019318826067\n",
      "train loss 0.5024823713721354 l2 0.18751916148998937 mse 0.5024823713721354\n",
      "train loss 0.5026665648910775 l2 0.18785529626916903 mse 0.5026665648910775\n",
      "train loss 0.5026304858535188 l2 0.1881963133774504 mse 0.5026304858535188\n",
      "val loss 0.5160495810918934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 0.7164865989403976 val auc 0.791182344821008\n",
      "NEW BEST AUC: 0.791182344821008 @ epoch 17\n",
      "NEW BEST TRAIN LOSS: 0.5026304858535188 @ epoch 17\n",
      "epoch 18\n",
      "training\n",
      "train loss 0.5026164474044811 l2 0.1885393346416888 mse 0.5026164474044811\n",
      "train loss 0.5023464927959197 l2 0.1888865921702514 mse 0.5023464927959197\n",
      "train loss 0.5035530420523615 l2 0.18924035751708312 mse 0.5035530420523615\n",
      "train loss 0.5035495653882316 l2 0.18959711787967165 mse 0.5035495653882316\n",
      "train loss 0.5025048153063101 l2 0.1899569063572295 mse 0.5025048153063101\n",
      "train loss 0.5028341449683388 l2 0.1903216002306104 mse 0.5028341449683388\n",
      "train loss 0.5038648766524597 l2 0.19069081046498884 mse 0.5038648766524597\n",
      "train loss 0.5029146050154093 l2 0.19106218775504671 mse 0.5029146050154093\n",
      "train loss 0.5030795610883948 l2 0.19143270859379757 mse 0.5030795610883948\n",
      "train loss 0.5024541827220496 l2 0.19180626663280909 mse 0.5024541827220496\n",
      "train loss 0.5036030155317173 l2 0.19218269846911618 mse 0.5036030155317173\n",
      "train loss 0.5000834445827407 l2 0.19255926974153845 mse 0.5000834445827407\n",
      "train loss 0.5041704963481172 l2 0.1929405973452149 mse 0.5041704963481172\n",
      "train loss 0.5016452516670025 l2 0.19332156458661798 mse 0.5016452516670025\n",
      "train loss 0.5018230257685645 l2 0.1937051846894808 mse 0.5018230257685645\n",
      "train loss 0.5017924455255618 l2 0.19409182336403563 mse 0.5017924455255618\n",
      "val loss 0.5157702770601128\n",
      "val acc 0.7168954270265621 val auc 0.7921291401288801\n",
      "NEW BEST AUC: 0.7921291401288801 @ epoch 18\n",
      "NEW BEST TRAIN LOSS: 0.5017924455255618 @ epoch 18\n",
      "epoch 19\n",
      "training\n",
      "train loss 0.501773528109666 l2 0.19447828721462282 mse 0.501773528109666\n",
      "train loss 0.5015089719163353 l2 0.1948665124781863 mse 0.5015089719163353\n",
      "train loss 0.5027127259871834 l2 0.19525816605947793 mse 0.5027127259871834\n",
      "train loss 0.5027344155654935 l2 0.1956504614250639 mse 0.5027344155654935\n",
      "train loss 0.5016859639959028 l2 0.19604343109613473 mse 0.5016859639959028\n",
      "train loss 0.5019968829466297 l2 0.1964388348043639 mse 0.5019968829466297\n",
      "train loss 0.5030199143695766 l2 0.19683720827609408 mse 0.5030199143695766\n",
      "train loss 0.5021121028092537 l2 0.19723721957411453 mse 0.5021121028092537\n",
      "train loss 0.5022272746078762 l2 0.19763503951424713 mse 0.5022272746078762\n",
      "train loss 0.5015998217168979 l2 0.19803504630797514 mse 0.5015998217168979\n",
      "train loss 0.5027377581956087 l2 0.19843728887265893 mse 0.5027377581956087\n",
      "train loss 0.4992133396148301 l2 0.19883938838920517 mse 0.4992133396148301\n",
      "train loss 0.503295851001039 l2 0.19924600341638482 mse 0.503295851001039\n",
      "train loss 0.5007906387468442 l2 0.1996530658701552 mse 0.5007906387468442\n",
      "train loss 0.5009638183649681 l2 0.2000629413641686 mse 0.5009638183649681\n",
      "train loss 0.5009318203817623 l2 0.2004762174543512 mse 0.5009318203817623\n",
      "val loss 0.5154931574026137\n",
      "val acc 0.7174940413619001 val auc 0.7930820902063257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucastong/.local/lib/python3.6/site-packages/ipykernel_launcher.py:100: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW BEST AUC: 0.7930820902063257 @ epoch 19\n",
      "NEW BEST TRAIN LOSS: 0.5009318203817623 @ epoch 19\n",
      "epoch 20\n",
      "training\n",
      "train loss 0.5009058913098953 l2 0.20088997122142516 mse 0.5009058913098953\n",
      "train loss 0.500627401986341 l2 0.20130581024960156 mse 0.500627401986341\n",
      "train loss 0.50183309984213 l2 0.20172538560142095 mse 0.50183309984213\n",
      "train loss 0.5018774380437251 l2 0.20214602167589657 mse 0.5018774380437251\n",
      "train loss 0.5008269430458534 l2 0.20256678602483025 mse 0.5008269430458534\n",
      "train loss 0.5011343769966076 l2 0.202989438541364 mse 0.5011343769966076\n",
      "train loss 0.5021496120164293 l2 0.20341454042514465 mse 0.5021496120164293\n",
      "train loss 0.501288575029533 l2 0.20384081768940623 mse 0.501288575029533\n",
      "train loss 0.5013668547596061 l2 0.20426405811212398 mse 0.5013668547596061\n",
      "train loss 0.5007414711719782 l2 0.20468839819088616 mse 0.5007414711719782\n",
      "train loss 0.5018721378064264 l2 0.20511328497544076 mse 0.5018721378064264\n",
      "train loss 0.49834121907178225 l2 0.20553663536834366 mse 0.49834121907178225\n",
      "train loss 0.5024119100120281 l2 0.20596257982433908 mse 0.5024119100120281\n",
      "train loss 0.49993064305211865 l2 0.20638840549255305 mse 0.49993064305211865\n",
      "train loss 0.5001024447893003 l2 0.20681563467098368 mse 0.5001024447893003\n",
      "train loss 0.5000680025278262 l2 0.20724500067075202 mse 0.5000680025278262\n",
      "val loss 0.515278241083232\n",
      "val acc 0.7179778837758012 val auc 0.7939602794523488\n",
      "NEW BEST AUC: 0.7939602794523488 @ epoch 20\n",
      "NEW BEST TRAIN LOSS: 0.5000680025278262 @ epoch 20\n",
      "epoch 21\n",
      "training\n",
      "train loss 0.500052505273694 l2 0.20767407829211093 mse 0.500052505273694\n",
      "train loss 0.499765008137327 l2 0.20810437997468936 mse 0.499765008137327\n",
      "train loss 0.5009863214097457 l2 0.20853791174116812 mse 0.5009863214097457\n",
      "train loss 0.5010492383193837 l2 0.20897176752813945 mse 0.5010492383193837\n",
      "train loss 0.4999939236026403 l2 0.20940496055479127 mse 0.4999939236026403\n",
      "train loss 0.5002983434475383 l2 0.20983934760909104 mse 0.5002983434475383\n",
      "train loss 0.5013025628120833 l2 0.21027582526020475 mse 0.5013025628120833\n",
      "train loss 0.5004805404279796 l2 0.21071277663288995 mse 0.5004805404279796\n",
      "train loss 0.5005274450421267 l2 0.2111463857834853 mse 0.5005274450421267\n",
      "train loss 0.4999042212563097 l2 0.21158006269117272 mse 0.4999042212563097\n",
      "train loss 0.501040770056252 l2 0.2120129870331182 mse 0.501040770056252\n",
      "train loss 0.49750437810000386 l2 0.212442122623301 mse 0.49750437810000386\n",
      "train loss 0.501570178389354 l2 0.21287128106857417 mse 0.501570178389354\n",
      "train loss 0.4991121821718202 l2 0.21329835610249756 mse 0.4991121821718202\n",
      "train loss 0.49928326698804093 l2 0.21372454294775142 mse 0.49928326698804093\n",
      "train loss 0.4992434985510619 l2 0.21415032663334782 mse 0.4992434985510619\n",
      "val loss 0.515126314290143\n",
      "val acc 0.7184774791985269 val auc 0.7947355698374426\n",
      "NEW BEST AUC: 0.7947355698374426 @ epoch 21\n",
      "NEW BEST TRAIN LOSS: 0.4992434985510619 @ epoch 21\n",
      "epoch 22\n",
      "training\n",
      "train loss 0.4992465451784031 l2 0.21457355393473318 mse 0.4992465451784031\n",
      "train loss 0.4989509111881883 l2 0.21499549754723815 mse 0.4989509111881883\n",
      "train loss 0.5001962898838705 l2 0.2154186908759942 mse 0.5001962898838705\n",
      "train loss 0.5002625432399838 l2 0.21584042855133478 mse 0.5002625432399838\n",
      "train loss 0.49920465442570267 l2 0.21626054864263686 mse 0.49920465442570267\n",
      "train loss 0.4995112073205981 l2 0.21668135095283672 mse 0.4995112073205981\n",
      "train loss 0.5005088359343393 l2 0.2171046703943966 mse 0.5005088359343393\n",
      "train loss 0.49972577065113366 l2 0.21752945458816084 mse 0.49972577065113366\n",
      "train loss 0.4997480542584192 l2 0.21795230379439237 mse 0.4997480542584192\n",
      "train loss 0.49911955506450195 l2 0.21837653238969967 mse 0.49911955506450195\n",
      "train loss 0.5002687696664976 l2 0.21880150197190548 mse 0.5002687696664976\n",
      "train loss 0.4967178130771242 l2 0.21922323658167694 mse 0.4967178130771242\n",
      "train loss 0.500782246895679 l2 0.21964539013772177 mse 0.500782246895679\n",
      "train loss 0.49834169001995354 l2 0.2200654676482774 mse 0.49834169001995354\n",
      "train loss 0.49851086266255723 l2 0.22048419996972207 mse 0.49851086266255723\n",
      "train loss 0.49845923533728886 l2 0.22090123519698915 mse 0.49845923533728886\n",
      "val loss 0.5149994595562035\n",
      "val acc 0.7189213139709685 val auc 0.7954160826789755\n",
      "NEW BEST AUC: 0.7954160826789755 @ epoch 22\n",
      "NEW BEST TRAIN LOSS: 0.49845923533728886 @ epoch 22\n",
      "epoch 23\n",
      "training\n",
      "train loss 0.49849260279975766 l2 0.22131432864155376 mse 0.49849260279975766\n",
      "train loss 0.49819934548445555 l2 0.22172331727231648 mse 0.49819934548445555\n",
      "train loss 0.49947459622159757 l2 0.22213090869763488 mse 0.49947459622159757\n",
      "train loss 0.49952818369811874 l2 0.22253325474334765 mse 0.49952818369811874\n",
      "train loss 0.49846905259116275 l2 0.22293089206011654 mse 0.49846905259116275\n",
      "train loss 0.49877613112514607 l2 0.22332634206381113 mse 0.49877613112514607\n",
      "train loss 0.4997614782591918 l2 0.22372274912557102 mse 0.4997614782591918\n",
      "train loss 0.4990132712310103 l2 0.2241205217949874 mse 0.4990132712310103\n",
      "train loss 0.4990160138697594 l2 0.22451695953774234 mse 0.4990160138697594\n",
      "train loss 0.4983847069373698 l2 0.2249161731048737 mse 0.4983847069373698\n",
      "train loss 0.49955474994142895 l2 0.2253181909275939 mse 0.49955474994142895\n",
      "train loss 0.49599678942137587 l2 0.22571909117301292 mse 0.49599678942137587\n",
      "train loss 0.5000674391212709 l2 0.2261235101509617 mse 0.5000674391212709\n",
      "train loss 0.49763829625016304 l2 0.2265291036513659 mse 0.49763829625016304\n",
      "train loss 0.49779332113032976 l2 0.2269366428451553 mse 0.49779332113032976\n",
      "train loss 0.4977203414238786 l2 0.2273455774668227 mse 0.4977203414238786\n",
      "val loss 0.5148617930180306\n",
      "val acc 0.7194726692798324 val auc 0.7960145427148672\n",
      "NEW BEST AUC: 0.7960145427148672 @ epoch 23\n",
      "NEW BEST TRAIN LOSS: 0.4977203414238786 @ epoch 23\n",
      "epoch 24\n",
      "training\n",
      "train loss 0.4977803302949862 l2 0.22775315478011962 mse 0.4977803302949862\n",
      "train loss 0.4974865496161845 l2 0.22815758639641034 mse 0.4974865496161845\n",
      "train loss 0.49878991392438093 l2 0.22856047373051805 mse 0.49878991392438093\n",
      "train loss 0.4988301439036153 l2 0.22895515145383136 mse 0.4988301439036153\n",
      "train loss 0.4977762739102179 l2 0.2293417683421945 mse 0.4977762739102179\n",
      "train loss 0.4980982126798845 l2 0.2297217796391742 mse 0.4980982126798845\n",
      "train loss 0.49907323492565986 l2 0.23009808931216352 mse 0.49907323492565986\n",
      "train loss 0.4983561147883531 l2 0.23047123484769572 mse 0.4983561147883531\n",
      "train loss 0.49833695820757096 l2 0.23083836482114356 mse 0.49833695820757096\n",
      "train loss 0.49769480893503154 l2 0.23120482269532722 mse 0.49769480893503154\n",
      "train loss 0.49887298601777275 l2 0.23157220296147865 mse 0.49887298601777275\n",
      "train loss 0.49530890777960646 l2 0.2319377547102602 mse 0.49530890777960646\n",
      "train loss 0.49938834252553327 l2 0.23230840148984228 mse 0.49938834252553327\n",
      "train loss 0.4969804822054044 l2 0.23268291956158776 mse 0.4969804822054044\n",
      "train loss 0.49713153481883826 l2 0.23306359283540656 mse 0.49713153481883826\n",
      "train loss 0.4970495914313009 l2 0.23345142895987603 mse 0.4970495914313009\n",
      "val loss 0.5147582391160764\n",
      "val acc 0.7199975195262295 val auc 0.7965725519458039\n",
      "NEW BEST AUC: 0.7965725519458039 @ epoch 24\n",
      "NEW BEST TRAIN LOSS: 0.4970495914313009 @ epoch 24\n",
      "epoch 25\n",
      "training\n",
      "train loss 0.4971307095961255 l2 0.2338437351771235 mse 0.4971307095961255\n",
      "train loss 0.4968332557526146 l2 0.23423997997899393 mse 0.4968332557526146\n",
      "train loss 0.4981453559171326 l2 0.23464197824289948 mse 0.4981453559171326\n",
      "train loss 0.49815642356945783 l2 0.23504196072965858 mse 0.49815642356945783\n",
      "train loss 0.49709518635643113 l2 0.23543828153995028 mse 0.49709518635643113\n",
      "train loss 0.49743409499462876 l2 0.23583020780265868 mse 0.49743409499462876\n",
      "train loss 0.49840517081845687 l2 0.2362173620357234 mse 0.49840517081845687\n",
      "train loss 0.49773523927257957 l2 0.2365977806283043 mse 0.49773523927257957\n",
      "train loss 0.49771117597700504 l2 0.23696601105997145 mse 0.49771117597700504\n",
      "train loss 0.4970705475705799 l2 0.2373258850750021 mse 0.4970705475705799\n",
      "train loss 0.4982555740777613 l2 0.23767816144911477 mse 0.4982555740777613\n",
      "train loss 0.4946860962689751 l2 0.23801964605645776 mse 0.4946860962689751\n",
      "train loss 0.498745116807329 l2 0.23835815905568303 mse 0.498745116807329\n",
      "train loss 0.4963467027471747 l2 0.2386943409086777 mse 0.4963467027471747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.4964825794391186 l2 0.23903301410777875 mse 0.4964825794391186\n",
      "train loss 0.4963936704312232 l2 0.23937845642110234 mse 0.4963936704312232\n",
      "val loss 0.5147682410456733\n",
      "val acc 0.7203870939349416 val auc 0.797186294601837\n",
      "NEW BEST AUC: 0.797186294601837 @ epoch 25\n",
      "NEW BEST TRAIN LOSS: 0.4963936704312232 @ epoch 25\n",
      "epoch 26\n",
      "training\n",
      "train loss 0.4965031471229236 l2 0.2397303575062705 mse 0.4965031471229236\n",
      "train loss 0.49623160238700204 l2 0.24009172334259335 mse 0.49623160238700204\n",
      "train loss 0.4975789475059948 l2 0.24046692377156087 mse 0.4975789475059948\n",
      "train loss 0.49758380282848774 l2 0.24085065364038274 mse 0.49758380282848774\n",
      "train loss 0.4965098829842991 l2 0.24124260662129063 mse 0.4965098829842991\n",
      "train loss 0.496853204296723 l2 0.24164377453720573 mse 0.496853204296723\n",
      "train loss 0.49778178529146405 l2 0.24205189610492203 mse 0.49778178529146405\n",
      "train loss 0.49710654201445614 l2 0.24246173482579952 mse 0.49710654201445614\n",
      "train loss 0.49706930458693865 l2 0.2428653551166496 mse 0.49706930458693865\n",
      "train loss 0.49644426270579767 l2 0.24326194406343482 mse 0.49644426270579767\n",
      "train loss 0.49766114396484257 l2 0.24364850245982456 mse 0.49766114396484257\n",
      "train loss 0.4941603427603606 l2 0.2440181272807151 mse 0.4941603427603606\n",
      "train loss 0.49824464719244077 l2 0.2443759747367051 mse 0.49824464719244077\n",
      "train loss 0.49588376868159134 l2 0.2447188112185116 mse 0.49588376868159134\n",
      "train loss 0.4959756130722648 l2 0.24505009634575073 mse 0.4959756130722648\n",
      "train loss 0.49582124506182995 l2 0.245372472030419 mse 0.49582124506182995\n",
      "val loss 0.5147802453083462\n",
      "val acc 0.7209764565031921 val auc 0.7977059916479262\n",
      "NEW BEST AUC: 0.7977059916479262 @ epoch 26\n",
      "NEW BEST TRAIN LOSS: 0.49582124506182995 @ epoch 26\n",
      "epoch 27\n",
      "training\n",
      "train loss 0.4959127588690978 l2 0.2456866976332988 mse 0.4959127588690978\n",
      "train loss 0.49561814044875174 l2 0.2459989435722175 mse 0.49561814044875174\n",
      "train loss 0.49699104491138824 l2 0.24631817195660557 mse 0.49699104491138824\n",
      "train loss 0.49702877877253354 l2 0.24664372936619736 mse 0.49702877877253354\n",
      "train loss 0.4960152504946117 l2 0.24697926954156443 mse 0.4960152504946117\n",
      "train loss 0.49646853716149414 l2 0.24733062921568946 mse 0.49646853716149414\n",
      "train loss 0.4974464312084925 l2 0.24769804079342192 mse 0.4974464312084925\n",
      "train loss 0.4967732147607562 l2 0.24807912597495288 mse 0.4967732147607562\n",
      "train loss 0.4966791220004966 l2 0.24847204241545282 mse 0.4966791220004966\n",
      "train loss 0.4959596547855908 l2 0.24887728755300342 mse 0.4959596547855908\n",
      "train loss 0.4970870722400393 l2 0.24929122774477688 mse 0.4970870722400393\n",
      "train loss 0.4935474214909795 l2 0.24970425191651874 mse 0.4935474214909795\n",
      "train loss 0.4976714048236342 l2 0.2501164645477197 mse 0.4976714048236342\n",
      "train loss 0.4954862286887932 l2 0.25051941397837946 mse 0.4954862286887932\n",
      "train loss 0.4957159142164493 l2 0.250912749842894 mse 0.4957159142164493\n",
      "train loss 0.4956418184901307 l2 0.25129457655293 mse 0.4956418184901307\n",
      "val loss 0.5152733445531318\n",
      "val acc 0.719951260690792 val auc 0.7976366013964007\n",
      "NEW BEST TRAIN LOSS: 0.4956418184901307 @ epoch 27\n",
      "epoch 28\n",
      "training\n",
      "train loss 0.49583584595234875 l2 0.25165920791704854 mse 0.49583584595234875\n",
      "train loss 0.49552183465803645 l2 0.25200445748031247 mse 0.49552183465803645\n",
      "train loss 0.4967916659005839 l2 0.25233241897033104 mse 0.4967916659005839\n",
      "train loss 0.4966394677524813 l2 0.2526406746940548 mse 0.4966394677524813\n",
      "train loss 0.49547222000535374 l2 0.2529337542286171 mse 0.49547222000535374\n",
      "train loss 0.4958897419802929 l2 0.2532220523123233 mse 0.4958897419802929\n",
      "train loss 0.4969614110733977 l2 0.2535116439870253 mse 0.4969614110733977\n",
      "train loss 0.49656720207015026 l2 0.2538081484542618 mse 0.49656720207015026\n",
      "train loss 0.4968195799423402 l2 0.25411807472691705 mse 0.4968195799423402\n",
      "train loss 0.4964237573379102 l2 0.25444576091405974 mse 0.4964237573379102\n",
      "train loss 0.4977839859996169 l2 0.25479159466184875 mse 0.4977839859996169\n",
      "train loss 0.4941814409099138 l2 0.25515059242485405 mse 0.4941814409099138\n",
      "train loss 0.4979641099112975 l2 0.25552449145142325 mse 0.4979641099112975\n",
      "train loss 0.4952444932654107 l2 0.25590709423896896 mse 0.4952444932654107\n",
      "train loss 0.495052909426475 l2 0.2563002497364875 mse 0.495052909426475\n",
      "train loss 0.49483856775046803 l2 0.2567049466958554 mse 0.49483856775046803\n",
      "val loss 0.5163272076567361\n",
      "val acc 0.7165253563430615 val auc 0.7960871463228975\n",
      "NEW BEST TRAIN LOSS: 0.49483856775046803 @ epoch 28\n",
      "epoch 29\n",
      "training\n",
      "train loss 0.4953319060307608 l2 0.2571157504388742 mse 0.4953319060307608\n",
      "train loss 0.4957233486798162 l2 0.25752613861259166 mse 0.4957233486798162\n",
      "train loss 0.4978250408378072 l2 0.25793145630706255 mse 0.4978250408378072\n",
      "train loss 0.4984099205372336 l2 0.25831974941631336 mse 0.4984099205372336\n",
      "train loss 0.49763607235745977 l2 0.25868581380105893 mse 0.49763607235745977\n",
      "train loss 0.49792750697667587 l2 0.2590236107538837 mse 0.49792750697667587\n",
      "train loss 0.49829126045702976 l2 0.2593275187264744 mse 0.49829126045702976\n",
      "train loss 0.4968309863033225 l2 0.25960216570058053 mse 0.4968309863033225\n",
      "train loss 0.4961481491952044 l2 0.25985239887947514 mse 0.4961481491952044\n",
      "train loss 0.49533592109622315 l2 0.26009156964691144 mse 0.49533592109622315\n",
      "train loss 0.49703765764039914 l2 0.2603322585775569 mse 0.49703765764039914\n",
      "train loss 0.4946665085862416 l2 0.26058641309692554 mse 0.4946665085862416\n",
      "train loss 0.5004864246631998 l2 0.2608679177910151 mse 0.5004864246631998\n",
      "train loss 0.4995032187952649 l2 0.2611710543907648 mse 0.4995032187952649\n",
      "train loss 0.5005041731488192 l2 0.2614918847307797 mse 0.5005041731488192\n",
      "train loss 0.500415418335907 l2 0.2618243891799414 mse 0.500415418335907\n",
      "val loss 0.5168601075899153\n",
      "val acc 0.717036704010466 val auc 0.7953440983051667\n",
      "epoch 30\n",
      "training\n",
      "train loss 0.4998122627894935 l2 0.2621537467978834 mse 0.4998122627894935\n",
      "train loss 0.49773770723869876 l2 0.26247463088989464 mse 0.49773770723869876\n",
      "train loss 0.49726654998252673 l2 0.2627858373351398 mse 0.49726654998252673\n",
      "train loss 0.49604476513537027 l2 0.26308897951920185 mse 0.49604476513537027\n",
      "train loss 0.4952278283829111 l2 0.26340033592145357 mse 0.4952278283829111\n",
      "train loss 0.49750676472271105 l2 0.26373260291149275 mse 0.49750676472271105\n",
      "train loss 0.5015891639896908 l2 0.2640860084119687 mse 0.5015891639896908\n",
      "train loss 0.504455662756144 l2 0.2644529247998401 mse 0.504455662756144\n",
      "train loss 0.5072153334615765 l2 0.26479861451794645 mse 0.5072153334615765\n",
      "train loss 0.5078192309810808 l2 0.2650997904045704 mse 0.5078192309810808\n",
      "train loss 0.5084183221620455 l2 0.26532912601377295 mse 0.5084183221620455\n",
      "train loss 0.5018007590104839 l2 0.26547516085171446 mse 0.5018007590104839\n",
      "train loss 0.5015040115033278 l2 0.265559984524574 mse 0.5015040115033278\n",
      "train loss 0.4959306797964211 l2 0.26561234348664187 mse 0.4959306797964211\n",
      "train loss 0.49477681427711934 l2 0.2656718831936522 mse 0.49477681427711934\n",
      "train loss 0.49582756856670146 l2 0.26577291128527714 mse 0.49582756856670146\n",
      "val loss 0.5237535293808769\n",
      "val acc 0.7046098304776213 val auc 0.7931821633369888\n",
      "epoch 31\n",
      "training\n",
      "train loss 0.49939871097262506 l2 0.26593995610949916 mse 0.49939871097262506\n",
      "train loss 0.5044378900845862 l2 0.2661917638904978 mse 0.5044378900845862\n",
      "train loss 0.5112397400837508 l2 0.2665154895602942 mse 0.5112397400837508\n",
      "train loss 0.5159534000767175 l2 0.2668727312782115 mse 0.5159534000767175\n",
      "train loss 0.5171552268558602 l2 0.267213786887708 mse 0.5171552268558602\n",
      "train loss 0.5161870280806322 l2 0.2675061948157211 mse 0.5161870280806322\n",
      "train loss 0.5118841437468348 l2 0.26773152441737463 mse 0.5118841437468348\n",
      "train loss 0.5043414158299767 l2 0.2678936776692104 mse 0.5043414158299767\n",
      "train loss 0.4988462638821729 l2 0.2680437892175756 mse 0.4988462638821729\n",
      "train loss 0.4953641705088413 l2 0.2682107753247366 mse 0.4953641705088413\n",
      "train loss 0.4971707400361997 l2 0.26841702438191434 mse 0.4971707400361997\n",
      "train loss 0.49783073883491463 l2 0.26866385036244605 mse 0.49783073883491463\n",
      "train loss 0.5084361405494892 l2 0.2689366716208116 mse 0.5084361405494892\n",
      "train loss 0.5125768000220229 l2 0.26919246524967766 mse 0.5125768000220229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.5179770631379226 l2 0.2693963246080548 mse 0.5179770631379226\n",
      "train loss 0.5202111915875174 l2 0.2695073408583064 mse 0.5202111915875174\n",
      "val loss 0.5244919545501348\n",
      "val acc 0.703145800847962 val auc 0.7916294232799382\n",
      "epoch 32\n",
      "training\n",
      "train loss 0.519459426262736 l2 0.26949971954466223 mse 0.519459426262736\n",
      "train loss 0.5127652427824159 l2 0.2693806298687996 mse 0.5127652427824159\n",
      "train loss 0.50657053780475 l2 0.2692006895358183 mse 0.50657053780475\n",
      "train loss 0.5001113778921261 l2 0.2690159992807865 mse 0.5001113778921261\n",
      "train loss 0.49586083019318195 l2 0.2688785362668152 mse 0.49586083019318195\n",
      "train loss 0.49682449909923637 l2 0.2688287615455154 mse 0.49682449909923637\n",
      "train loss 0.5007869760534883 l2 0.2688752725130312 mse 0.5007869760534883\n",
      "train loss 0.5043013134868165 l2 0.2690037709322184 mse 0.5043013134868165\n",
      "train loss 0.5083787803786018 l2 0.26920426089208505 mse 0.5083787803786018\n",
      "train loss 0.5111134272047905 l2 0.2694498422273414 mse 0.5111134272047905\n",
      "train loss 0.5137153015572136 l2 0.2697177479221601 mse 0.5137153015572136\n",
      "train loss 0.5086895602817071 l2 0.2699860121417843 mse 0.5086895602817071\n",
      "train loss 0.5088591076367242 l2 0.27026248160873145 mse 0.5088591076367242\n",
      "train loss 0.5023366189619318 l2 0.27055090653513253 mse 0.5023366189619318\n",
      "train loss 0.4986693734308108 l2 0.27086820385270083 mse 0.4986693734308108\n",
      "train loss 0.4957953125960935 l2 0.2712223577125184 mse 0.4957953125960935\n",
      "val loss 0.5140595129746793\n",
      "val acc 0.7208586840086456 val auc 0.7974208689120845\n",
      "epoch 33\n",
      "training\n",
      "train loss 0.49493108447034306 l2 0.27160873691516857 mse 0.49493108447034306\n",
      "train loss 0.4950040872126832 l2 0.27201795652648586 mse 0.4950040872126832\n",
      "train loss 0.4974869402560889 l2 0.27243568287089553 mse 0.4974869402560889\n",
      "train loss 0.4988025800911717 l2 0.27284451745624416 mse 0.4988025800911717\n",
      "train loss 0.4990710414458534 l2 0.2732323192608238 mse 0.4990710414458534\n",
      "train loss 0.5001930590438527 l2 0.2735947863128629 mse 0.5001930590438527\n",
      "train loss 0.500920315437089 l2 0.2739262387763379 mse 0.500920315437089\n",
      "train loss 0.49900484519261795 l2 0.2742354342180108 mse 0.49900484519261795\n",
      "train loss 0.4976200735588271 l2 0.27452884878571737 mse 0.4976200735588271\n",
      "train loss 0.49558966116731634 l2 0.27481645342482663 mse 0.49558966116731634\n",
      "train loss 0.4959622788771766 l2 0.2751054846761225 mse 0.4959622788771766\n",
      "train loss 0.4922290869674291 l2 0.2753988768714961 mse 0.4922290869674291\n",
      "train loss 0.49634942767979207 l2 0.2756991450021562 mse 0.49634942767979207\n",
      "train loss 0.4942090905622126 l2 0.2760005018872125 mse 0.4942090905622126\n",
      "train loss 0.4943265495817553 l2 0.27630284092199603 mse 0.4943265495817553\n",
      "train loss 0.49415651954562034 l2 0.27660402610602336 mse 0.49415651954562034\n",
      "val loss 0.514156369703416\n",
      "val acc 0.7214795525945455 val auc 0.7985182821160007\n",
      "NEW BEST AUC: 0.7985182821160007 @ epoch 33\n",
      "NEW BEST TRAIN LOSS: 0.49415651954562034 @ epoch 33\n",
      "epoch 34\n",
      "training\n",
      "train loss 0.49430141674841543 l2 0.27690143025427216 mse 0.49430141674841543\n",
      "train loss 0.49396013064339594 l2 0.2771936264329144 mse 0.49396013064339594\n",
      "train loss 0.49532657347246895 l2 0.2774804127129682 mse 0.49532657347246895\n",
      "train loss 0.495197118707139 l2 0.2777558371730596 mse 0.495197118707139\n",
      "train loss 0.4940682910501043 l2 0.27801825990238194 mse 0.4940682910501043\n",
      "train loss 0.4944719627814926 l2 0.2782689208385704 mse 0.4944719627814926\n",
      "train loss 0.49529824280877843 l2 0.27851002337704905 mse 0.49529824280877843\n",
      "train loss 0.49464338447238776 l2 0.2787423178326998 mse 0.49464338447238776\n",
      "train loss 0.494576589411855 l2 0.2789683273462628 mse 0.494576589411855\n",
      "train loss 0.49393396363214526 l2 0.2791896578895097 mse 0.49393396363214526\n",
      "train loss 0.4951591837624522 l2 0.27940673919634007 mse 0.4951591837624522\n",
      "train loss 0.49162335618467373 l2 0.2796188142990656 mse 0.49162335618467373\n",
      "train loss 0.49563181764729036 l2 0.2798296229807808 mse 0.49563181764729036\n",
      "train loss 0.49334819369499616 l2 0.28003808114420503 mse 0.49334819369499616\n",
      "train loss 0.4933797950819942 l2 0.280246754322769 mse 0.4933797950819942\n",
      "train loss 0.4931977568139304 l2 0.2804558739560033 mse 0.4931977568139304\n",
      "val loss 0.514394683910439\n",
      "val acc 0.7219828987336582 val auc 0.7991376928505026\n",
      "NEW BEST AUC: 0.7991376928505026 @ epoch 34\n",
      "NEW BEST TRAIN LOSS: 0.4931977568139304 @ epoch 34\n",
      "epoch 35\n",
      "training\n",
      "train loss 0.4933568652599799 l2 0.28066460625857353 mse 0.4933568652599799\n",
      "train loss 0.49305512462314166 l2 0.2808747504879172 mse 0.49305512462314166\n",
      "train loss 0.4944557726722707 l2 0.2810880734762713 mse 0.4944557726722707\n",
      "train loss 0.49437919594687313 l2 0.2813012230629089 mse 0.49437919594687313\n",
      "train loss 0.4933194383679456 l2 0.2815131356811538 mse 0.4933194383679456\n",
      "train loss 0.4938324022097447 l2 0.2817262454785566 mse 0.4938324022097447\n",
      "train loss 0.49470299559309777 l2 0.2819400600335159 mse 0.49470299559309777\n",
      "train loss 0.49406228730184515 l2 0.2821546096286982 mse 0.49406228730184515\n",
      "train loss 0.49399101033646126 l2 0.28237185205356113 mse 0.49399101033646126\n",
      "train loss 0.49335721741753596 l2 0.28259284272916657 mse 0.49335721741753596\n",
      "train loss 0.4945928456584871 l2 0.2828175365941837 mse 0.4945928456584871\n",
      "train loss 0.4910721889009696 l2 0.28304442786368716 mse 0.4910721889009696\n",
      "train loss 0.49508523172432567 l2 0.28327631431279066 mse 0.49508523172432567\n",
      "train loss 0.4928383244119789 l2 0.28351120851275013 mse 0.4928383244119789\n",
      "train loss 0.49285892511411394 l2 0.2837505222892216 mse 0.49285892511411394\n",
      "train loss 0.49269428354304023 l2 0.28399398776883633 mse 0.49269428354304023\n",
      "val loss 0.5143576385798831\n",
      "val acc 0.7220471610077525 val auc 0.7990387634596847\n",
      "NEW BEST TRAIN LOSS: 0.49269428354304023 @ epoch 35\n",
      "epoch 36\n",
      "training\n",
      "train loss 0.4928749332366091 l2 0.2842398887674408 mse 0.4928749332366091\n",
      "train loss 0.492563901525021 l2 0.2844895652444146 mse 0.492563901525021\n",
      "train loss 0.493981920706565 l2 0.28474388722952226 mse 0.493981920706565\n",
      "train loss 0.49389465641506614 l2 0.28499881251956655 mse 0.49389465641506614\n",
      "train loss 0.4928296581277974 l2 0.28525275143059053 mse 0.4928296581277974\n",
      "train loss 0.49334382466073595 l2 0.2855076501608836 mse 0.49334382466073595\n",
      "train loss 0.49419971872227786 l2 0.2857616729982311 mse 0.49419971872227786\n",
      "train loss 0.4935476600403261 l2 0.28601275746964144 mse 0.4935476600403261\n",
      "train loss 0.4934864291156254 l2 0.2862626016938283 mse 0.4934864291156254\n",
      "train loss 0.492868159974948 l2 0.2865113552230413 mse 0.492868159974948\n",
      "train loss 0.49412715069776314 l2 0.28675845871753575 mse 0.49412715069776314\n",
      "train loss 0.49062525313917016 l2 0.2870018554100882 mse 0.49062525313917016\n",
      "train loss 0.49462746649325784 l2 0.2872441049458066 mse 0.49462746649325784\n",
      "train loss 0.4924025919977983 l2 0.287483918644768 mse 0.4924025919977983\n",
      "train loss 0.49241192415639773 l2 0.28772262220818184 mse 0.49241192415639773\n",
      "train loss 0.4922359384574053 l2 0.2879602740314765 mse 0.4922359384574053\n",
      "val loss 0.5146027379654883\n",
      "val acc 0.7221069224221827 val auc 0.7991348809052514\n",
      "NEW BEST TRAIN LOSS: 0.4922359384574053 @ epoch 36\n",
      "epoch 37\n",
      "training\n",
      "train loss 0.4924135048747341 l2 0.28819542594328074 mse 0.4924135048747341\n",
      "train loss 0.492091038708918 l2 0.2884301892142681 mse 0.492091038708918\n",
      "train loss 0.4935167318344617 l2 0.28866697757176923 mse 0.4935167318344617\n",
      "train loss 0.49342600171690765 l2 0.2889031754672737 mse 0.49342600171690765\n",
      "train loss 0.4923611822659845 l2 0.28913856724981685 mse 0.4923611822659845\n",
      "train loss 0.49289590869034094 l2 0.2893761728029631 mse 0.49289590869034094\n",
      "train loss 0.49376140172592337 l2 0.2896144527725611 mse 0.49376140172592337\n",
      "train loss 0.4931082901040061 l2 0.28985186334037044 mse 0.4931082901040061\n",
      "train loss 0.49305578291379154 l2 0.290090410999696 mse 0.49305578291379154\n",
      "train loss 0.4924368145805947 l2 0.29033013076127206 mse 0.4924368145805947\n",
      "train loss 0.49369363570414987 l2 0.290570867238873 mse 0.49369363570414987\n",
      "train loss 0.49018102329446095 l2 0.29081048668955345 mse 0.49018102329446095\n",
      "train loss 0.4941664494114236 l2 0.2910514178970084 mse 0.4941664494114236\n",
      "train loss 0.4919505432635017 l2 0.2912923802463681 mse 0.4919505432635017\n",
      "train loss 0.4919458913578192 l2 0.2915348528200606 mse 0.4919458913578192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.491776775396132 l2 0.29177860641113856 mse 0.491776775396132\n",
      "val loss 0.5146693849795472\n",
      "val acc 0.722204191000481 val auc 0.7991774357974157\n",
      "NEW BEST AUC: 0.7991774357974157 @ epoch 37\n",
      "NEW BEST TRAIN LOSS: 0.491776775396132 @ epoch 37\n",
      "epoch 38\n",
      "training\n",
      "train loss 0.491969082436253 l2 0.2920216617819011 mse 0.491969082436253\n",
      "train loss 0.49165333885793777 l2 0.292265588490352 mse 0.49165333885793777\n",
      "train loss 0.4930954649316016 l2 0.2925122066592264 mse 0.4930954649316016\n",
      "train loss 0.4930029546133061 l2 0.29275824869388684 mse 0.4930029546133061\n",
      "train loss 0.4919333845004645 l2 0.29300337747763716 mse 0.4919333845004645\n",
      "train loss 0.4924741043860026 l2 0.2932503167938392 mse 0.4924741043860026\n",
      "train loss 0.493320673611682 l2 0.2934972018469045 mse 0.493320673611682\n",
      "train loss 0.4926534306473357 l2 0.2937431141053488 mse 0.4926534306473357\n",
      "train loss 0.4926015850279129 l2 0.29398983803717477 mse 0.4926015850279129\n",
      "train loss 0.4919848874351587 l2 0.2942372662623367 mse 0.4919848874351587\n",
      "train loss 0.49325266916170213 l2 0.294485108696725 mse 0.49325266916170213\n",
      "train loss 0.489751448318373 l2 0.29473145372043413 mse 0.489751448318373\n",
      "train loss 0.49372953218226195 l2 0.2949788578377352 mse 0.49372953218226195\n",
      "train loss 0.49153196256856774 l2 0.2952262987907832 mse 0.49153196256856774\n",
      "train loss 0.49151590369700726 l2 0.2954752528134673 mse 0.49151590369700726\n",
      "train loss 0.49134189781288734 l2 0.29572580645182384 mse 0.49134189781288734\n",
      "val loss 0.5147232110350827\n",
      "val acc 0.7223837252915307 val auc 0.7992862939052562\n",
      "NEW BEST AUC: 0.7992862939052562 @ epoch 38\n",
      "NEW BEST TRAIN LOSS: 0.49134189781288734 @ epoch 38\n",
      "epoch 39\n",
      "training\n",
      "train loss 0.49153242671100744 l2 0.2959762846280445 mse 0.49153242671100744\n",
      "train loss 0.4912062391287523 l2 0.2962283595365165 mse 0.4912062391287523\n",
      "train loss 0.49265063989496966 l2 0.29648387278384364 mse 0.49265063989496966\n",
      "train loss 0.4925505739741023 l2 0.2967392065165039 mse 0.4925505739741023\n",
      "train loss 0.4914759974581565 l2 0.2969939787436291 mse 0.4914759974581565\n",
      "train loss 0.49203128859452583 l2 0.29725061932515046 mse 0.49203128859452583\n",
      "train loss 0.4928705660620046 l2 0.29750717190224246 mse 0.4928705660620046\n",
      "train loss 0.4921997536931702 l2 0.29776260524970116 mse 0.4921997536931702\n",
      "train loss 0.4921573774667511 l2 0.2980187745632625 mse 0.4921573774667511\n",
      "train loss 0.49154303469036004 l2 0.29827528230787903 mse 0.49154303469036004\n",
      "train loss 0.4928128502278947 l2 0.29853183858552146 mse 0.4928128502278947\n",
      "train loss 0.48931618709694086 l2 0.2987864863456075 mse 0.48931618709694086\n",
      "train loss 0.493275507764991 l2 0.2990417651151266 mse 0.493275507764991\n",
      "train loss 0.49109229833235496 l2 0.2992970013908242 mse 0.49109229833235496\n",
      "train loss 0.49106023081467914 l2 0.2995536390646125 mse 0.49106023081467914\n",
      "train loss 0.49087831136575066 l2 0.2998115742462576 mse 0.49087831136575066\n",
      "val loss 0.5147750972394074\n",
      "val acc 0.7225910148838428 val auc 0.7994494106441535\n",
      "NEW BEST AUC: 0.7994494106441535 @ epoch 39\n",
      "NEW BEST TRAIN LOSS: 0.49087831136575066 @ epoch 39\n",
      "epoch 40\n",
      "training\n",
      "train loss 0.49107313816724624 l2 0.3000694096758338 mse 0.49107313816724624\n",
      "train loss 0.4907453992984586 l2 0.30032938009785504 mse 0.4907453992984586\n",
      "train loss 0.49219774139208317 l2 0.30059357240109513 mse 0.49219774139208317\n",
      "train loss 0.4920988960865838 l2 0.30085859944763366 mse 0.4920988960865838\n",
      "train loss 0.4910170564005389 l2 0.30112422345050877 mse 0.4910170564005389\n",
      "train loss 0.49158447999553223 l2 0.30139296697078755 mse 0.49158447999553223\n",
      "train loss 0.4924098111570009 l2 0.3016628246158361 mse 0.4924098111570009\n",
      "train loss 0.49172010413067135 l2 0.30193212430000116 mse 0.49172010413067135\n",
      "train loss 0.49168268283413974 l2 0.3022030651317144 mse 0.49168268283413974\n",
      "train loss 0.4910675074108459 l2 0.3024749976130419 mse 0.4910675074108459\n",
      "train loss 0.4923371241277057 l2 0.30274752793086 mse 0.4923371241277057\n",
      "train loss 0.488854812532668 l2 0.30301854552200735 mse 0.488854812532668\n",
      "train loss 0.4928021853286036 l2 0.3032902889350321 mse 0.4928021853286036\n",
      "train loss 0.49064023405054963 l2 0.3035620686309491 mse 0.49064023405054963\n",
      "train loss 0.4905984074131999 l2 0.3038350649407946 mse 0.4905984074131999\n",
      "train loss 0.49040555175458544 l2 0.3041088025266792 mse 0.49040555175458544\n",
      "val loss 0.5148303791531796\n",
      "val acc 0.72270003570682 val auc 0.7996061782939428\n",
      "NEW BEST AUC: 0.7996061782939428 @ epoch 40\n",
      "NEW BEST TRAIN LOSS: 0.49040555175458544 @ epoch 40\n",
      "epoch 41\n",
      "training\n",
      "train loss 0.49060148403948123 l2 0.30438164984851485 mse 0.49060148403948123\n",
      "train loss 0.49026821356816813 l2 0.30465605148443464 mse 0.49026821356816813\n",
      "train loss 0.4917180618548466 l2 0.3049340480674503 mse 0.4917180618548466\n",
      "train loss 0.49161375921367884 l2 0.30521247015846803 mse 0.49161375921367884\n",
      "train loss 0.4905226326950198 l2 0.3054915654923241 mse 0.4905226326950198\n",
      "train loss 0.4911051886405688 l2 0.3057739457046953 mse 0.4911051886405688\n",
      "train loss 0.4919212279680413 l2 0.3060577636232994 mse 0.4919212279680413\n",
      "train loss 0.49122165931218803 l2 0.3063413445497733 mse 0.49122165931218803\n",
      "train loss 0.4911958290533036 l2 0.3066271443220858 mse 0.4911958290533036\n",
      "train loss 0.4905845550658928 l2 0.3069145078712356 mse 0.4905845550658928\n",
      "train loss 0.491850603579024 l2 0.30720306977682177 mse 0.491850603579024\n",
      "train loss 0.48837692821796924 l2 0.3074905342367945 mse 0.48837692821796924\n",
      "train loss 0.49230482981464446 l2 0.30777914319580935 mse 0.49230482981464446\n",
      "train loss 0.49015096055665636 l2 0.3080683262811537 mse 0.49015096055665636\n",
      "train loss 0.4900949020108835 l2 0.3083593458120102 mse 0.4900949020108835\n",
      "train loss 0.4898919779515253 l2 0.3086516992499797 mse 0.4898919779515253\n",
      "val loss 0.5148561909446646\n",
      "val acc 0.7229125763020737 val auc 0.7997771767302208\n",
      "NEW BEST AUC: 0.7997771767302208 @ epoch 41\n",
      "NEW BEST TRAIN LOSS: 0.4898919779515253 @ epoch 41\n",
      "epoch 42\n",
      "training\n",
      "train loss 0.490092386870697 l2 0.30894380025577556 mse 0.490092386870697\n",
      "train loss 0.4897632932290283 l2 0.30923782352310064 mse 0.4897632932290283\n",
      "train loss 0.4912155389837796 l2 0.3095353545801582 mse 0.4912155389837796\n",
      "train loss 0.4911123943516533 l2 0.3098327308769226 mse 0.4911123943516533\n",
      "train loss 0.49001740277394074 l2 0.3101301373128318 mse 0.49001740277394074\n",
      "train loss 0.49061399434725994 l2 0.31042994557682335 mse 0.49061399434725994\n",
      "train loss 0.4914075637341397 l2 0.31073014927731657 mse 0.4914075637341397\n",
      "train loss 0.490690376470599 l2 0.3110298473629505 mse 0.490690376470599\n",
      "train loss 0.4906703512488136 l2 0.31133118351366185 mse 0.4906703512488136\n",
      "train loss 0.4900587226606619 l2 0.31163390868563695 mse 0.4900587226606619\n",
      "train loss 0.4913223390905416 l2 0.3119374465863612 mse 0.4913223390905416\n",
      "train loss 0.48786588312598533 l2 0.3122397698750406 mse 0.48786588312598533\n",
      "train loss 0.49178403439938584 l2 0.3125433931426367 mse 0.49178403439938584\n",
      "train loss 0.4896472111818753 l2 0.31284792266669387 mse 0.4896472111818753\n",
      "train loss 0.48958424625635527 l2 0.31315496243429836 mse 0.48958424625635527\n",
      "train loss 0.48937242362078226 l2 0.3134640403513618 mse 0.48937242362078226\n",
      "val loss 0.514854817768081\n",
      "val acc 0.7232286366696039 val auc 0.8000043835048172\n",
      "NEW BEST AUC: 0.8000043835048172 @ epoch 42\n",
      "NEW BEST TRAIN LOSS: 0.48937242362078226 @ epoch 42\n",
      "epoch 43\n",
      "training\n",
      "train loss 0.4895685287701118 l2 0.3137738619466991 mse 0.4895685287701118\n",
      "train loss 0.4892283937678055 l2 0.3140868732877859 mse 0.4892283937678055\n",
      "train loss 0.4906735422201274 l2 0.31440437287191275 mse 0.4906735422201274\n",
      "train loss 0.49056589192952316 l2 0.3147221378653849 mse 0.49056589192952316\n",
      "train loss 0.48946326396400974 l2 0.31504048504488424 mse 0.48946326396400974\n",
      "train loss 0.49008363859835274 l2 0.3153612746859749 mse 0.49008363859835274\n",
      "train loss 0.4908620165603741 l2 0.31568274372461275 mse 0.4908620165603741\n",
      "train loss 0.4901443215831038 l2 0.3160035473948673 mse 0.4901443215831038\n",
      "train loss 0.49014056430464753 l2 0.31632532665984686 mse 0.49014056430464753\n",
      "train loss 0.48953083393869995 l2 0.3166477760746666 mse 0.48953083393869995\n",
      "train loss 0.49078438896605325 l2 0.31697010258805836 mse 0.49078438896605325\n",
      "train loss 0.48733452464663873 l2 0.31729015584157283 mse 0.48733452464663873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.49122339810407367 l2 0.31761033260537125 mse 0.49122339810407367\n",
      "train loss 0.48909641089034506 l2 0.3179308978098309 mse 0.48909641089034506\n",
      "train loss 0.48902361457301413 l2 0.3182533759900306 mse 0.48902361457301413\n",
      "train loss 0.4888071379490512 l2 0.3185771576415583 mse 0.4888071379490512\n",
      "val loss 0.5148829689577169\n",
      "val acc 0.7234849356227039 val auc 0.8002626877403609\n",
      "NEW BEST AUC: 0.8002626877403609 @ epoch 43\n",
      "NEW BEST TRAIN LOSS: 0.4888071379490512 @ epoch 43\n",
      "epoch 44\n",
      "training\n",
      "train loss 0.48901284657475147 l2 0.31890122950264055 mse 0.48901284657475147\n",
      "train loss 0.4886814179488302 l2 0.3192287426200279 mse 0.4886814179488302\n",
      "train loss 0.49013742735859783 l2 0.3195615891317528 mse 0.49013742735859783\n",
      "train loss 0.49003202351322883 l2 0.31989581631839964 mse 0.49003202351322883\n",
      "train loss 0.4889158440583371 l2 0.32023180748327756 mse 0.4889158440583371\n",
      "train loss 0.4895488712962318 l2 0.3205717694248836 mse 0.4895488712962318\n",
      "train loss 0.49028868531171393 l2 0.32091383655314326 mse 0.49028868531171393\n",
      "train loss 0.48954865567692674 l2 0.3212557120378217 mse 0.48954865567692674\n",
      "train loss 0.4895478158054062 l2 0.3215997291981586 mse 0.4895478158054062\n",
      "train loss 0.48894188383963744 l2 0.3219452038892677 mse 0.48894188383963744\n",
      "train loss 0.49019999611884374 l2 0.3222910794215138 mse 0.49019999611884374\n",
      "train loss 0.48678732467139374 l2 0.3226350102316339 mse 0.48678732467139374\n",
      "train loss 0.4906722706599342 l2 0.32297883439674796 mse 0.4906722706599342\n",
      "train loss 0.48857568435861615 l2 0.32332259309783673 mse 0.48857568435861615\n",
      "train loss 0.4884939347911612 l2 0.3236668195157972 mse 0.4884939347911612\n",
      "train loss 0.48825310152308554 l2 0.32401021081902764 mse 0.48825310152308554\n",
      "val loss 0.5149515583532618\n",
      "val acc 0.7235837044875572 val auc 0.8004326258727671\n",
      "NEW BEST AUC: 0.8004326258727671 @ epoch 44\n",
      "NEW BEST TRAIN LOSS: 0.48825310152308554 @ epoch 44\n",
      "epoch 45\n",
      "training\n",
      "train loss 0.4884449283869254 l2 0.3243514284964445 mse 0.4884449283869254\n",
      "train loss 0.4880926583894222 l2 0.3246937327896671 mse 0.4880926583894222\n",
      "train loss 0.4895399009382905 l2 0.3250396056253031 mse 0.4895399009382905\n",
      "train loss 0.4894349111665391 l2 0.32538577219546366 mse 0.4894349111665391\n",
      "train loss 0.4883249651595019 l2 0.32573338542481345 mse 0.4883249651595019\n",
      "train loss 0.4890060922780133 l2 0.32608561461588814 mse 0.4890060922780133\n",
      "train loss 0.4897538205017283 l2 0.3264411753514286 mse 0.4897538205017283\n",
      "train loss 0.48901641250711597 l2 0.3267975399525753 mse 0.48901641250711597\n",
      "train loss 0.4890268708162806 l2 0.32715771233426344 mse 0.4890268708162806\n",
      "train loss 0.48840613832133856 l2 0.3275205764810949 mse 0.48840613832133856\n",
      "train loss 0.48962242041087733 l2 0.32788545223947524 mse 0.48962242041087733\n",
      "train loss 0.48619164627346606 l2 0.3282500330082921 mse 0.48619164627346606\n",
      "train loss 0.4900335662032956 l2 0.32861570155138886 mse 0.4900335662032956\n",
      "train loss 0.48795916185413607 l2 0.32898316775602066 mse 0.48795916185413607\n",
      "train loss 0.48789738020996215 l2 0.3293526959443328 mse 0.48789738020996215\n",
      "train loss 0.48769030179015876 l2 0.32972251267286246 mse 0.48769030179015876\n",
      "val loss 0.5151668469773952\n",
      "val acc 0.7233796655161135 val auc 0.8004145392302399\n",
      "NEW BEST TRAIN LOSS: 0.48769030179015876 @ epoch 45\n",
      "epoch 46\n",
      "training\n",
      "train loss 0.4879380025067419 l2 0.33009047840226907 mse 0.4879380025067419\n",
      "train loss 0.4876204081574692 l2 0.33045821361679184 mse 0.4876204081574692\n",
      "train loss 0.48906910026335804 l2 0.3308265853357701 mse 0.48906910026335804\n",
      "train loss 0.48893499613178176 l2 0.33119123176140214 mse 0.48893499613178176\n",
      "train loss 0.4877705618549497 l2 0.3315534693316923 mse 0.4877705618549497\n",
      "train loss 0.4884167993112042 l2 0.3319160709212725 mse 0.4884167993112042\n",
      "train loss 0.48910150865113505 l2 0.33227829583209945 mse 0.48910150865113505\n",
      "train loss 0.4883588575918582 l2 0.3326400535263594 mse 0.4883588575918582\n",
      "train loss 0.48842183521679183 l2 0.3330047401306664 mse 0.48842183521679183\n",
      "train loss 0.4878738236402835 l2 0.3333725216419458 mse 0.4878738236402835\n",
      "train loss 0.48915935896266977 l2 0.3337436146978539 mse 0.48915935896266977\n",
      "train loss 0.4858045560082451 l2 0.3341160342774248 mse 0.4858045560082451\n",
      "train loss 0.4896405673395174 l2 0.3344911466909806 mse 0.4896405673395174\n",
      "train loss 0.48752779778641603 l2 0.3348695631850856 mse 0.48752779778641603\n",
      "train loss 0.48737836575944365 l2 0.33525171288872924 mse 0.48737836575944365\n",
      "train loss 0.48707862816300757 l2 0.33563699998280616 mse 0.48707862816300757\n",
      "val loss 0.5152039130479512\n",
      "val acc 0.723491186816682 val auc 0.800381775040441\n",
      "NEW BEST TRAIN LOSS: 0.48707862816300757 @ epoch 46\n",
      "epoch 47\n",
      "training\n",
      "train loss 0.48726109013603963 l2 0.33602439509149074 mse 0.48726109013603963\n",
      "train loss 0.4869466315298887 l2 0.3364155130009122 mse 0.4869466315298887\n",
      "train loss 0.48846442434784276 l2 0.33681008006580476 mse 0.48846442434784276\n",
      "train loss 0.48845314693719893 l2 0.33720249178114436 mse 0.48845314693719893\n",
      "train loss 0.4874174719711734 l2 0.3375925080838055 mse 0.4874174719711734\n",
      "train loss 0.48818457213398847 l2 0.33797973775927187 mse 0.48818457213398847\n",
      "train loss 0.4888556116266658 l2 0.33836154669771057 mse 0.4888556116266658\n",
      "train loss 0.48803643910951605 l2 0.3387379651910162 mse 0.48803643910951605\n",
      "train loss 0.48797625723272653 l2 0.3391092610741217 mse 0.48797625723272653\n",
      "train loss 0.4872733403967187 l2 0.3394759164983893 mse 0.4872733403967187\n",
      "train loss 0.48844000188104136 l2 0.339839070730626 mse 0.48844000188104136\n",
      "train loss 0.485098442839788 l2 0.3401997014556226 mse 0.485098442839788\n",
      "train loss 0.48904903874684247 l2 0.3405624630991461 mse 0.48904903874684247\n",
      "train loss 0.4871731915413351 l2 0.3409296971957362 mse 0.4871731915413351\n",
      "train loss 0.48726720285938885 l2 0.34130291566226517 mse 0.48726720285938885\n",
      "train loss 0.48714333543146043 l2 0.34168234538959735 mse 0.48714333543146043\n",
      "val loss 0.5155278850689832\n",
      "val acc 0.723603958356046 val auc 0.8005723212186608\n",
      "NEW BEST AUC: 0.8005723212186608 @ epoch 47\n",
      "epoch 48\n",
      "training\n",
      "train loss 0.4873498251129778 l2 0.34206734962393043 mse 0.4873498251129778\n",
      "train loss 0.4868728687113643 l2 0.3424601151511563 mse 0.4868728687113643\n",
      "train loss 0.48812697054935433 l2 0.3428595504620312 mse 0.48812697054935433\n",
      "train loss 0.4878352773072044 l2 0.343259944176389 mse 0.4878352773072044\n",
      "train loss 0.48659735996370235 l2 0.3436621304441593 mse 0.48659735996370235\n",
      "train loss 0.48739780773127395 l2 0.34406748974343704 mse 0.48739780773127395\n",
      "train loss 0.4882982229099231 l2 0.34447486463323007 mse 0.4882982229099231\n",
      "train loss 0.48792238658859666 l2 0.3448833324575335 mse 0.48792238658859666\n",
      "train loss 0.48833288092690735 l2 0.34528973974853144 mse 0.48833288092690735\n",
      "train loss 0.4879216401727152 l2 0.34569048436187205 mse 0.4879216401727152\n",
      "train loss 0.4890919087162509 l2 0.3460794539976911 mse 0.4890919087162509\n",
      "train loss 0.48547077335035127 l2 0.34645256554114734 mse 0.48547077335035127\n",
      "train loss 0.48886950700670107 l2 0.34681156921804523 mse 0.48886950700670107\n",
      "train loss 0.4864950277707904 l2 0.34715940198222933 mse 0.4864950277707904\n",
      "train loss 0.48629162390412245 l2 0.3475006692591559 mse 0.48629162390412245\n",
      "train loss 0.48625228088365297 l2 0.3478402094828057 mse 0.48625228088365297\n",
      "val loss 0.5168179236680136\n",
      "val acc 0.7222872068565096 val auc 0.8006271671172976\n",
      "NEW BEST AUC: 0.8006271671172976 @ epoch 48\n",
      "NEW BEST TRAIN LOSS: 0.48625228088365297 @ epoch 48\n",
      "epoch 49\n",
      "training\n",
      "train loss 0.4869378977536722 l2 0.3481839523409088 mse 0.4869378977536722\n",
      "train loss 0.4872622430964791 l2 0.34854248573761626 mse 0.4872622430964791\n",
      "train loss 0.48932903398808336 l2 0.3489201545306916 mse 0.48932903398808336\n",
      "train loss 0.4896144591291685 l2 0.3493117880179473 mse 0.4896144591291685\n",
      "train loss 0.4884017228248668 l2 0.3497147928888557 mse 0.4884017228248668\n",
      "train loss 0.4887167941032136 l2 0.35012608410833274 mse 0.4887167941032136\n",
      "train loss 0.48863226328117654 l2 0.35053877909756725 mse 0.48863226328117654\n",
      "train loss 0.4871299655857242 l2 0.3509436204214836 mse 0.4871299655857242\n",
      "train loss 0.4869155207061265 l2 0.3513492302141895 mse 0.4869155207061265\n",
      "train loss 0.4866576872324155 l2 0.35175993593638677 mse 0.4866576872324155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.48883547372361974 l2 0.35217861026240027 mse 0.48883547372361974\n",
      "train loss 0.48703559515532635 l2 0.3526044745341234 mse 0.48703559515532635\n",
      "train loss 0.4921329137787639 l2 0.3530320685642139 mse 0.4921329137787639\n",
      "train loss 0.4907702461832901 l2 0.3534515357184507 mse 0.4907702461832901\n",
      "train loss 0.4904741558911813 l2 0.35384794139335535 mse 0.4904741558911813\n",
      "train loss 0.48913498757020424 l2 0.3542071448576372 mse 0.48913498757020424\n",
      "val loss 0.5164739511960234\n",
      "val acc 0.7225807629257188 val auc 0.7995162579309897\n",
      "epoch 50\n",
      "training\n",
      "train loss 0.4879042322352181 l2 0.35452365295486754 mse 0.4879042322352181\n",
      "train loss 0.48623017809719477 l2 0.35480796765839323 mse 0.48623017809719477\n",
      "train loss 0.4873012483122907 l2 0.35508159518254645 mse 0.4873012483122907\n",
      "train loss 0.4879026645457536 l2 0.3553650947311949 mse 0.4879026645457536\n",
      "train loss 0.48850255636828804 l2 0.3556800347630227 mse 0.48850255636828804\n",
      "train loss 0.49172619005708473 l2 0.35604480932991356 mse 0.49172619005708473\n",
      "train loss 0.4947693631801535 l2 0.3564571522980324 mse 0.4947693631801535\n",
      "train loss 0.49559364559759234 l2 0.3568931122077887 mse 0.49559364559759234\n",
      "train loss 0.49602308124687944 l2 0.35734492481467706 mse 0.49602308124687944\n",
      "train loss 0.49425732574739883 l2 0.35778341722308893 mse 0.49425732574739883\n",
      "train loss 0.49292409631649325 l2 0.35819141803635385 mse 0.49292409631649325\n",
      "train loss 0.4863551965551504 l2 0.358562423697443 mse 0.4863551965551504\n",
      "train loss 0.4880471341066134 l2 0.3589161720323381 mse 0.4880471341066134\n",
      "train loss 0.48641873443200845 l2 0.35927992157337524 mse 0.48641873443200845\n",
      "train loss 0.48931250026990786 l2 0.3596763516029609 mse 0.48931250026990786\n",
      "train loss 0.4936038350508152 l2 0.3601088213948152 mse 0.4936038350508152\n",
      "val loss 0.530659325974926\n",
      "val acc 0.6968886057236933 val auc 0.7941424641764065\n",
      "epoch 51\n",
      "training\n",
      "train loss 0.4989571920843706 l2 0.3605585196393514 mse 0.4989571920843706\n",
      "train loss 0.5023074534691605 l2 0.360991820042562 mse 0.5023074534691605\n",
      "train loss 0.5046354706738976 l2 0.3613693545465483 mse 0.5046354706738976\n",
      "train loss 0.5028389112319286 l2 0.361662020176546 mse 0.5028389112319286\n",
      "train loss 0.4971903176466042 l2 0.36186240524747243 mse 0.4971903176466042\n",
      "train loss 0.4923840429543672 l2 0.36198775051467713 mse 0.4923840429543672\n",
      "train loss 0.4889397470022578 l2 0.36207462860281625 mse 0.4889397470022578\n",
      "train loss 0.48732534906902 l2 0.36217891886494125 mse 0.48732534906902\n",
      "train loss 0.4901869127390078 l2 0.36234980163659086 mse 0.4901869127390078\n",
      "train loss 0.4953864580446493 l2 0.36261425275643827 mse 0.4953864580446493\n",
      "train loss 0.5035410929393264 l2 0.36297600051525697 mse 0.5035410929393264\n",
      "train loss 0.5072050791817233 l2 0.36341060829515553 mse 0.5072050791817233\n",
      "train loss 0.5156221051497635 l2 0.3638780415008993 mse 0.5156221051497635\n",
      "train loss 0.5146646166171283 l2 0.36431776122180864 mse 0.5146646166171283\n",
      "train loss 0.5119132707332166 l2 0.36469654923319045 mse 0.5119132707332166\n",
      "train loss 0.5043075141931074 l2 0.365004270941369 mse 0.5043075141931074\n",
      "val loss 0.517159699923877\n",
      "val acc 0.7195981932549117 val auc 0.7964747091053225\n",
      "epoch 52\n",
      "training\n",
      "train loss 0.49579851150536175 l2 0.3652634586797084 mse 0.49579851150536175\n",
      "train loss 0.48823986296717825 l2 0.3655123747944115 mse 0.48823986296717825\n",
      "train loss 0.4874238947455388 l2 0.36578561052275144 mse 0.4874238947455388\n",
      "train loss 0.4902289978017496 l2 0.3660967211091709 mse 0.4902289978017496\n",
      "train loss 0.49588172826948546 l2 0.3664420077209766 mse 0.49588172826948546\n",
      "train loss 0.5055299091514927 l2 0.3667938155909189 mse 0.5055299091514927\n",
      "train loss 0.5153719352814152 l2 0.3671022113970979 mse 0.5153719352814152\n",
      "train loss 0.5193690055073786 l2 0.3673287513283465 mse 0.5193690055073786\n",
      "train loss 0.5201127966259536 l2 0.36742954038220443 mse 0.5201127966259536\n",
      "train loss 0.5138054693383325 l2 0.3673993974413068 mse 0.5138054693383325\n",
      "train loss 0.5059289744425356 l2 0.36726480443721365 mse 0.5059289744425356\n",
      "train loss 0.49290715905332033 l2 0.3670842609967907 mse 0.49290715905332033\n",
      "train loss 0.4905526999945197 l2 0.36692609797288284 mse 0.4905526999945197\n",
      "train loss 0.4871396020764439 l2 0.3668396725049108 mse 0.4871396020764439\n",
      "train loss 0.4897059164925467 l2 0.3668535317105272 mse 0.4897059164925467\n",
      "train loss 0.4944921320901593 l2 0.3669679947103761 mse 0.4944921320901593\n",
      "val loss 0.5213927602688362\n",
      "val acc 0.7070860534362063 val auc 0.7959995978596436\n",
      "epoch 53\n",
      "training\n",
      "train loss 0.5003365050640004 l2 0.3671668256036888 mse 0.5003365050640004\n",
      "train loss 0.5047135512584086 l2 0.36742846428650083 mse 0.5047135512584086\n",
      "train loss 0.5090231927431206 l2 0.36772217610614655 mse 0.5090231927431206\n",
      "train loss 0.5092229328095399 l2 0.3680161116233469 mse 0.5092229328095399\n",
      "train loss 0.5050039295939213 l2 0.36829723789503166 mse 0.5050039295939213\n",
      "train loss 0.5001143943660474 l2 0.36857834947706347 mse 0.5001143943660474\n",
      "train loss 0.49463118234992254 l2 0.36887894984680936 mse 0.49463118234992254\n",
      "train loss 0.48949766578092313 l2 0.36920855266289965 mse 0.48949766578092313\n",
      "train loss 0.4877496256360064 l2 0.369583448019331 mse 0.4877496256360064\n",
      "train loss 0.4875394008145237 l2 0.36999332948102037 mse 0.4875394008145237\n",
      "train loss 0.4903044509795894 l2 0.3704195192329155 mse 0.4903044509795894\n",
      "train loss 0.48872697937440535 l2 0.3708423857540104 mse 0.48872697937440535\n",
      "train loss 0.49434367296628456 l2 0.37124942040045716 mse 0.49434367296628456\n",
      "train loss 0.4934439841830245 l2 0.3716262436332631 mse 0.4934439841830245\n",
      "train loss 0.4932764170790755 l2 0.37196981131962353 mse 0.4932764170790755\n",
      "train loss 0.49170730514923744 l2 0.37228192610924415 mse 0.49170730514923744\n",
      "val loss 0.516578955358533\n",
      "val acc 0.7210932288067021 val auc 0.8011632651963443\n",
      "NEW BEST AUC: 0.8011632651963443 @ epoch 53\n",
      "epoch 54\n",
      "training\n",
      "train loss 0.49015023094680077 l2 0.3725687533111116 mse 0.49015023094680077\n",
      "train loss 0.48787112163259533 l2 0.3728450411232206 mse 0.48787112163259533\n",
      "train loss 0.4877403473936792 l2 0.3731248971331815 mse 0.4877403473936792\n",
      "train loss 0.48698020048955326 l2 0.3734158881199428 mse 0.48698020048955326\n",
      "train loss 0.4860821181886817 l2 0.37371901644612404 mse 0.4860821181886817\n",
      "train loss 0.48724637363033857 l2 0.37402922621715506 mse 0.48724637363033857\n",
      "train loss 0.48810538374045953 l2 0.37434116720636895 mse 0.48810538374045953\n",
      "train loss 0.48730838098590323 l2 0.37464913753173046 mse 0.48730838098590323\n",
      "train loss 0.487316878698227 l2 0.3749552709973116 mse 0.487316878698227\n",
      "train loss 0.4865790438284771 l2 0.3752585737861061 mse 0.4865790438284771\n",
      "train loss 0.48758941029707165 l2 0.3755584356239453 mse 0.48758941029707165\n",
      "train loss 0.4841069260460105 l2 0.3758520151147339 mse 0.4841069260460105\n",
      "train loss 0.48753275562123377 l2 0.3761393613013554 mse 0.48753275562123377\n",
      "train loss 0.4853687570067709 l2 0.3764214495610995 mse 0.4853687570067709\n",
      "train loss 0.485249784169452 l2 0.3766978056565327 mse 0.485249784169452\n",
      "train loss 0.4850337328256422 l2 0.37696628629319534 mse 0.4850337328256422\n",
      "val loss 0.5143713695398382\n",
      "val acc 0.7250342315382238 val auc 0.8021836109465206\n",
      "NEW BEST AUC: 0.8021836109465206 @ epoch 54\n",
      "NEW BEST TRAIN LOSS: 0.4850337328256422 @ epoch 54\n",
      "epoch 55\n",
      "training\n",
      "train loss 0.4852217551412347 l2 0.3772265433305048 mse 0.4852217551412347\n",
      "train loss 0.4848634133408918 l2 0.37748089457913936 mse 0.4848634133408918\n",
      "train loss 0.4862660787890198 l2 0.37773363626240086 mse 0.4862660787890198\n",
      "train loss 0.4861451034682975 l2 0.37798524285406754 mse 0.4861451034682975\n",
      "train loss 0.48500498987994034 l2 0.3782386193294474 mse 0.48500498987994034\n",
      "train loss 0.4857774027367419 l2 0.3784956599341197 mse 0.4857774027367419\n",
      "train loss 0.48634115923197196 l2 0.3787553610618662 mse 0.48634115923197196\n",
      "train loss 0.48555501702082465 l2 0.3790180300155332 mse 0.48555501702082465\n",
      "train loss 0.4856226106904119 l2 0.3792854308102197 mse 0.4856226106904119\n",
      "train loss 0.48502688727046356 l2 0.3795574496595392 mse 0.48502688727046356\n",
      "train loss 0.4861710172737245 l2 0.37983270534297287 mse 0.4861710172737245\n",
      "train loss 0.4828957917304262 l2 0.3801102780834369 mse 0.4828957917304262\n",
      "train loss 0.48653930481198554 l2 0.3803901631038091 mse 0.48653930481198554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.48457675288754276 l2 0.3806723797368561 mse 0.48457675288754276\n",
      "train loss 0.4845148182734456 l2 0.3809555837281378 mse 0.4845148182734456\n",
      "train loss 0.48427582866485114 l2 0.3812385213030954 mse 0.48427582866485114\n",
      "val loss 0.5148164417327596\n",
      "val acc 0.7247159207408616 val auc 0.8019146588490469\n",
      "NEW BEST TRAIN LOSS: 0.48427582866485114 @ epoch 55\n",
      "epoch 56\n",
      "training\n",
      "train loss 0.4844646459578777 l2 0.3815204287294637 mse 0.4844646459578777\n",
      "train loss 0.48410747008437277 l2 0.3818028654433817 mse 0.48410747008437277\n",
      "train loss 0.48553643321846496 l2 0.3820879129252364 mse 0.48553643321846496\n",
      "train loss 0.4854199276723844 l2 0.3823741554884415 mse 0.4854199276723844\n",
      "train loss 0.48425535429925737 l2 0.38266373838965745 mse 0.48425535429925737\n",
      "train loss 0.48505845791167235 l2 0.3829582289377594 mse 0.48505845791167235\n",
      "train loss 0.4856466299510612 l2 0.3832565321917625 mse 0.4856466299510612\n",
      "train loss 0.48487544628031026 l2 0.3835575606350615 mse 0.48487544628031026\n",
      "train loss 0.48497676734227413 l2 0.3838626193120656 mse 0.48497676734227413\n",
      "train loss 0.48438685555452743 l2 0.38417066494280755 mse 0.48438685555452743\n",
      "train loss 0.48551997565151245 l2 0.3844808210326876 mse 0.48551997565151245\n",
      "train loss 0.4822673177436054 l2 0.3847914576043548 mse 0.4822673177436054\n",
      "train loss 0.4858861585809532 l2 0.3851022579515485 mse 0.4858861585809532\n",
      "train loss 0.48393087126335194 l2 0.3854136556476517 mse 0.48393087126335194\n",
      "train loss 0.48386174063611415 l2 0.38572357193747847 mse 0.48386174063611415\n",
      "train loss 0.4836310007636384 l2 0.3860305703903387 mse 0.4836310007636384\n",
      "val loss 0.5152507137375893\n",
      "val acc 0.7246256534998184 val auc 0.8017127323552181\n",
      "NEW BEST TRAIN LOSS: 0.4836310007636384 @ epoch 56\n",
      "epoch 57\n",
      "training\n",
      "train loss 0.4838482803195421 l2 0.3863341057141694 mse 0.4838482803195421\n",
      "train loss 0.48351296447873365 l2 0.38663520519898 mse 0.48351296447873365\n",
      "train loss 0.48498448587657633 l2 0.38693491210397585 mse 0.48498448587657633\n",
      "train loss 0.48487787961670636 l2 0.38723088462130706 mse 0.48487787961670636\n",
      "train loss 0.4836989047038743 l2 0.3875253019451864 mse 0.4836989047038743\n",
      "train loss 0.4845064724923027 l2 0.38781991770688273 mse 0.4845064724923027\n",
      "train loss 0.48507158455660027 l2 0.38811395663861453 mse 0.48507158455660027\n",
      "train loss 0.48429497050445275 l2 0.3884073905936344 mse 0.48429497050445275\n",
      "train loss 0.4844022798123637 l2 0.3887014693105442 mse 0.4844022798123637\n",
      "train loss 0.48380707476311546 l2 0.38899599236863414 mse 0.48380707476311546\n",
      "train loss 0.48493230263541204 l2 0.3892906039010046 mse 0.48493230263541204\n",
      "train loss 0.4817103709930751 l2 0.3895852837304639 mse 0.4817103709930751\n",
      "train loss 0.4853350308058671 l2 0.3898802576697704 mse 0.4853350308058671\n",
      "train loss 0.4834133589727106 l2 0.390177084757872 mse 0.4834133589727106\n",
      "train loss 0.48335956381159384 l2 0.39047415463918717 mse 0.48335956381159384\n",
      "train loss 0.4831372863267627 l2 0.3907705673491234 mse 0.4831372863267627\n",
      "val loss 0.5154881645839424\n",
      "val acc 0.7244558710713747 val auc 0.8014471445418108\n",
      "NEW BEST TRAIN LOSS: 0.4831372863267627 @ epoch 57\n",
      "epoch 58\n",
      "training\n",
      "train loss 0.48336304502419397 l2 0.39106630965408784 mse 0.48336304502419397\n",
      "train loss 0.4830163913976134 l2 0.39136267046196277 mse 0.4830163913976134\n",
      "train loss 0.484484237428196 l2 0.3916602591127024 mse 0.484484237428196\n",
      "train loss 0.48435042020244906 l2 0.39195614977247767 mse 0.48435042020244906\n",
      "train loss 0.48314521329179466 l2 0.39225255062889963 mse 0.48314521329179466\n",
      "train loss 0.48396701597026126 l2 0.39255100862317416 mse 0.48396701597026126\n",
      "train loss 0.48453155785338137 l2 0.39285084919113095 mse 0.48453155785338137\n",
      "train loss 0.483773743056538 l2 0.39315092099428983 mse 0.483773743056538\n",
      "train loss 0.48390778062418577 l2 0.39345235004705553 mse 0.48390778062418577\n",
      "train loss 0.48332963166624576 l2 0.39375445221527244 mse 0.48332963166624576\n",
      "train loss 0.48445291620355596 l2 0.39405646296991154 mse 0.48445291620355596\n",
      "train loss 0.48125189446534145 l2 0.3943582218975152 mse 0.48125189446534145\n",
      "train loss 0.48485686096659536 l2 0.3946594209793791 mse 0.48485686096659536\n",
      "train loss 0.4829414619148823 l2 0.3949615844985738 mse 0.4829414619148823\n",
      "train loss 0.482872482263757 l2 0.3952632184387731 mse 0.482872482263757\n",
      "train loss 0.4826358270903588 l2 0.39556347762786903 mse 0.4826358270903588\n",
      "val loss 0.515817522983128\n",
      "val acc 0.7244158634299152 val auc 0.8013074151873376\n",
      "NEW BEST TRAIN LOSS: 0.4826358270903588 @ epoch 58\n",
      "epoch 59\n",
      "training\n",
      "train loss 0.48286467790178156 l2 0.39586256097979294 mse 0.48286467790178156\n",
      "train loss 0.48252537981764826 l2 0.3961621673374591 mse 0.48252537981764826\n",
      "train loss 0.4840166401282777 l2 0.3964630669525479 mse 0.4840166401282777\n",
      "train loss 0.48388420567578216 l2 0.3967624852623992 mse 0.48388420567578216\n",
      "train loss 0.48268021056283256 l2 0.39706295663646757 mse 0.48268021056283256\n",
      "train loss 0.48351759590106536 l2 0.3973658183979202 mse 0.48351759590106536\n",
      "train loss 0.4840700407654005 l2 0.3976703030147055 mse 0.4840700407654005\n",
      "train loss 0.48330788938004626 l2 0.3979748223846351 mse 0.48330788938004626\n",
      "train loss 0.48344013453741974 l2 0.39828114097655726 mse 0.48344013453741974\n",
      "train loss 0.4828536008776971 l2 0.3985882319580382 mse 0.4828536008776971\n",
      "train loss 0.4839639862034339 l2 0.39889555302067436 mse 0.4839639862034339\n",
      "train loss 0.4807822952197714 l2 0.3992025911187126 mse 0.4807822952197714\n",
      "train loss 0.48438134516302606 l2 0.39950899455038774 mse 0.48438134516302606\n",
      "train loss 0.4824868913353391 l2 0.3998162264512363 mse 0.4824868913353391\n",
      "train loss 0.48242249406954457 l2 0.40012266608473024 mse 0.48242249406954457\n",
      "train loss 0.48218541841423185 l2 0.4004272874721763 mse 0.48218541841423185\n",
      "val loss 0.5161433739503413\n",
      "val acc 0.7242925898846679 val auc 0.8011435546312754\n",
      "NEW BEST TRAIN LOSS: 0.48218541841423185 @ epoch 59\n",
      "epoch 60\n",
      "training\n",
      "train loss 0.4824195333791632 l2 0.400730199915831 mse 0.4824195333791632\n",
      "train loss 0.4820786796464761 l2 0.4010330917812191 mse 0.4820786796464761\n",
      "train loss 0.48357586681101355 l2 0.4013367452068358 mse 0.48357586681101355\n",
      "train loss 0.48342942628936403 l2 0.40163882945776563 mse 0.48342942628936403\n",
      "train loss 0.4822125544682626 l2 0.4019419666483568 mse 0.4822125544682626\n",
      "train loss 0.4830575754394912 l2 0.4022477138184156 mse 0.4830575754394912\n",
      "train loss 0.48359752567132697 l2 0.40255531641338915 mse 0.48359752567132697\n",
      "train loss 0.48284714699573794 l2 0.402863529213476 mse 0.48284714699573794\n",
      "train loss 0.48299203960671594 l2 0.4031738788358116 mse 0.48299203960671594\n",
      "train loss 0.4824084682034087 l2 0.403485436269713 mse 0.4824084682034087\n",
      "train loss 0.4835145231753058 l2 0.4037976993804314 mse 0.4835145231753058\n",
      "train loss 0.48034821535621 l2 0.40411041852156343 mse 0.48034821535621\n",
      "train loss 0.4839361298475148 l2 0.40442321936730313 mse 0.4839361298475148\n",
      "train loss 0.4820447303571529 l2 0.40473734966913577 mse 0.4820447303571529\n",
      "train loss 0.4819711790366204 l2 0.4050511971844199 mse 0.4819711790366204\n",
      "train loss 0.48172517901930334 l2 0.40536384820153193 mse 0.48172517901930334\n",
      "val loss 0.5163710528849498\n",
      "val acc 0.7241505627574867 val auc 0.800969302545355\n",
      "NEW BEST TRAIN LOSS: 0.48172517901930334 @ epoch 60\n",
      "epoch 61\n",
      "training\n",
      "train loss 0.48196027951615655 l2 0.4056753696896487 mse 0.48196027951615655\n",
      "train loss 0.4816202052628852 l2 0.40598735242754 mse 0.4816202052628852\n",
      "train loss 0.4831310065632354 l2 0.4063001186158256 mse 0.4831310065632354\n",
      "train loss 0.4829875779915491 l2 0.4066109197805491 mse 0.4829875779915491\n",
      "train loss 0.4817703031593144 l2 0.40692254663298844 mse 0.4817703031593144\n",
      "train loss 0.4826274385946333 l2 0.4072362794802337 mse 0.4826274385946333\n",
      "train loss 0.48315109485602487 l2 0.4075512166116386 mse 0.48315109485602487\n",
      "train loss 0.4824041176500549 l2 0.4078666533948632 mse 0.4824041176500549\n",
      "train loss 0.48254756357805406 l2 0.4081835622857173 mse 0.48254756357805406\n",
      "train loss 0.48195665490539974 l2 0.40850135378029323 mse 0.48195665490539974\n",
      "train loss 0.48305012839120537 l2 0.40881974316563024 mse 0.48305012839120537\n",
      "train loss 0.4798995108317826 l2 0.4091386190084752 mse 0.4798995108317826\n",
      "train loss 0.48348203245922394 l2 0.4094579905680161 mse 0.48348203245922394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.4816069430473521 l2 0.4097792633352296 mse 0.4816069430473521\n",
      "train loss 0.4815391586651866 l2 0.4101009283216842 mse 0.4815391586651866\n",
      "train loss 0.48129150200268683 l2 0.4104218931896055 mse 0.48129150200268683\n",
      "val loss 0.5165737868500099\n",
      "val acc 0.7241543134738735 val auc 0.8008880414255892\n",
      "NEW BEST TRAIN LOSS: 0.48129150200268683 @ epoch 61\n",
      "epoch 62\n",
      "training\n",
      "train loss 0.4815246368231634 l2 0.4107423847409829 mse 0.4815246368231634\n",
      "train loss 0.48117770173974267 l2 0.41106419548115597 mse 0.48117770173974267\n",
      "train loss 0.48269099586508873 l2 0.4113874288382815 mse 0.48269099586508873\n",
      "train loss 0.4825358901959386 l2 0.4117091062358134 mse 0.4825358901959386\n",
      "train loss 0.48131022419425434 l2 0.4120317616095216 mse 0.48131022419425434\n",
      "train loss 0.4821766232134944 l2 0.41235668394476954 mse 0.4821766232134944\n",
      "train loss 0.4826891191724213 l2 0.4126828547022649 mse 0.4826891191724213\n",
      "train loss 0.48195694879985335 l2 0.4130092184970925 mse 0.48195694879985335\n",
      "train loss 0.48211216094551745 l2 0.413336608871862 mse 0.48211216094551745\n",
      "train loss 0.48152436749236827 l2 0.4136644104843623 mse 0.48152436749236827\n",
      "train loss 0.48260620969841966 l2 0.4139922083299324 mse 0.48260620969841966\n",
      "train loss 0.4794699491140709 l2 0.4143199481291985 mse 0.4794699491140709\n",
      "train loss 0.483035243492141 l2 0.41464734997804187 mse 0.483035243492141\n",
      "train loss 0.48116629199880023 l2 0.41497617488497024 mse 0.48116629199880023\n",
      "train loss 0.48109054386666433 l2 0.41530495898087816 mse 0.48109054386666433\n",
      "train loss 0.48083052845934643 l2 0.4156328286856535 mse 0.48083052845934643\n",
      "val loss 0.5168150041396214\n",
      "val acc 0.724132809366589 val auc 0.8008205003148655\n",
      "NEW BEST TRAIN LOSS: 0.48083052845934643 @ epoch 62\n",
      "epoch 63\n",
      "training\n",
      "train loss 0.4810649315102821 l2 0.41596031962998375 mse 0.4810649315102821\n",
      "train loss 0.4807206065764332 l2 0.41628936442713216 mse 0.4807206065764332\n",
      "train loss 0.4822513352920791 l2 0.4166206125756069 mse 0.4822513352920791\n",
      "train loss 0.4820964082294199 l2 0.41695117032262036 mse 0.4820964082294199\n",
      "train loss 0.48087460853832165 l2 0.41728352729222884 mse 0.48087460853832165\n",
      "train loss 0.48174952234406926 l2 0.417619069457681 mse 0.48174952234406926\n",
      "train loss 0.48224322135532655 l2 0.41795673385701354 mse 0.48224322135532655\n",
      "train loss 0.48150736848816444 l2 0.41829462919416904 mse 0.48150736848816444\n",
      "train loss 0.48165662592100156 l2 0.4186340922169975 mse 0.48165662592100156\n",
      "train loss 0.48105935568446734 l2 0.4189740165396585 mse 0.48105935568446734\n",
      "train loss 0.48212909045863145 l2 0.41931408105989815 mse 0.48212909045863145\n",
      "train loss 0.479015055946403 l2 0.41965405024763386 mse 0.479015055946403\n",
      "train loss 0.4825807388105923 l2 0.4199933520402304 mse 0.4825807388105923\n",
      "train loss 0.48073661768540554 l2 0.4203335942158746 mse 0.48073661768540554\n",
      "train loss 0.48066585053943867 l2 0.42067296653800684 mse 0.48066585053943867\n",
      "train loss 0.4803966339577134 l2 0.4210105567316291 mse 0.4803966339577134\n",
      "val loss 0.5171085514928889\n",
      "val acc 0.7239282702996273 val auc 0.8006521097445717\n",
      "NEW BEST TRAIN LOSS: 0.4803966339577134 @ epoch 63\n",
      "epoch 64\n",
      "training\n",
      "train loss 0.48062697296974277 l2 0.4213465573042822 mse 0.48062697296974277\n",
      "train loss 0.48027011180923906 l2 0.42168321283842736 mse 0.48027011180923906\n",
      "train loss 0.4817951887766012 l2 0.42202127171774223 mse 0.4817951887766012\n",
      "train loss 0.48162545416356256 l2 0.4223583583958384 mse 0.48162545416356256\n",
      "train loss 0.48039941565620625 l2 0.42269735468763503 mse 0.48039941565620625\n",
      "train loss 0.4812865707516693 l2 0.42303975512513153 mse 0.4812865707516693\n",
      "train loss 0.4817824812456102 l2 0.42338489306955573 mse 0.4817824812456102\n",
      "train loss 0.4810691799313981 l2 0.4237310404867053 mse 0.4810691799313981\n",
      "train loss 0.4812332825500133 l2 0.42407969074350266 mse 0.4812332825500133\n",
      "train loss 0.4806348599690754 l2 0.42442947257620856 mse 0.4806348599690754\n",
      "train loss 0.4816856042750304 l2 0.4247806082928424 mse 0.4816856042750304\n",
      "train loss 0.4785711982090106 l2 0.42513257057384707 mse 0.4785711982090106\n",
      "train loss 0.48211390958925626 l2 0.42548484542276716 mse 0.48211390958925626\n",
      "train loss 0.48026521246010684 l2 0.4258386399869521 mse 0.48026521246010684\n",
      "train loss 0.48018603634929824 l2 0.4261921011167466 mse 0.48018603634929824\n",
      "train loss 0.4799163868871899 l2 0.4265439153726923 mse 0.4799163868871899\n",
      "val loss 0.5173870731403538\n",
      "val acc 0.7237087283671181 val auc 0.8004241779689276\n",
      "NEW BEST TRAIN LOSS: 0.4799163868871899 @ epoch 64\n",
      "epoch 65\n",
      "training\n",
      "train loss 0.48016417909560566 l2 0.4268942451889879 mse 0.48016417909560566\n",
      "train loss 0.4798250055978893 l2 0.42724488189044946 mse 0.4798250055978893\n",
      "train loss 0.4813700927366986 l2 0.4275960634082631 mse 0.4813700927366986\n",
      "train loss 0.48120687484077407 l2 0.42794504792140514 mse 0.48120687484077407\n",
      "train loss 0.47997659872625215 l2 0.42829480558231153 mse 0.47997659872625215\n",
      "train loss 0.4808515610578412 l2 0.42864622446513184 mse 0.4808515610578412\n",
      "train loss 0.48130906716272503 l2 0.42899891887485586 mse 0.48130906716272503\n",
      "train loss 0.4805851271241085 l2 0.4293520994058165 mse 0.4805851271241085\n",
      "train loss 0.4807455907350613 l2 0.42970714005647476 mse 0.4807455907350613\n",
      "train loss 0.4801486652066775 l2 0.4300631934843991 mse 0.4801486652066775\n",
      "train loss 0.4812054801494758 l2 0.4304210459298206 mse 0.4812054801494758\n",
      "train loss 0.4781344417148484 l2 0.4307807697506112 mse 0.4781344417148484\n",
      "train loss 0.4816940966689813 l2 0.4311422654336942 mse 0.4816940966689813\n",
      "train loss 0.47986539780083254 l2 0.4315066295794571 mse 0.47986539780083254\n",
      "train loss 0.47978282747509204 l2 0.43187221805634446 mse 0.47978282747509204\n",
      "train loss 0.47948578698268063 l2 0.4322375480663913 mse 0.47948578698268063\n",
      "val loss 0.5175380076031997\n",
      "val acc 0.723798745560402 val auc 0.8003468639323333\n",
      "NEW BEST TRAIN LOSS: 0.47948578698268063 @ epoch 65\n",
      "epoch 66\n",
      "training\n",
      "train loss 0.4796989389114072 l2 0.4326026789064212 mse 0.4796989389114072\n",
      "train loss 0.47932295785695334 l2 0.4329695983683598 mse 0.47932295785695334\n",
      "train loss 0.4808578921011993 l2 0.4333378888997769 mse 0.4808578921011993\n",
      "train loss 0.4806976749384066 l2 0.43370389510152885 mse 0.4806976749384066\n",
      "train loss 0.4794969682332059 l2 0.43407032851610766 mse 0.4794969682332059\n",
      "train loss 0.4804224762780568 l2 0.434437553469308 mse 0.4804224762780568\n",
      "train loss 0.48090044036585894 l2 0.43480478602147576 mse 0.48090044036585894\n",
      "train loss 0.4802144154895131 l2 0.4351714627597893 mse 0.4802144154895131\n",
      "train loss 0.48037864633415417 l2 0.4355377613931588 mse 0.48037864633415417\n",
      "train loss 0.47974188439475485 l2 0.43590321836176926 mse 0.47974188439475485\n",
      "train loss 0.4807353649884439 l2 0.4362679822802858 mse 0.4807353649884439\n",
      "train loss 0.4776351783562844 l2 0.43663287239321275 mse 0.4776351783562844\n",
      "train loss 0.4811585440248023 l2 0.43699826377692536 mse 0.4811585440248023\n",
      "train loss 0.47934786512587196 l2 0.4373663816143066 mse 0.47934786512587196\n",
      "train loss 0.47930681315183077 l2 0.4377362811539515 mse 0.47930681315183077\n",
      "train loss 0.47906808318853555 l2 0.4381074830575204 mse 0.47906808318853555\n",
      "val loss 0.5178147554340053\n",
      "val acc 0.7238512555898177 val auc 0.8004165443797827\n",
      "NEW BEST TRAIN LOSS: 0.47906808318853555 @ epoch 66\n",
      "epoch 67\n",
      "training\n",
      "train loss 0.4793360720959981 l2 0.43848053057111347 mse 0.4793360720959981\n",
      "train loss 0.4789844215436561 l2 0.43885797309008256 mse 0.4789844215436561\n",
      "train loss 0.48052854531029093 l2 0.4392394454739325 mse 0.48052854531029093\n",
      "train loss 0.48032177890597844 l2 0.43962051119202916 mse 0.48032177890597844\n",
      "train loss 0.47904739992337303 l2 0.44000329866179844 mse 0.47904739992337303\n",
      "train loss 0.4799073386159136 l2 0.4403882038104159 mse 0.4799073386159136\n",
      "train loss 0.48032586418831374 l2 0.44077433033490043 mse 0.48032586418831374\n",
      "train loss 0.47965639680139566 l2 0.4411596971356427 mse 0.47965639680139566\n",
      "train loss 0.47988540106498967 l2 0.44154504715024717 mse 0.47988540106498967\n",
      "train loss 0.47934968527526894 l2 0.4419297542386514 mse 0.47934968527526894\n",
      "train loss 0.48042452011589104 l2 0.4423127023756295 mse 0.48042452011589104\n",
      "train loss 0.47740488443728996 l2 0.4426935903096047 mse 0.47740488443728996\n",
      "train loss 0.48090723080512326 l2 0.4430711416428046 mse 0.48090723080512326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.4790459767347069 l2 0.44344788948360336 mse 0.4790459767347069\n",
      "train loss 0.4788856589781378 l2 0.4438224068940503 mse 0.4788856589781378\n",
      "train loss 0.47852610367389353 l2 0.4441946099293294 mse 0.47852610367389353\n",
      "val loss 0.5180856248401018\n",
      "val acc 0.7238490051599855 val auc 0.8003374976528866\n",
      "NEW BEST TRAIN LOSS: 0.47852610367389353 @ epoch 67\n",
      "epoch 68\n",
      "training\n",
      "train loss 0.4787259248235964 l2 0.444566000904269 mse 0.4787259248235964\n",
      "train loss 0.47838058714303694 l2 0.44494049287121235 mse 0.47838058714303694\n",
      "train loss 0.48003537296457927 l2 0.4453203097490449 mse 0.48003537296457927\n",
      "train loss 0.4799722777549547 l2 0.4457029509682817 mse 0.4799722777549547\n",
      "train loss 0.4788552510400805 l2 0.446090884674236 mse 0.4788552510400805\n",
      "train loss 0.47983114707866786 l2 0.44648575177296995 mse 0.47983114707866786\n",
      "train loss 0.4802445843572034 l2 0.4468858056581605 mse 0.4802445843572034\n",
      "train loss 0.47946088212042054 l2 0.4472858758540927 mse 0.47946088212042054\n",
      "train loss 0.47949590299301836 l2 0.4476883163455374 mse 0.47949590299301836\n",
      "train loss 0.4787655070806485 l2 0.44809102353673713 mse 0.4787655070806485\n",
      "train loss 0.47971879418321745 l2 0.4484942525917181 mse 0.47971879418321745\n",
      "train loss 0.47674758040518106 l2 0.4488970382722121 mse 0.47674758040518106\n",
      "train loss 0.4804094658434476 l2 0.4492977777821612 mse 0.4804094658434476\n",
      "train loss 0.47884265354321875 l2 0.44969868059233364 mse 0.47884265354321875\n",
      "train loss 0.4789486130569325 l2 0.4500964039643095 mse 0.4789486130569325\n",
      "train loss 0.47872651749814105 l2 0.45048818148522196 mse 0.47872651749814105\n",
      "val loss 0.5189319016181226\n",
      "val acc 0.7226107686568134 val auc 0.7996905105820425\n",
      "epoch 69\n",
      "training\n",
      "train loss 0.4789152611438358 l2 0.4508734709567818 mse 0.4789152611438358\n",
      "train loss 0.47836822889298924 l2 0.45125330738579567 mse 0.47836822889298924\n",
      "train loss 0.4797091306152149 l2 0.4516304524785316 mse 0.4797091306152149\n",
      "train loss 0.4793416308860171 l2 0.4520045980690771 mse 0.4793416308860171\n",
      "train loss 0.4780668713949711 l2 0.45238136172400795 mse 0.4780668713949711\n",
      "train loss 0.4791105693439396 l2 0.4527647206243677 mse 0.4791105693439396\n",
      "train loss 0.47981889365877134 l2 0.4531564588533595 mse 0.47981889365877134\n",
      "train loss 0.47950162220550313 l2 0.4535540991907669 mse 0.47950162220550313\n",
      "train loss 0.47995928582978575 l2 0.4539613082294 mse 0.47995928582978575\n",
      "train loss 0.4794976841125056 l2 0.4543742324450758 mse 0.4794976841125056\n",
      "train loss 0.4803871624994827 l2 0.45479218571793284 mse 0.4803871624994827\n",
      "train loss 0.4770651369371191 l2 0.4552122120130962 mse 0.4770651369371191\n",
      "train loss 0.4801799002956555 l2 0.45563204214234143 mse 0.4801799002956555\n",
      "train loss 0.4780834106804276 l2 0.45605200197596135 mse 0.4780834106804276\n",
      "train loss 0.4779076914490147 l2 0.45647092616202983 mse 0.4779076914490147\n",
      "train loss 0.4778369380868854 l2 0.45688807814797794 mse 0.4778369380868854\n",
      "val loss 0.5204859510658832\n",
      "val acc 0.7197212167523998 val auc 0.7984815287555929\n",
      "NEW BEST TRAIN LOSS: 0.4778369380868854 @ epoch 69\n",
      "epoch 70\n",
      "training\n",
      "train loss 0.47861359990087504 l2 0.4573043240507413 mse 0.47861359990087504\n",
      "train loss 0.47889142583475813 l2 0.4577190569613186 mse 0.47889142583475813\n",
      "train loss 0.4809261572600183 l2 0.45813076334405206 mse 0.4809261572600183\n",
      "train loss 0.48096796588182017 l2 0.4585338228951218 mse 0.48096796588182017\n",
      "train loss 0.47955415445141597 l2 0.4589296509135466 mse 0.47955415445141597\n",
      "train loss 0.47990557076798707 l2 0.4593157417118814 mse 0.47990557076798707\n",
      "train loss 0.4796518152043588 l2 0.4596923464460165 mse 0.4796518152043588\n",
      "train loss 0.4784922936336085 l2 0.4600653491705778 mse 0.4784922936336085\n",
      "train loss 0.4786080235807117 l2 0.46044071519677604 mse 0.4786080235807117\n",
      "train loss 0.47849996706798154 l2 0.4608240033447896 mse 0.47849996706798154\n",
      "train loss 0.48037246275009465 l2 0.4612220669173124 mse 0.48037246275009465\n",
      "train loss 0.478412186385507 l2 0.4616382025904909 mse 0.478412186385507\n",
      "train loss 0.48270804307119725 l2 0.4620718662536724 mse 0.48270804307119725\n",
      "train loss 0.4812445195303935 l2 0.4625178919221757 mse 0.4812445195303935\n",
      "train loss 0.4809290943615751 l2 0.46296899922516727 mse 0.4809290943615751\n",
      "train loss 0.47973631855809956 l2 0.4634167369732273 mse 0.47973631855809956\n",
      "val loss 0.5191880961755544\n",
      "val acc 0.7224444868969974 val auc 0.7984924507083105\n",
      "epoch 71\n",
      "training\n",
      "train loss 0.4788284856110289 l2 0.46385645767836253 mse 0.4788284856110289\n",
      "train loss 0.4774881503328758 l2 0.46429014622566184 mse 0.4774881503328758\n",
      "train loss 0.478811941144166 l2 0.4647195716159889 mse 0.478811941144166\n",
      "train loss 0.4794288013637725 l2 0.4651458817639793 mse 0.4794288013637725\n",
      "train loss 0.47994843769977735 l2 0.465577481625223 mse 0.47994843769977735\n",
      "train loss 0.4829651679371645 l2 0.4660120163050786 mse 0.4829651679371645\n",
      "train loss 0.48504878119540906 l2 0.4664417342362218 mse 0.48504878119540906\n",
      "train loss 0.4849837121929401 l2 0.4668631671274342 mse 0.4849837121929401\n",
      "train loss 0.4844341472216711 l2 0.46725816273478266 mse 0.4844341472216711\n",
      "train loss 0.4819501067792643 l2 0.4676249514017745 mse 0.4819501067792643\n",
      "train loss 0.48075589892819365 l2 0.4679642829749538 mse 0.48075589892819365\n",
      "train loss 0.4762177459775259 l2 0.46829137278330624 mse 0.4762177459775259\n",
      "train loss 0.4795488346702168 l2 0.4686253332970918 mse 0.4795488346702168\n",
      "train loss 0.47916634929400703 l2 0.46898552347211814 mse 0.47916634929400703\n",
      "train loss 0.48184171013696464 l2 0.4693846581675116 mse 0.48184171013696464\n",
      "train loss 0.484624866813911 l2 0.4698258857488461 mse 0.484624866813911\n",
      "val loss 0.5269193267945776\n",
      "val acc 0.711979988177742 val auc 0.7977257890740631\n",
      "epoch 72\n",
      "training\n",
      "train loss 0.4876213400326888 l2 0.47030249604345664 mse 0.4876213400326888\n",
      "train loss 0.48865771984853074 l2 0.4708030827890757 mse 0.48865771984853074\n",
      "train loss 0.4898069835045042 l2 0.47130271301218774 mse 0.4898069835045042\n",
      "train loss 0.48762949659455734 l2 0.47177447707184234 mse 0.48762949659455734\n",
      "train loss 0.4829537016861513 l2 0.472204267296251 mse 0.4829537016861513\n",
      "train loss 0.4802470163270429 l2 0.4726001164001757 mse 0.4802470163270429\n",
      "train loss 0.4787705722085347 l2 0.4729807836552376 mse 0.4787705722085347\n",
      "train loss 0.479397918645353 l2 0.473366926421552 mse 0.479397918645353\n",
      "train loss 0.483835660391143 l2 0.4737744363426339 mse 0.483835660391143\n",
      "train loss 0.4889851217143404 l2 0.47420633367107995 mse 0.4889851217143404\n",
      "train loss 0.4950876429445557 l2 0.4746394441774143 mse 0.4950876429445557\n",
      "train loss 0.49530592630600395 l2 0.47504771333893714 mse 0.49530592630600395\n",
      "train loss 0.49870097794750634 l2 0.47540127669959853 mse 0.49870097794750634\n",
      "train loss 0.49308768967537037 l2 0.47568679283342125 mse 0.49308768967537037\n",
      "train loss 0.4872546294541285 l2 0.47590917540106065 mse 0.4872546294541285\n",
      "train loss 0.4812349842816166 l2 0.47609059783679325 mse 0.4812349842816166\n",
      "val loss 0.5202197155793614\n",
      "val acc 0.7196241982218604 val auc 0.7969301511692722\n",
      "epoch 73\n",
      "training\n",
      "train loss 0.47830365056356605 l2 0.4762682109400603 mse 0.47830365056356605\n",
      "train loss 0.47845425114481505 l2 0.476482873734974 mse 0.47845425114481505\n",
      "train loss 0.484187874382325 l2 0.47676859561818236 mse 0.484187874382325\n",
      "train loss 0.49076121907331655 l2 0.4771339294782854 mse 0.49076121907331655\n",
      "train loss 0.497035542445804 l2 0.4775663610009463 mse 0.497035542445804\n",
      "train loss 0.503945973241291 l2 0.47804190442077127 mse 0.503945973241291\n",
      "train loss 0.5073581559602923 l2 0.47851982546123695 mse 0.5073581559602923\n",
      "train loss 0.5054627724382709 l2 0.4789401469816687 mse 0.5054627724382709\n",
      "train loss 0.5002322273384513 l2 0.4793042615718184 mse 0.5002322273384513\n",
      "train loss 0.4912291663176513 l2 0.47960959655214164 mse 0.4912291663176513\n",
      "train loss 0.48431751233079084 l2 0.4798882594134737 mse 0.48431751233079084\n",
      "train loss 0.47678660454587857 l2 0.48016960888322185 mse 0.47678660454587857\n",
      "train loss 0.4808822604282205 l2 0.48047763987295455 mse 0.4808822604282205\n",
      "train loss 0.4845808851995782 l2 0.48081775691355416 mse 0.4845808851995782\n",
      "train loss 0.49272030678856327 l2 0.48117407378239285 mse 0.49272030678856327\n",
      "train loss 0.500463355588262 l2 0.4815158644509252 mse 0.500463355588262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 0.5371622029582673\n",
      "val acc 0.696619804382637 val auc 0.7945309206393956\n",
      "epoch 74\n",
      "training\n",
      "train loss 0.507122491449288 l2 0.481808100388516 mse 0.507122491449288\n",
      "train loss 0.5088851802075416 l2 0.4820174064350615 mse 0.5088851802075416\n",
      "train loss 0.5072325458021638 l2 0.48213283083003855 mse 0.5072325458021638\n",
      "train loss 0.4995922432091862 l2 0.4821667995900618 mse 0.4995922432091862\n",
      "train loss 0.4891556454603232 l2 0.48215687112244715 mse 0.4891556454603232\n",
      "train loss 0.482496060659826 l2 0.4821481430141183 mse 0.482496060659826\n",
      "train loss 0.4799187415878429 l2 0.4821821623967579 mse 0.4799187415878429\n",
      "train loss 0.48125382965679625 l2 0.4822885662367238 mse 0.48125382965679625\n",
      "train loss 0.4867292393903124 l2 0.48247498695747637 mse 0.4867292393903124\n",
      "train loss 0.49266200964903817 l2 0.4827236339135006 mse 0.49266200964903817\n",
      "train loss 0.4991968828728579 l2 0.4830113875480442 mse 0.4991968828728579\n",
      "train loss 0.49947399113778446 l2 0.4833066852796475 mse 0.49947399113778446\n",
      "train loss 0.5033021868237025 l2 0.48359289606533107 mse 0.5033021868237025\n",
      "train loss 0.49810008090224694 l2 0.4838571805306835 mse 0.49810008090224694\n",
      "train loss 0.4925324274622843 l2 0.48411102908014303 mse 0.4925324274622843\n",
      "train loss 0.4859397908057647 l2 0.484369268086328 mse 0.4859397908057647\n",
      "val loss 0.5166154754493197\n",
      "val acc 0.7229388313167815 val auc 0.799632748090075\n",
      "epoch 75\n",
      "training\n",
      "train loss 0.48114593082119506 l2 0.48464786394056514 mse 0.48114593082119506\n",
      "train loss 0.4781345436132767 l2 0.4849595761067868 mse 0.4781345436132767\n",
      "train loss 0.4799728359807557 l2 0.4853029246488494 mse 0.4799728359807557\n",
      "train loss 0.4819680889481352 l2 0.48566329736223146 mse 0.4819680889481352\n",
      "train loss 0.4838274969760842 l2 0.4860274167477339 mse 0.4838274969760842\n",
      "train loss 0.48756999562580494 l2 0.48637961999233564 mse 0.48756999562580494\n",
      "train loss 0.49036178468677016 l2 0.48670432711065603 mse 0.49036178468677016\n",
      "train loss 0.4894427998180226 l2 0.48699867460889085 mse 0.4894427998180226\n",
      "train loss 0.48778486253566183 l2 0.4872635282626088 mse 0.48778486253566183\n",
      "train loss 0.4841530572974775 l2 0.48750748857957493 mse 0.4841530572974775\n",
      "train loss 0.48229041793892696 l2 0.4877417992065409 mse 0.48229041793892696\n",
      "train loss 0.4768738298225574 l2 0.4879809652555046 mse 0.4768738298225574\n",
      "train loss 0.4793414250845787 l2 0.488235110401648 mse 0.4793414250845787\n",
      "train loss 0.47783940264859315 l2 0.4885060895087845 mse 0.47783940264859315\n",
      "train loss 0.4785982681434511 l2 0.48878972513575103 mse 0.4785982681434511\n",
      "train loss 0.4791410648811809 l2 0.4890798067448863 mse 0.4791410648811809\n",
      "val loss 0.5167981357485795\n",
      "val acc 0.7232066324668012 val auc 0.8003890201605321\n",
      "epoch 76\n",
      "training\n",
      "train loss 0.47991315209341 l2 0.4893721138607078 mse 0.47991315209341\n",
      "train loss 0.4797330293622946 l2 0.4896656533438869 mse 0.4797330293622946\n",
      "train loss 0.4812038736028957 l2 0.48995866330908866 mse 0.4812038736028957\n",
      "train loss 0.48055425451108036 l2 0.49024848119951314 mse 0.48055425451108036\n",
      "train loss 0.4787154569153702 l2 0.49053688471376894 mse 0.4787154569153702\n",
      "train loss 0.4787809775117013 l2 0.4908268173167472 mse 0.4787809775117013\n",
      "train loss 0.47851313505401233 l2 0.4911224652554603 mse 0.47851313505401233\n",
      "train loss 0.47754409424756533 l2 0.49142333324314424 mse 0.47754409424756533\n",
      "train loss 0.4776504526509803 l2 0.4917309826700507 mse 0.4776504526509803\n",
      "train loss 0.47719202011446943 l2 0.49204339963033206 mse 0.47719202011446943\n",
      "train loss 0.4782549832327763 l2 0.49235849533690756 mse 0.4782549832327763\n",
      "train loss 0.4752952685544314 l2 0.4926766078572289 mse 0.4752952685544314\n",
      "train loss 0.478741546214228 l2 0.4929985069886714 mse 0.478741546214228\n",
      "train loss 0.476991277743876 l2 0.4933247846814995 mse 0.476991277743876\n",
      "train loss 0.47685998773180843 l2 0.4936539666554238 mse 0.47685998773180843\n",
      "train loss 0.47642997463384457 l2 0.49398532360183867 mse 0.47642997463384457\n",
      "val loss 0.5175546869604494\n",
      "val acc 0.7242093239808803 val auc 0.8014474950000391\n",
      "NEW BEST TRAIN LOSS: 0.47642997463384457 @ epoch 76\n",
      "epoch 77\n",
      "training\n",
      "train loss 0.47654408822616745 l2 0.49431870214973744 mse 0.47654408822616745\n",
      "train loss 0.47601105676498395 l2 0.494655119905709 mse 0.47601105676498395\n",
      "train loss 0.47747875935603024 l2 0.4949955203899425 mse 0.47747875935603024\n",
      "train loss 0.4771953687851395 l2 0.4953377939027965 mse 0.4771953687851395\n",
      "train loss 0.47602387361627324 l2 0.495682957278142 mse 0.47602387361627324\n",
      "train loss 0.4769155707396533 l2 0.49603002166669513 mse 0.4769155707396533\n",
      "train loss 0.4772937839321072 l2 0.49637826904357635 mse 0.4772937839321072\n",
      "train loss 0.4766603715745094 l2 0.496726277565371 mse 0.4766603715745094\n",
      "train loss 0.4767849164220343 l2 0.4970746982970696 mse 0.4767849164220343\n",
      "train loss 0.4762005132186533 l2 0.4974227305576502 mse 0.4762005132186533\n",
      "train loss 0.4771137683628346 l2 0.497769842800853 mse 0.4771137683628346\n",
      "train loss 0.4741095480707469 l2 0.4981166162966446 mse 0.4741095480707469\n",
      "train loss 0.47747522434112083 l2 0.49846307176873245 mse 0.47747522434112083\n",
      "train loss 0.4757362375619151 l2 0.49881071573747326 mse 0.4757362375619151\n",
      "train loss 0.4756142606198551 l2 0.49915862564140445 mse 0.4756142606198551\n",
      "train loss 0.4752776640794563 l2 0.49950653586992266 mse 0.4752776640794563\n",
      "val loss 0.5181327652154015\n",
      "val acc 0.7239765295171378 val auc 0.8007282271755605\n",
      "NEW BEST TRAIN LOSS: 0.4752776640794563 @ epoch 77\n",
      "epoch 78\n",
      "training\n",
      "train loss 0.4755101242976202 l2 0.4998543807616342 mse 0.4755101242976202\n",
      "train loss 0.47508067989131936 l2 0.5002028298079619 mse 0.47508067989131936\n",
      "train loss 0.47667298741678327 l2 0.500552340938957 mse 0.47667298741678327\n",
      "train loss 0.47644654017270127 l2 0.5009008133721548 mse 0.47644654017270127\n",
      "train loss 0.4752727007757494 l2 0.5012499192766545 mse 0.4752727007757494\n",
      "train loss 0.476167770405576 l2 0.5015994615452295 mse 0.476167770405576\n",
      "train loss 0.47652740706794533 l2 0.5019490261378915 mse 0.47652740706794533\n",
      "train loss 0.475889365301627 l2 0.5022978500859347 mse 0.475889365301627\n",
      "train loss 0.47601302477693436 l2 0.5026467673922793 mse 0.47601302477693436\n",
      "train loss 0.4754175254515105 l2 0.5029953137756092 mse 0.4754175254515105\n",
      "train loss 0.47632296487924264 l2 0.5033432460554651 mse 0.47632296487924264\n",
      "train loss 0.47336402494989194 l2 0.5036915407832241 mse 0.47336402494989194\n",
      "train loss 0.4767547459487269 l2 0.5040400001095545 mse 0.4767547459487269\n",
      "train loss 0.4750486722624295 l2 0.5043900186287189 mse 0.4750486722624295\n",
      "train loss 0.4749460931156588 l2 0.5047402794003585 mse 0.4749460931156588\n",
      "train loss 0.4746054268639885 l2 0.5050908164800121 mse 0.4746054268639885\n",
      "val loss 0.5187771021826437\n",
      "val acc 0.7236219617947028 val auc 0.80019393796471\n",
      "NEW BEST TRAIN LOSS: 0.4746054268639885 @ epoch 78\n",
      "epoch 79\n",
      "training\n",
      "train loss 0.47485513284954156 l2 0.5054416894388615 mse 0.47485513284954156\n",
      "train loss 0.4744193137950086 l2 0.505793453779337 mse 0.4744193137950086\n",
      "train loss 0.47602626593377073 l2 0.5061463270534321 mse 0.47602626593377073\n",
      "train loss 0.4758001502902306 l2 0.5064979979723956 mse 0.4758001502902306\n",
      "train loss 0.4746221427009164 l2 0.5068500865050628 mse 0.4746221427009164\n",
      "train loss 0.47553212688670143 l2 0.5072026521914317 mse 0.47553212688670143\n",
      "train loss 0.47589605238511884 l2 0.5075554851872016 mse 0.47589605238511884\n",
      "train loss 0.47526841615784415 l2 0.5079072716837891 mse 0.47526841615784415\n",
      "train loss 0.47539826804366253 l2 0.5082591094115169 mse 0.47539826804366253\n",
      "train loss 0.4748087153458757 l2 0.5086103797499272 mse 0.4748087153458757\n",
      "train loss 0.4757089428258328 l2 0.5089610641334281 mse 0.4757089428258328\n",
      "train loss 0.4727694357586492 l2 0.509312184980186 mse 0.4727694357586492\n",
      "train loss 0.4761626825900314 l2 0.5096631267970251 mse 0.4761626825900314\n",
      "train loss 0.47446707190118886 l2 0.5100154751781035 mse 0.47446707190118886\n",
      "train loss 0.4743656791714907 l2 0.5103679431048108 mse 0.4743656791714907\n",
      "train loss 0.4740151924148506 l2 0.510720586223646 mse 0.4740151924148506\n",
      "val loss 0.5193653793824979\n",
      "val acc 0.7233274055344571 val auc 0.7998263632384486\n",
      "NEW BEST TRAIN LOSS: 0.4740151924148506 @ epoch 79\n",
      "epoch 80\n",
      "training\n",
      "train loss 0.47427394961869246 l2 0.5110736529910278 mse 0.47427394961869246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.47383645219146897 l2 0.5114276313122236 mse 0.47383645219146897\n",
      "train loss 0.47545779962423573 l2 0.5117831020484613 mse 0.47545779962423573\n",
      "train loss 0.4752320786482834 l2 0.5121376788651194 mse 0.4752320786482834\n",
      "train loss 0.4740520753719279 l2 0.5124931189892424 mse 0.4740520753719279\n",
      "train loss 0.4749700636718455 l2 0.5128496362126923 mse 0.4749700636718455\n",
      "train loss 0.47533433060102037 l2 0.5132068780207195 mse 0.47533433060102037\n",
      "train loss 0.47471172329629263 l2 0.513563037278696 mse 0.47471172329629263\n",
      "train loss 0.4748435612103502 l2 0.5139192041197339 mse 0.4748435612103502\n",
      "train loss 0.4742553858238507 l2 0.5142748891227309 mse 0.4742553858238507\n",
      "train loss 0.47514643662474376 l2 0.5146298903678999 mse 0.47514643662474376\n",
      "train loss 0.47222005750809226 l2 0.5149852363685208 mse 0.47222005750809226\n",
      "train loss 0.47561083172847596 l2 0.5153399889662349 mse 0.47561083172847596\n",
      "train loss 0.4739262588818156 l2 0.5156961106217849 mse 0.4739262588818156\n",
      "train loss 0.4738237527458603 l2 0.5160522763230734 mse 0.4738237527458603\n",
      "train loss 0.4734664941078314 l2 0.5164082729917744 mse 0.4734664941078314\n",
      "val loss 0.5199044152021868\n",
      "val acc 0.7230618548142695 val auc 0.799403236732845\n",
      "NEW BEST TRAIN LOSS: 0.4734664941078314 @ epoch 80\n",
      "epoch 81\n",
      "training\n",
      "train loss 0.47373592660691355 l2 0.5167644079669673 mse 0.47373592660691355\n",
      "train loss 0.47329388250293153 l2 0.5171213432083566 mse 0.47329388250293153\n",
      "train loss 0.47492771486516727 l2 0.5174795857764591 mse 0.47492771486516727\n",
      "train loss 0.47470008171803857 l2 0.5178369361898516 mse 0.47470008171803857\n",
      "train loss 0.47351826266100455 l2 0.518195092986316 mse 0.47351826266100455\n",
      "train loss 0.4744430771611284 l2 0.5185543932038676 mse 0.4744430771611284\n",
      "train loss 0.474802525454002 l2 0.5189143814547909 mse 0.474802525454002\n",
      "train loss 0.4741848481711282 l2 0.5192735389074347 mse 0.4741848481711282\n",
      "train loss 0.4743154206507384 l2 0.5196329730639417 mse 0.4743154206507384\n",
      "train loss 0.47372766732174026 l2 0.5199922027928472 mse 0.47372766732174026\n",
      "train loss 0.47461164125640143 l2 0.5203511441820041 mse 0.47461164125640143\n",
      "train loss 0.4716970705918823 l2 0.5207104583844266 mse 0.4716970705918823\n",
      "train loss 0.47508210978107734 l2 0.5210692967485427 mse 0.47508210978107734\n",
      "train loss 0.473407341932039 l2 0.5214294186859565 mse 0.473407341932039\n",
      "train loss 0.4733053261583889 l2 0.5217894428172829 mse 0.4733053261583889\n",
      "train loss 0.47294043180140793 l2 0.5221487568908604 mse 0.47294043180140793\n",
      "val loss 0.5204180918395104\n",
      "val acc 0.7227672985540238 val auc 0.7989856029595388\n",
      "NEW BEST TRAIN LOSS: 0.47294043180140793 @ epoch 81\n",
      "epoch 82\n",
      "training\n",
      "train loss 0.47321763853383214 l2 0.5225078741283509 mse 0.47321763853383214\n",
      "train loss 0.4727699113596898 l2 0.5228673840945792 mse 0.4727699113596898\n",
      "train loss 0.4744142091832529 l2 0.5232279054222418 mse 0.4744142091832529\n",
      "train loss 0.47418581271114935 l2 0.5235871567141471 mse 0.47418581271114935\n",
      "train loss 0.47300297291659904 l2 0.5239469861086549 mse 0.47300297291659904\n",
      "train loss 0.4739341955857597 l2 0.5243075150019292 mse 0.4739341955857597\n",
      "train loss 0.4742864775915077 l2 0.5246687368901629 mse 0.4742864775915077\n",
      "train loss 0.4736744981893359 l2 0.5250291367557381 mse 0.4736744981893359\n",
      "train loss 0.4738034653381083 l2 0.5253899551834947 mse 0.4738034653381083\n",
      "train loss 0.47321450915939345 l2 0.5257509731576917 mse 0.47321450915939345\n",
      "train loss 0.4740924721865548 l2 0.5261123142180504 mse 0.4740924721865548\n",
      "train loss 0.47118833951786043 l2 0.5264746538170639 mse 0.47118833951786043\n",
      "train loss 0.4745663068306678 l2 0.5268369589288265 mse 0.4745663068306678\n",
      "train loss 0.4728982505716849 l2 0.5272011128531269 mse 0.4728982505716849\n",
      "train loss 0.47280106350030504 l2 0.527565759976187 mse 0.47280106350030504\n",
      "train loss 0.47242854241674653 l2 0.5279302150406897 mse 0.47242854241674653\n",
      "val loss 0.520920371430199\n",
      "val acc 0.722605767701631 val auc 0.7986724808370774\n",
      "NEW BEST TRAIN LOSS: 0.47242854241674653 @ epoch 82\n",
      "epoch 83\n",
      "training\n",
      "train loss 0.4727092748990934 l2 0.528294660073615 mse 0.4727092748990934\n",
      "train loss 0.47225500910941326 l2 0.5286596258927045 mse 0.47225500910941326\n",
      "train loss 0.47390902531767165 l2 0.5290254158592508 mse 0.47390902531767165\n",
      "train loss 0.47368050128590933 l2 0.5293898202604002 mse 0.47368050128590933\n",
      "train loss 0.47249407390035303 l2 0.5297544493959435 mse 0.47249407390035303\n",
      "train loss 0.4734332499676322 l2 0.5301193610346787 mse 0.4734332499676322\n",
      "train loss 0.4737778760523666 l2 0.5304842489424764 mse 0.4737778760523666\n",
      "train loss 0.4731738254778176 l2 0.5308477850263804 mse 0.4731738254778176\n",
      "train loss 0.473299432427686 l2 0.5312110932533242 mse 0.473299432427686\n",
      "train loss 0.47271200961817716 l2 0.5315741270181403 mse 0.47271200961817716\n",
      "train loss 0.47358212287076196 l2 0.5319369226324194 mse 0.47358212287076196\n",
      "train loss 0.4706876786882652 l2 0.5323005543253232 mse 0.4706876786882652\n",
      "train loss 0.47405723521857546 l2 0.5326640076484548 mse 0.47405723521857546\n",
      "train loss 0.4723970222913592 l2 0.5330293009838272 mse 0.4723970222913592\n",
      "train loss 0.4723045514707689 l2 0.533395484937242 mse 0.4723045514707689\n",
      "train loss 0.4719246156329348 l2 0.5337617207699189 mse 0.4719246156329348\n",
      "val loss 0.521418381381799\n",
      "val acc 0.7223897264377496 val auc 0.7983678764960173\n",
      "NEW BEST TRAIN LOSS: 0.4719246156329348 @ epoch 83\n",
      "epoch 84\n",
      "training\n",
      "train loss 0.472207508323937 l2 0.5341286388390566 mse 0.472207508323937\n",
      "train loss 0.4717473749795609 l2 0.5344970413045069 mse 0.4717473749795609\n",
      "train loss 0.47341276488894424 l2 0.5348673823898598 mse 0.47341276488894424\n",
      "train loss 0.47318088907124156 l2 0.5352371478709673 mse 0.47318088907124156\n",
      "train loss 0.47198994195808386 l2 0.535607883424578 mse 0.47198994195808386\n",
      "train loss 0.47293773176316517 l2 0.5359798028964512 mse 0.47293773176316517\n",
      "train loss 0.47327419486935607 l2 0.5363521310678824 mse 0.47327419486935607\n",
      "train loss 0.47267449962273 l2 0.5367230901873655 mse 0.47267449962273\n",
      "train loss 0.4727963153222827 l2 0.5370935924006198 mse 0.4727963153222827\n",
      "train loss 0.4722121856062463 l2 0.5374633589841102 mse 0.4722121856062463\n",
      "train loss 0.473074292761054 l2 0.5378324852144922 mse 0.473074292761054\n",
      "train loss 0.47019321996144486 l2 0.5382012368430017 mse 0.47019321996144486\n",
      "train loss 0.4735554636208584 l2 0.5385685483491834 mse 0.4735554636208584\n",
      "train loss 0.47190411156721873 l2 0.5389366209407032 mse 0.47190411156721873\n",
      "train loss 0.47181256211366357 l2 0.5393046924341575 mse 0.47181256211366357\n",
      "train loss 0.47142491796696884 l2 0.5396719521924492 mse 0.47142491796696884\n",
      "val loss 0.5219341176814631\n",
      "val acc 0.7221651835500581 val auc 0.797979007074251\n",
      "NEW BEST TRAIN LOSS: 0.47142491796696884 @ epoch 84\n",
      "epoch 85\n",
      "training\n",
      "train loss 0.4717089092083078 l2 0.54003917664149 mse 0.4717089092083078\n",
      "train loss 0.4712412568443683 l2 0.540407438852683 mse 0.4712412568443683\n",
      "train loss 0.47291582421290457 l2 0.5407775968918884 mse 0.47291582421290457\n",
      "train loss 0.4726821874688382 l2 0.5411477419328828 mse 0.4726821874688382\n",
      "train loss 0.47148985421443906 l2 0.5415198235976074 mse 0.47148985421443906\n",
      "train loss 0.47244793380150796 l2 0.541894501602431 mse 0.47244793380150796\n",
      "train loss 0.4727801457708466 l2 0.542270915095326 mse 0.4727801457708466\n",
      "train loss 0.47218291500541376 l2 0.5426472165339251 mse 0.47218291500541376\n",
      "train loss 0.47229925823539287 l2 0.5430247523955973 mse 0.47229925823539287\n",
      "train loss 0.4717161315178893 l2 0.5434028791273665 mse 0.4717161315178893\n",
      "train loss 0.4725666324287223 l2 0.5437812783377756 mse 0.4725666324287223\n",
      "train loss 0.46969338723966364 l2 0.544160245117098 mse 0.46969338723966364\n",
      "train loss 0.47304897878576047 l2 0.5445380640652893 mse 0.47304897878576047\n",
      "train loss 0.47140755337480367 l2 0.5449164452038504 mse 0.47140755337480367\n",
      "train loss 0.4713187667901711 l2 0.545294177939085 mse 0.4713187667901711\n",
      "train loss 0.47092953259290005 l2 0.5456698977454071 mse 0.47092953259290005\n",
      "val loss 0.522473857257439\n",
      "val acc 0.7218261187886886 val auc 0.7975262241104992\n",
      "NEW BEST TRAIN LOSS: 0.47092953259290005 @ epoch 85\n",
      "epoch 86\n",
      "training\n",
      "train loss 0.4712183483757826 l2 0.5460440465501828 mse 0.4712183483757826\n",
      "train loss 0.47074881353006137 l2 0.5464172948869558 mse 0.47074881353006137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.47242547285580455 l2 0.5467903756607565 mse 0.47242547285580455\n",
      "train loss 0.47218980531127236 l2 0.5471618169647642 mse 0.47218980531127236\n",
      "train loss 0.47098898731653704 l2 0.5475340267330823 mse 0.47098898731653704\n",
      "train loss 0.47195208865217314 l2 0.5479074928025435 mse 0.47195208865217314\n",
      "train loss 0.47227551577474164 l2 0.5482822705230531 mse 0.47227551577474164\n",
      "train loss 0.4716862012421964 l2 0.548656977183234 mse 0.4716862012421964\n",
      "train loss 0.4718048345391679 l2 0.5490334772345372 mse 0.4718048345391679\n",
      "train loss 0.47122597800617405 l2 0.549411478361029 mse 0.47122597800617405\n",
      "train loss 0.47207366939052425 l2 0.5497913500078898 mse 0.47207366939052425\n",
      "train loss 0.4692112047036023 l2 0.550173700653758 mse 0.4692112047036023\n",
      "train loss 0.472554637970306 l2 0.5505571482350318 mse 0.472554637970306\n",
      "train loss 0.47091138370939317 l2 0.55094308528828 mse 0.47091138370939317\n",
      "train loss 0.47081958636364507 l2 0.5513303429990857 mse 0.47081958636364507\n",
      "train loss 0.4704221582710485 l2 0.5517172573640208 mse 0.4704221582710485\n",
      "val loss 0.5229560881176112\n",
      "val acc 0.7215520664446909 val auc 0.7971341682214454\n",
      "NEW BEST TRAIN LOSS: 0.4704221582710485 @ epoch 86\n",
      "epoch 87\n",
      "training\n",
      "train loss 0.470708625854059 l2 0.5521039535309858 mse 0.470708625854059\n",
      "train loss 0.47024107031818724 l2 0.5524903354251577 mse 0.47024107031818724\n",
      "train loss 0.47193162451398046 l2 0.5528761058068576 mse 0.47193162451398046\n",
      "train loss 0.4717046886574695 l2 0.5532589733727742 mse 0.4717046886574695\n",
      "train loss 0.47050871764227087 l2 0.5536408821657239 mse 0.47050871764227087\n",
      "train loss 0.47148169492162884 l2 0.5540215995812868 mse 0.47148169492162884\n",
      "train loss 0.4717844234505751 l2 0.5544010835667498 mse 0.4717844234505751\n",
      "train loss 0.4711956099717101 l2 0.5547784734554663 mse 0.4711956099717101\n",
      "train loss 0.47130426324171826 l2 0.5551554404979219 mse 0.47130426324171826\n",
      "train loss 0.4707179761353536 l2 0.5555322284129953 mse 0.4707179761353536\n",
      "train loss 0.47155672447788277 l2 0.5559097849508134 mse 0.47155672447788277\n",
      "train loss 0.46871504341844106 l2 0.5562893095932989 mse 0.46871504341844106\n",
      "train loss 0.47205704248097463 l2 0.5566700999169657 mse 0.47205704248097463\n",
      "train loss 0.4704279636463102 l2 0.5570541998517347 mse 0.4704279636463102\n",
      "train loss 0.47035628030520804 l2 0.5574409318185812 mse 0.47035628030520804\n",
      "train loss 0.46995129096043176 l2 0.5578295423469117 mse 0.46995129096043176\n",
      "val loss 0.5233864483052131\n",
      "val acc 0.7215058076092533 val auc 0.7969675201908248\n",
      "NEW BEST TRAIN LOSS: 0.46995129096043176 @ epoch 87\n",
      "epoch 88\n",
      "training\n",
      "train loss 0.470227993415227 l2 0.5582205288871439 mse 0.470227993415227\n",
      "train loss 0.469739264499535 l2 0.5586144956180646 mse 0.469739264499535\n",
      "train loss 0.4714264848134769 l2 0.5590109796291878 mse 0.4714264848134769\n",
      "train loss 0.47118493843429626 l2 0.5594072227689654 mse 0.47118493843429626\n",
      "train loss 0.46998823632580183 l2 0.5598041647383378 mse 0.46998823632580183\n",
      "train loss 0.47098408151643206 l2 0.560201202867843 mse 0.47098408151643206\n",
      "train loss 0.47129350514679247 l2 0.5605965766799375 mse 0.47129350514679247\n",
      "train loss 0.4707344414143973 l2 0.560988638149832 mse 0.4707344414143973\n",
      "train loss 0.4708541634875904 l2 0.5613778388126175 mse 0.4708541634875904\n",
      "train loss 0.4702677760114398 l2 0.5617641071878036 mse 0.4702677760114398\n",
      "train loss 0.47108203912009844 l2 0.562147692894134 mse 0.47108203912009844\n",
      "train loss 0.4682333938235695 l2 0.5625296658513543 mse 0.4682333938235695\n",
      "train loss 0.47153982745250506 l2 0.5629096297001008 mse 0.47153982745250506\n",
      "train loss 0.4699067250778899 l2 0.563290750212978 mse 0.4699067250778899\n",
      "train loss 0.4698435100072237 l2 0.5636730044280129 mse 0.4698435100072237\n",
      "train loss 0.469451301528772 l2 0.5640560580878244 mse 0.469451301528772\n",
      "val loss 0.5239102818194413\n",
      "val acc 0.7215183099972095 val auc 0.7968575601541464\n",
      "NEW BEST TRAIN LOSS: 0.469451301528772 @ epoch 88\n",
      "epoch 89\n",
      "training\n",
      "train loss 0.4697586780638534 l2 0.5644415161071805 mse 0.4697586780638534\n",
      "train loss 0.46928883548974837 l2 0.5648310432087249 mse 0.46928883548974837\n",
      "train loss 0.47101225469763425 l2 0.565224892666855 mse 0.47101225469763425\n",
      "train loss 0.47075380446561566 l2 0.5656211539701861 mse 0.47075380446561566\n",
      "train loss 0.4695284027747083 l2 0.5660212849543905 mse 0.4695284027747083\n",
      "train loss 0.4704946792290153 l2 0.5664259580682347 mse 0.4704946792290153\n",
      "train loss 0.4707656263854135 l2 0.5668332777193832 mse 0.4707656263854135\n",
      "train loss 0.47018972978774026 l2 0.5672406422730061 mse 0.47018972978774026\n",
      "train loss 0.4703230075897594 l2 0.567648208180927 mse 0.4703230075897594\n",
      "train loss 0.46977758888558613 l2 0.5680543488433187 mse 0.46977758888558613\n",
      "train loss 0.47063827671221115 l2 0.5684578252811691 mse 0.47063827671221115\n",
      "train loss 0.46785451781716103 l2 0.5688579720990902 mse 0.46785451781716103\n",
      "train loss 0.47116986772094455 l2 0.569252553925212 mse 0.47116986772094455\n",
      "train loss 0.46953250148011466 l2 0.5696443678279269 mse 0.46953250148011466\n",
      "train loss 0.46941222893179174 l2 0.5700326266234241 mse 0.46941222893179174\n",
      "train loss 0.46895166164938723 l2 0.5704166110240814 mse 0.46895166164938723\n",
      "val loss 0.5244766593201484\n",
      "val acc 0.7211494895525046 val auc 0.7964563964375261\n",
      "NEW BEST TRAIN LOSS: 0.46895166164938723 @ epoch 89\n",
      "epoch 90\n",
      "training\n",
      "train loss 0.4692149263628231 l2 0.5707990224339806 mse 0.4692149263628231\n",
      "train loss 0.4687225869018449 l2 0.5711817410044652 mse 0.4687225869018449\n",
      "train loss 0.4704975786444512 l2 0.5715665131111846 mse 0.4704975786444512\n",
      "train loss 0.47030598814936486 l2 0.5719529001054291 mse 0.47030598814936486\n",
      "train loss 0.4691661012383529 l2 0.5723434472943992 mse 0.4691661012383529\n",
      "train loss 0.47020339107697995 l2 0.572740448471165 mse 0.47020339107697995\n",
      "train loss 0.4704913941012471 l2 0.5731428878962855 mse 0.4704913941012471\n",
      "train loss 0.46987522609582005 l2 0.5735490301267727 mse 0.46987522609582005\n",
      "train loss 0.469912553724661 l2 0.5739609669748335 mse 0.469912553724661\n",
      "train loss 0.469256879830583 l2 0.5743770372305423 mse 0.469256879830583\n",
      "train loss 0.4700373090548278 l2 0.5747969456280068 mse 0.4700373090548278\n",
      "train loss 0.4672636466497287 l2 0.5752190204650277 mse 0.4672636466497287\n",
      "train loss 0.4706563282175898 l2 0.5756398521075389 mse 0.4706563282175898\n",
      "train loss 0.4691796543486748 l2 0.5760594157824391 mse 0.4691796543486748\n",
      "train loss 0.46919565211327846 l2 0.576474042542942 mse 0.46919565211327846\n",
      "train loss 0.4688261720483125 l2 0.576879950357722 mse 0.4688261720483125\n",
      "val loss 0.525435202543041\n",
      "val acc 0.7197007128361517 val auc 0.7953083625612566\n",
      "NEW BEST TRAIN LOSS: 0.4688261720483125 @ epoch 90\n",
      "epoch 91\n",
      "training\n",
      "train loss 0.46911725140803107 l2 0.5772775855585746 mse 0.46911725140803107\n",
      "train loss 0.4685105371651105 l2 0.5776664209223981 mse 0.4685105371651105\n",
      "train loss 0.4700972816685206 l2 0.5780488774704626 mse 0.4700972816685206\n",
      "train loss 0.4697377059455272 l2 0.5784265716369593 mse 0.4697377059455272\n",
      "train loss 0.4685129605242717 l2 0.5788047284557375 mse 0.4685129605242717\n",
      "train loss 0.46961369001539743 l2 0.5791864363725927 mse 0.46961369001539743\n",
      "train loss 0.4700911279185137 l2 0.5795734807013411 mse 0.4700911279185137\n",
      "train loss 0.46974150928133984 l2 0.5799650670153311 mse 0.46974150928133984\n",
      "train loss 0.46999450052965036 l2 0.5803648396596893 mse 0.46999450052965036\n",
      "train loss 0.46944851101139795 l2 0.5807708691823261 mse 0.46944851101139795\n",
      "train loss 0.47015845558207386 l2 0.581184974047677 mse 0.47015845558207386\n",
      "train loss 0.4671755756391418 l2 0.5816073817656723 mse 0.4671755756391418\n",
      "train loss 0.470234025064378 l2 0.5820366100114749 mse 0.470234025064378\n",
      "train loss 0.46849658863021654 l2 0.5824726970214852 mse 0.46849658863021654\n",
      "train loss 0.46842654862825633 l2 0.5829133014656013 mse 0.46842654862825633\n",
      "train loss 0.4682112733489346 l2 0.5833540139327563 mse 0.4682112733489346\n",
      "val loss 0.5269431492886095\n",
      "val acc 0.7171547265527716 val auc 0.7937402885639667\n",
      "NEW BEST TRAIN LOSS: 0.4682112733489346 @ epoch 91\n",
      "epoch 92\n",
      "training\n",
      "train loss 0.46889476822553317 l2 0.5837924112353924 mse 0.46889476822553317\n",
      "train loss 0.4687910910015251 l2 0.5842234783895819 mse 0.4687910910015251\n",
      "train loss 0.47072004150692565 l2 0.5846425327286977 mse 0.47072004150692565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.4704915328021812 l2 0.5850458060857184 mse 0.4704915328021812\n",
      "train loss 0.46903098105880736 l2 0.5854359034772365 mse 0.46903098105880736\n",
      "train loss 0.4696304301997253 l2 0.5858120214464356 mse 0.4696304301997253\n",
      "train loss 0.4695268871590477 l2 0.5861801224933743 mse 0.4695268871590477\n",
      "train loss 0.4688529616684792 l2 0.5865462618878469 mse 0.4688529616684792\n",
      "train loss 0.4691782604033424 l2 0.5869167772376244 mse 0.4691782604033424\n",
      "train loss 0.4690793969242718 l2 0.5872949973892246 mse 0.4690793969242718\n",
      "train loss 0.4705210794984757 l2 0.5876850977825071 mse 0.4705210794984757\n",
      "train loss 0.46829116521093733 l2 0.5880883998546163 mse 0.46829116521093733\n",
      "train loss 0.47174965923265116 l2 0.5885032929672449 mse 0.47174965923265116\n",
      "train loss 0.4699553357357721 l2 0.588929086781492 mse 0.4699553357357721\n",
      "train loss 0.46948886782985105 l2 0.5893651775654163 mse 0.46948886782985105\n",
      "train loss 0.4684183852258808 l2 0.5898092981869701 mse 0.4684183852258808\n",
      "val loss 0.526437929971159\n",
      "val acc 0.718895809099538 val auc 0.7943637483229948\n",
      "epoch 93\n",
      "training\n",
      "train loss 0.46818711515820893 l2 0.5902614783555059 mse 0.46818711515820893\n",
      "train loss 0.4675014654696094 l2 0.590722475881279 mse 0.4675014654696094\n",
      "train loss 0.46964321766604117 l2 0.5911899505242859 mse 0.46964321766604117\n",
      "train loss 0.4704361972740229 l2 0.591657677647718 mse 0.4704361972740229\n",
      "train loss 0.47047813361006535 l2 0.5921219976876716 mse 0.47047813361006535\n",
      "train loss 0.4725113744203152 l2 0.5925705538645758 mse 0.4725113744203152\n",
      "train loss 0.47306122471560125 l2 0.5929943229497642 mse 0.47306122471560125\n",
      "train loss 0.47214863103126503 l2 0.5933911229733425 mse 0.47214863103126503\n",
      "train loss 0.4710754698525925 l2 0.5937539255821552 mse 0.4710754698525925\n",
      "train loss 0.46920319632971963 l2 0.5940925685842442 mse 0.46920319632971963\n",
      "train loss 0.46916578689953126 l2 0.5944179762357438 mse 0.46916578689953126\n",
      "train loss 0.4665499419032497 l2 0.5947469827898629 mse 0.4665499419032497\n",
      "train loss 0.4710417619594883 l2 0.5950930554884363 mse 0.4710417619594883\n",
      "train loss 0.4712274578504898 l2 0.5954677460499108 mse 0.4712274578504898\n",
      "train loss 0.4731801665148836 l2 0.5958731741607209 mse 0.4731801665148836\n",
      "train loss 0.4739740723619271 l2 0.5963045350521287 mse 0.4739740723619271\n",
      "val loss 0.5301241253830375\n",
      "val acc 0.7163715769712015 val auc 0.794498874428935\n",
      "epoch 94\n",
      "training\n",
      "train loss 0.4747159153540627 l2 0.5967533354957173 mse 0.4747159153540627\n",
      "train loss 0.4733568743167456 l2 0.5972144942802715 mse 0.4733568743167456\n",
      "train loss 0.47329554205420316 l2 0.5976802846014296 mse 0.47329554205420316\n",
      "train loss 0.47107031771125624 l2 0.5981445379961803 mse 0.47107031771125624\n",
      "train loss 0.4684882157243741 l2 0.5986083698282635 mse 0.4684882157243741\n",
      "train loss 0.469522853284563 l2 0.5990806937062042 mse 0.469522853284563\n",
      "train loss 0.47168662689777047 l2 0.5995632545967359 mse 0.47168662689777047\n",
      "train loss 0.47460087396396256 l2 0.6000587314865221 mse 0.47460087396396256\n",
      "train loss 0.47850568447202524 l2 0.6005520190693999 mse 0.47850568447202524\n",
      "train loss 0.48076585053917514 l2 0.6010292286823358 mse 0.48076585053917514\n",
      "train loss 0.48203997627352657 l2 0.6014632803890143 mse 0.48203997627352657\n",
      "train loss 0.4776837949931602 l2 0.6018369135152881 mse 0.4776837949931602\n",
      "train loss 0.477036423069767 l2 0.6021443849699836 mse 0.477036423069767\n",
      "train loss 0.471469774080008 l2 0.602403416761088 mse 0.471469774080008\n",
      "train loss 0.46935527958390705 l2 0.6026386085549904 mse 0.46935527958390705\n",
      "train loss 0.4698925539781484 l2 0.6028824720321773 mse 0.4698925539781484\n",
      "val loss 0.5374013138873098\n",
      "val acc 0.7053229666866372 val auc 0.7913775077736211\n",
      "epoch 95\n",
      "training\n",
      "train loss 0.4741094935516588 l2 0.603167857211369 mse 0.4741094935516588\n",
      "train loss 0.4793066804044175 l2 0.6035165400435031 mse 0.4793066804044175\n",
      "train loss 0.48615808696659685 l2 0.6039260061209576 mse 0.48615808696659685\n",
      "train loss 0.4894492805734973 l2 0.6043746527103896 mse 0.4894492805734973\n",
      "train loss 0.48882031055404157 l2 0.6048308107992073 mse 0.48882031055404157\n",
      "train loss 0.4872532636020191 l2 0.605280576560598 mse 0.4872532636020191\n",
      "train loss 0.48264939795895107 l2 0.6057092674763798 mse 0.48264939795895107\n",
      "train loss 0.4775145209613547 l2 0.6061080815776392 mse 0.4775145209613547\n",
      "train loss 0.4744753600207071 l2 0.6064937211695296 mse 0.4744753600207071\n",
      "train loss 0.47382319993344335 l2 0.6068771942444097 mse 0.47382319993344335\n",
      "train loss 0.4784893579079651 l2 0.6072727505979688 mse 0.4784893579079651\n",
      "train loss 0.4818866141015144 l2 0.6076854852982888 mse 0.4818866141015144\n",
      "train loss 0.49314634885232606 l2 0.6081125624018051 mse 0.49314634885232606\n",
      "train loss 0.49777647840011885 l2 0.608534978410185 mse 0.49777647840011885\n",
      "train loss 0.5005853915413551 l2 0.6089202564328238 mse 0.5005853915413551\n",
      "train loss 0.4981077297678403 l2 0.6092300774032091 mse 0.4981077297678403\n",
      "val loss 0.5325253354283137\n",
      "val acc 0.7071813216324317 val auc 0.7848393866406403\n",
      "epoch 96\n",
      "training\n",
      "train loss 0.4920692031126518 l2 0.6094488457748233 mse 0.4920692031126518\n",
      "train loss 0.48187201601061846 l2 0.609579747768407 mse 0.48187201601061846\n",
      "train loss 0.47619222159806884 l2 0.6096613518695364 mse 0.47619222159806884\n",
      "train loss 0.47489439334061445 l2 0.6097367888083208 mse 0.47489439334061445\n",
      "train loss 0.4796511171124072 l2 0.6098429546260402 mse 0.4796511171124072\n",
      "train loss 0.49143249429177194 l2 0.6099937284346778 mse 0.49143249429177194\n",
      "train loss 0.504602156594532 l2 0.6101878125805679 mse 0.504602156594532\n",
      "train loss 0.5127769239676118 l2 0.6103834991109586 mse 0.5127769239676118\n",
      "train loss 0.51608144116929 l2 0.6105698174254105 mse 0.51608144116929\n",
      "train loss 0.5112211355762943 l2 0.6107315016030153 mse 0.5112211355762943\n",
      "train loss 0.5030583650510543 l2 0.6108840758254673 mse 0.5030583650510543\n",
      "train loss 0.48885906239553 l2 0.6110411211254853 mse 0.48885906239553\n",
      "train loss 0.4820020019864413 l2 0.611220273043812 mse 0.4820020019864413\n",
      "train loss 0.47514130096073054 l2 0.6114329086478435 mse 0.47514130096073054\n",
      "train loss 0.4762745940445824 l2 0.6116830204057813 mse 0.4762745940445824\n",
      "train loss 0.48167352329645013 l2 0.6119567922914642 mse 0.48167352329645013\n",
      "val loss 0.5450491846520596\n",
      "val acc 0.6966923182327824 val auc 0.7867855171243775\n",
      "epoch 97\n",
      "training\n",
      "train loss 0.49125055920989813 l2 0.6122353377881838 mse 0.49125055920989813\n",
      "train loss 0.5007774415901602 l2 0.6124929110500255 mse 0.5007774415901602\n",
      "train loss 0.5108046713445894 l2 0.6127090363515443 mse 0.5108046713445894\n",
      "train loss 0.5144062826611283 l2 0.612863685290387 mse 0.5144062826611283\n",
      "train loss 0.5100980980603992 l2 0.6129503727985445 mse 0.5100980980603992\n",
      "train loss 0.5008825563218838 l2 0.6129736043209527 mse 0.5008825563218838\n",
      "train loss 0.4889184512993139 l2 0.6129584078381073 mse 0.4889184512993139\n",
      "train loss 0.4776173114465357 l2 0.6129460559705596 mse 0.4776173114465357\n",
      "train loss 0.4728873470492414 l2 0.6129660238398978 mse 0.4728873470492414\n",
      "train loss 0.47305948724393765 l2 0.6130322807910042 mse 0.47305948724393765\n",
      "train loss 0.4784710819639972 l2 0.6131457060818901 mse 0.4784710819639972\n",
      "train loss 0.4820597733602834 l2 0.6132919469405504 mse 0.4820597733602834\n",
      "train loss 0.4915044700906757 l2 0.6134508037378423 mse 0.4915044700906757\n",
      "train loss 0.49342138852701756 l2 0.613598319024089 mse 0.49342138852701756\n",
      "train loss 0.4935128256957601 l2 0.6137249608071857 mse 0.4935128256957601\n",
      "train loss 0.4894430921902031 l2 0.6138297343438953 mse 0.4894430921902031\n",
      "val loss 0.5240277097887457\n",
      "val acc 0.7143841973816999 val auc 0.7937021046388009\n",
      "epoch 98\n",
      "training\n",
      "train loss 0.4842972058703166 l2 0.6139226716534727 mse 0.4842972058703166\n",
      "train loss 0.47802694806832857 l2 0.6140213775094899 mse 0.47802694806832857\n",
      "train loss 0.4746784997326224 l2 0.614136276518011 mse 0.4746784997326224\n",
      "train loss 0.4719214674701196 l2 0.6142756154945652 mse 0.4719214674701196\n",
      "train loss 0.47079253512075364 l2 0.6144395006161655 mse 0.47079253512075364\n",
      "train loss 0.4730497242843973 l2 0.6146194451930087 mse 0.4730497242843973\n",
      "train loss 0.47457438101771815 l2 0.6148034017002777 mse 0.47457438101771815\n",
      "train loss 0.4746928439205683 l2 0.6149873797038653 mse 0.4746928439205683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.4751989399128556 l2 0.6151678767521432 mse 0.4751989399128556\n",
      "train loss 0.4743956207494237 l2 0.615346114844796 mse 0.4743956207494237\n",
      "train loss 0.47457525561200803 l2 0.6155230606718723 mse 0.47457525561200803\n",
      "train loss 0.4705321003078213 l2 0.6157024052795627 mse 0.4705321003078213\n",
      "train loss 0.4725903674824463 l2 0.6158879575773669 mse 0.4725903674824463\n",
      "train loss 0.46996399151464396 l2 0.6160827101340748 mse 0.46996399151464396\n",
      "train loss 0.4694500432340532 l2 0.6162870763651127 mse 0.4694500432340532\n",
      "train loss 0.4689819286198926 l2 0.6164999223538703 mse 0.4689819286198926\n",
      "val loss 0.5209943484194073\n",
      "val acc 0.7231316181390646 val auc 0.7988681491018581\n",
      "epoch 99\n",
      "training\n",
      "train loss 0.46933707004036956 l2 0.6167205574445545 mse 0.46933707004036956\n",
      "train loss 0.4688731254311888 l2 0.6169495382531618 mse 0.4688731254311888\n",
      "train loss 0.4706621695254813 l2 0.617187210249433 mse 0.4706621695254813\n",
      "train loss 0.4703760106447728 l2 0.6174327251430646 mse 0.4703760106447728\n",
      "train loss 0.4691526710689113 l2 0.6176855505560027 mse 0.4691526710689113\n",
      "train loss 0.4700103784285752 l2 0.6179447794674513 mse 0.4700103784285752\n",
      "train loss 0.4700897324949742 l2 0.6182074738435902 mse 0.4700897324949742\n",
      "train loss 0.46936470643057254 l2 0.6184719821554117 mse 0.46936470643057254\n",
      "train loss 0.4692248952226993 l2 0.6187382215563794 mse 0.4692248952226993\n",
      "train loss 0.46849801528752844 l2 0.6190063974451993 mse 0.46849801528752844\n",
      "train loss 0.46922752675319557 l2 0.6192765705735083 mse 0.46922752675319557\n",
      "train loss 0.46643902499900414 l2 0.6195487500085937 mse 0.46643902499900414\n",
      "train loss 0.46969112526791773 l2 0.6198212133725061 mse 0.46969112526791773\n",
      "train loss 0.4681324861719621 l2 0.6200935033560979 mse 0.4681324861719621\n",
      "train loss 0.4680375365500648 l2 0.6203633999848891 mse 0.4680375365500648\n",
      "train loss 0.46758758321273425 l2 0.6206290177090399 mse 0.46758758321273425\n",
      "val loss 0.522622219030284\n",
      "val acc 0.7216728395123468 val auc 0.7975260228771738\n",
      "NEW BEST TRAIN LOSS: 0.46758758321273425 @ epoch 99\n",
      "epoch 100\n",
      "training\n",
      "train loss 0.4678057002122335 l2 0.6208908853369475 mse 0.4678057002122335\n",
      "train loss 0.46712061744571703 l2 0.6211495337246028 mse 0.46712061744571703\n",
      "train loss 0.4687478345202676 l2 0.6214075184293522 mse 0.4687478345202676\n",
      "train loss 0.4684128400411421 l2 0.6216659967214707 mse 0.4684128400411421\n",
      "train loss 0.4672267960237208 l2 0.6219264912952573 mse 0.4672267960237208\n",
      "train loss 0.4681857595044408 l2 0.6221889780065445 mse 0.4681857595044408\n",
      "train loss 0.46834661468464045 l2 0.622453230064835 mse 0.46834661468464045\n",
      "train loss 0.4677410983676582 l2 0.6227199245311171 mse 0.4677410983676582\n",
      "train loss 0.46779868540176933 l2 0.6229897529247421 mse 0.46779868540176933\n",
      "train loss 0.4671893961891279 l2 0.6232632308192085 mse 0.4671893961891279\n",
      "train loss 0.46799217403492577 l2 0.6235409603720632 mse 0.46799217403492577\n",
      "train loss 0.4652528643251513 l2 0.6238233956267765 mse 0.4652528643251513\n",
      "train loss 0.4684787155275759 l2 0.6241099673995579 mse 0.4684787155275759\n",
      "train loss 0.46686316584632576 l2 0.6244003682276967 mse 0.46686316584632576\n",
      "train loss 0.466741527208053 l2 0.6246928904794434 mse 0.466741527208053\n",
      "train loss 0.4662949756980556 l2 0.6249860605789538 mse 0.4662949756980556\n",
      "val loss 0.5239247271273637\n",
      "val acc 0.7215940744682234 val auc 0.7970366976549917\n",
      "NEW BEST TRAIN LOSS: 0.4662949756980556 @ epoch 100\n",
      "epoch 101\n",
      "training\n",
      "train loss 0.4665682530237446 l2 0.625279632751093 mse 0.4665682530237446\n",
      "train loss 0.4659518216716078 l2 0.625572804591881 mse 0.4659518216716078\n",
      "train loss 0.4676726190906983 l2 0.6258652571980328 mse 0.4676726190906983\n",
      "train loss 0.4673816073818467 l2 0.626155726000308 mse 0.4673816073818467\n",
      "train loss 0.46620831252837963 l2 0.6264442042762224 mse 0.46620831252837963\n",
      "train loss 0.4672283624012587 l2 0.6267301937603693 mse 0.4672283624012587\n",
      "train loss 0.4674210283809445 l2 0.6270130828134812 mse 0.4674210283809445\n",
      "train loss 0.46687563781206287 l2 0.6272930763644238 mse 0.46687563781206287\n",
      "train loss 0.466969336782816 l2 0.6275704407453669 mse 0.466969336782816\n",
      "train loss 0.46635416799082186 l2 0.6278456748377528 mse 0.46635416799082186\n",
      "train loss 0.46714477080132705 l2 0.6281194183099198 mse 0.46714477080132705\n",
      "train loss 0.46441059543415136 l2 0.6283926965984603 mse 0.46441059543415136\n",
      "train loss 0.46764242024959257 l2 0.6286651733597042 mse 0.46764242024959257\n",
      "train loss 0.4660553287737068 l2 0.6289383114128098 mse 0.4660553287737068\n",
      "train loss 0.4659397118799564 l2 0.6292112304516926 mse 0.4659397118799564\n",
      "train loss 0.4654875137900415 l2 0.6294837812916889 mse 0.4654875137900415\n",
      "val loss 0.5252064922197665\n",
      "val acc 0.7208229271790912 val auc 0.7961793187139591\n",
      "NEW BEST TRAIN LOSS: 0.4654875137900415 @ epoch 101\n",
      "epoch 102\n",
      "training\n",
      "train loss 0.4657846927786046 l2 0.6297569096980005 mse 0.4657846927786046\n",
      "train loss 0.4651911319824765 l2 0.6300306039828913 mse 0.4651911319824765\n",
      "train loss 0.4669524112826071 l2 0.6303052981658939 mse 0.4669524112826071\n",
      "train loss 0.4666818204010338 l2 0.6305802810085983 mse 0.4666818204010338\n",
      "train loss 0.46550365294338086 l2 0.630856195153806 mse 0.46550365294338086\n",
      "train loss 0.46653572510329244 l2 0.6311332469624579 mse 0.46653572510329244\n",
      "train loss 0.46672750566992977 l2 0.6314111316227872 mse 0.46672750566992977\n",
      "train loss 0.46619658436168526 l2 0.6316897456750261 mse 0.46619658436168526\n",
      "train loss 0.46631049017260906 l2 0.6319694988437488 mse 0.46631049017260906\n",
      "train loss 0.46568406097156956 l2 0.632249715218417 mse 0.46568406097156956\n",
      "train loss 0.46646901385537864 l2 0.6325308889890908 mse 0.46646901385537864\n",
      "train loss 0.4637467668263648 l2 0.6328133138572821 mse 0.4637467668263648\n",
      "train loss 0.4669767762344743 l2 0.6330960383539129 mse 0.4669767762344743\n",
      "train loss 0.4654072655826206 l2 0.6333799335735278 mse 0.4654072655826206\n",
      "train loss 0.46530075705818424 l2 0.63366346069171 mse 0.46530075705818424\n",
      "train loss 0.46485421776446545 l2 0.6339458609505909 mse 0.46485421776446545\n",
      "val loss 0.5261643559331063\n",
      "val acc 0.7201155420685351 val auc 0.7953535573999745\n",
      "NEW BEST TRAIN LOSS: 0.46485421776446545 @ epoch 102\n",
      "epoch 103\n",
      "training\n",
      "train loss 0.4651697186685899 l2 0.6342277789671191 mse 0.4651697186685899\n",
      "train loss 0.46457757047621295 l2 0.6345086885515515 mse 0.46457757047621295\n",
      "train loss 0.46635079124010376 l2 0.6347887048715168 mse 0.46635079124010376\n",
      "train loss 0.46608395935473323 l2 0.6350670671447366 mse 0.46608395935473323\n",
      "train loss 0.4648944840056229 l2 0.6353448433161969 mse 0.4648944840056229\n",
      "train loss 0.46593548970569293 l2 0.6356221297071497 mse 0.46593548970569293\n",
      "train loss 0.46611552322718736 l2 0.6358990119869671 mse 0.46611552322718736\n",
      "train loss 0.46560905104195616 l2 0.6361758888927536 mse 0.46560905104195616\n",
      "train loss 0.4657399777202156 l2 0.636453341381372 mse 0.4657399777202156\n",
      "train loss 0.4651130518626455 l2 0.6367311536990824 mse 0.4651130518626455\n",
      "train loss 0.46590270504744297 l2 0.6370101990809387 mse 0.46590270504744297\n",
      "train loss 0.4631965381173751 l2 0.6372909475844052 mse 0.4631965381173751\n",
      "train loss 0.4664163650830365 l2 0.6375728177818258 mse 0.4664163650830365\n",
      "train loss 0.46484602026345584 l2 0.6378568796516585 mse 0.46484602026345584\n",
      "train loss 0.4647348602009022 l2 0.6381418407426834 mse 0.4647348602009022\n",
      "train loss 0.46428011290894633 l2 0.6384268209693633 mse 0.46428011290894633\n",
      "val loss 0.5269391787785136\n",
      "val acc 0.7198467407274789 val auc 0.7948798604336363\n",
      "NEW BEST TRAIN LOSS: 0.46428011290894633 @ epoch 103\n",
      "epoch 104\n",
      "training\n",
      "train loss 0.46459651841279154 l2 0.6387123050878502 mse 0.46459651841279154\n",
      "train loss 0.4640017583235307 l2 0.63899779366064 mse 0.4640017583235307\n",
      "train loss 0.4657955348967053 l2 0.6392829521924446 mse 0.4657955348967053\n",
      "train loss 0.4655480416969309 l2 0.639566534101765 mse 0.4655480416969309\n",
      "train loss 0.4643625966627897 l2 0.639849296270464 mse 0.4643625966627897\n",
      "train loss 0.46541872701941545 l2 0.6401310993012705 mse 0.46541872701941545\n",
      "train loss 0.46558608404695684 l2 0.6404116698000544 mse 0.46558608404695684\n",
      "train loss 0.46509137904688896 l2 0.6406913468248854 mse 0.46509137904688896\n",
      "train loss 0.4652229357361439 l2 0.6409704507422611 mse 0.4652229357361439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.46457952559415255 l2 0.6412489765053913 mse 0.46457952559415255\n",
      "train loss 0.46536128130236265 l2 0.6415277814055392 mse 0.46536128130236265\n",
      "train loss 0.4626686866492371 l2 0.6418076356592805 mse 0.4626686866492371\n",
      "train loss 0.465884767849135 l2 0.6420879385863221 mse 0.465884767849135\n",
      "train loss 0.4643290492231014 l2 0.6423703344187909 mse 0.4643290492231014\n",
      "train loss 0.4642265837697254 l2 0.6426537844165565 mse 0.4642265837697254\n",
      "train loss 0.463775131766719 l2 0.6429377078428676 mse 0.463775131766719\n",
      "val loss 0.5276483563038732\n",
      "val acc 0.7197262177075822 val auc 0.7944698398463805\n",
      "NEW BEST TRAIN LOSS: 0.463775131766719 @ epoch 104\n",
      "epoch 105\n",
      "training\n",
      "train loss 0.4640957511481016 l2 0.6432229076366591 mse 0.4640957511481016\n",
      "train loss 0.4634909283947054 l2 0.6435091716772509 mse 0.4634909283947054\n",
      "train loss 0.46529046463578816 l2 0.6437960757917128 mse 0.46529046463578816\n",
      "train loss 0.46504106570200565 l2 0.6440821877438482 mse 0.46504106570200565\n",
      "train loss 0.4638465917582071 l2 0.6443681971546601 mse 0.4638465917582071\n",
      "train loss 0.46491012361549183 l2 0.6446539059307334 mse 0.46491012361549183\n",
      "train loss 0.4650706329118665 l2 0.6449388080558441 mse 0.4650706329118665\n",
      "train loss 0.4645945126819323 l2 0.6452227202420788 mse 0.4645945126819323\n",
      "train loss 0.4647401062964062 l2 0.6455057044571957 mse 0.4647401062964062\n",
      "train loss 0.4640963573241365 l2 0.6457874249399527 mse 0.4640963573241365\n",
      "train loss 0.464875976201839 l2 0.6460683877751596 mse 0.464875976201839\n",
      "train loss 0.46219571604078746 l2 0.6463493485057769 mse 0.46219571604078746\n",
      "train loss 0.46539688186671724 l2 0.6466294696976729 mse 0.46539688186671724\n",
      "train loss 0.4638432560715374 l2 0.646910514768853 mse 0.4638432560715374\n",
      "train loss 0.4637305067927916 l2 0.6471916677544598 mse 0.4637305067927916\n",
      "train loss 0.4632728802132098 l2 0.6474725451946749 mse 0.4632728802132098\n",
      "val loss 0.5283832976915444\n",
      "val acc 0.7193473953525124 val auc 0.7939211716612568\n",
      "NEW BEST TRAIN LOSS: 0.4632728802132098 @ epoch 105\n",
      "epoch 106\n",
      "training\n",
      "train loss 0.46360020838381977 l2 0.6477542985315604 mse 0.46360020838381977\n",
      "train loss 0.46299285886197367 l2 0.6480370440367611 mse 0.46299285886197367\n",
      "train loss 0.4648111276248772 l2 0.6483208238883806 mse 0.4648111276248772\n",
      "train loss 0.46457877836116596 l2 0.6486045929702497 mse 0.46457877836116596\n",
      "train loss 0.4633873414491526 l2 0.6488892922259546 mse 0.4633873414491526\n",
      "train loss 0.46445657665983975 l2 0.6491750002223922 mse 0.46445657665983975\n",
      "train loss 0.4645980010906112 l2 0.64946116754495 mse 0.4645980010906112\n",
      "train loss 0.46412140527239304 l2 0.6497476732248412 mse 0.46412140527239304\n",
      "train loss 0.4642599109512293 l2 0.6500345922829271 mse 0.4642599109512293\n",
      "train loss 0.4636052753442417 l2 0.6503211538435136 mse 0.4636052753442417\n",
      "train loss 0.46438033177955346 l2 0.6506077440711326 mse 0.46438033177955346\n",
      "train loss 0.46171991318968664 l2 0.6508948327621175 mse 0.46171991318968664\n",
      "train loss 0.46492407887718207 l2 0.6511809907967986 mse 0.46492407887718207\n",
      "train loss 0.46339468400198486 l2 0.6514677754417508 mse 0.46339468400198486\n",
      "train loss 0.46329175485803503 l2 0.6517536363880941 mse 0.46329175485803503\n",
      "train loss 0.4628321861463048 l2 0.6520377451609325 mse 0.4628321861463048\n",
      "val loss 0.5292346728448079\n",
      "val acc 0.7187530318290793 val auc 0.7933962868333226\n",
      "NEW BEST TRAIN LOSS: 0.4628321861463048 @ epoch 106\n",
      "epoch 107\n",
      "training\n",
      "train loss 0.46315567130990876 l2 0.6523211058588483 mse 0.46315567130990876\n",
      "train loss 0.46252683235132963 l2 0.6526037631064826 mse 0.46252683235132963\n",
      "train loss 0.4643340864959757 l2 0.6528859508747248 mse 0.4643340864959757\n",
      "train loss 0.464096849233171 l2 0.6531669796237446 mse 0.464096849233171\n",
      "train loss 0.46290534109393305 l2 0.6534484943781381 mse 0.46290534109393305\n",
      "train loss 0.46399255373858556 l2 0.6537306448259962 mse 0.46399255373858556\n",
      "train loss 0.4641413464198473 l2 0.6540137427338938 mse 0.4641413464198473\n",
      "train loss 0.4636958650743113 l2 0.6542978044076992 mse 0.4636958650743113\n",
      "train loss 0.46384900942484925 l2 0.6545835032951041 mse 0.46384900942484925\n",
      "train loss 0.4631910549883264 l2 0.6548700876298108 mse 0.4631910549883264\n",
      "train loss 0.4639505341740957 l2 0.6551584155116964 mse 0.4639505341740957\n",
      "train loss 0.4612778241721299 l2 0.655448937429004 mse 0.4612778241721299\n",
      "train loss 0.46444942627357805 l2 0.6557402260403452 mse 0.46444942627357805\n",
      "train loss 0.46291087526851177 l2 0.6560335165906201 mse 0.46291087526851177\n",
      "train loss 0.46280529202183274 l2 0.6563270438275259 mse 0.46280529202183274\n",
      "train loss 0.46236159496997536 l2 0.6566195805141112 mse 0.46236159496997536\n",
      "val loss 0.5300473826330031\n",
      "val acc 0.7182574371704996 val auc 0.7929336460672286\n",
      "NEW BEST TRAIN LOSS: 0.46236159496997536 @ epoch 107\n",
      "epoch 108\n",
      "training\n",
      "train loss 0.46271325275315106 l2 0.656911629309055 mse 0.46271325275315106\n",
      "train loss 0.4621088735345174 l2 0.657202585210248 mse 0.4621088735345174\n",
      "train loss 0.4639402324282133 l2 0.6574920655327949 mse 0.4639402324282133\n",
      "train loss 0.46371186801412584 l2 0.6577789128169838 mse 0.46371186801412584\n",
      "train loss 0.4625061766707921 l2 0.6580644910379937 mse 0.4625061766707921\n",
      "train loss 0.4635652500139731 l2 0.658348599281476 mse 0.4635652500139731\n",
      "train loss 0.4636670788334491 l2 0.6586316923032159 mse 0.4636670788334491\n",
      "train loss 0.46321107939771755 l2 0.6589143457142026 mse 0.46321107939771755\n",
      "train loss 0.4633667783090006 l2 0.659197572727013 mse 0.4633667783090006\n",
      "train loss 0.46272540114366745 l2 0.6594812239999663 mse 0.46272540114366745\n",
      "train loss 0.46352014562737165 l2 0.6597665634054181 mse 0.46352014562737165\n",
      "train loss 0.4609018019612818 l2 0.6600545921150134 mse 0.4609018019612818\n",
      "train loss 0.46409379356700947 l2 0.660344436716488 mse 0.46409379356700947\n",
      "train loss 0.46256306691402044 l2 0.660637545694907 mse 0.46256306691402044\n",
      "train loss 0.4624274181457758 l2 0.6609327422976479 mse 0.4624274181457758\n",
      "train loss 0.4619384555541142 l2 0.6612288609103214 mse 0.4619384555541142\n",
      "val loss 0.5305150190535329\n",
      "val acc 0.718221430293186 val auc 0.7925773183953381\n",
      "NEW BEST TRAIN LOSS: 0.4619384555541142 @ epoch 108\n",
      "epoch 109\n",
      "training\n",
      "train loss 0.46224109618899173 l2 0.6615261372270449 mse 0.46224109618899173\n",
      "train loss 0.461597705860602 l2 0.6618244903440395 mse 0.461597705860602\n",
      "train loss 0.4634346839400094 l2 0.6621231416206317 mse 0.4634346839400094\n",
      "train loss 0.4632441489622982 l2 0.662419939055042 mse 0.4632441489622982\n",
      "train loss 0.46210155178361584 l2 0.6627159513474303 mse 0.46210155178361584\n",
      "train loss 0.4632286488949538 l2 0.6630102554122821 mse 0.4632286488949538\n",
      "train loss 0.4633648214962305 l2 0.6633020703845299 mse 0.4633648214962305\n",
      "train loss 0.4629156710508289 l2 0.6635913582635891 mse 0.4629156710508289\n",
      "train loss 0.4630371890373597 l2 0.6638785204592115 mse 0.4630371890373597\n",
      "train loss 0.4623192308945836 l2 0.664163611667755 mse 0.4623192308945836\n",
      "train loss 0.4630381094523796 l2 0.6644477772085543 mse 0.4630381094523796\n",
      "train loss 0.4603878515995003 l2 0.6647324515461371 mse 0.4603878515995003\n",
      "train loss 0.4635713907916182 l2 0.6650176238122375 mse 0.4635713907916182\n",
      "train loss 0.4621053109666832 l2 0.6653055449280993 mse 0.4621053109666832\n",
      "train loss 0.46205795891391477 l2 0.6655959582324049 mse 0.46205795891391477\n",
      "train loss 0.4616570060088067 l2 0.6658885097364269 mse 0.4616570060088067\n",
      "val loss 0.5310710428574558\n",
      "val acc 0.7184029649663085 val auc 0.7924016477769389\n",
      "NEW BEST TRAIN LOSS: 0.4616570060088067 @ epoch 109\n",
      "epoch 110\n",
      "training\n",
      "train loss 0.46201985721367145 l2 0.6661837003229021 mse 0.46201985721367145\n",
      "train loss 0.46137504092529735 l2 0.6664820231796821 mse 0.46137504092529735\n",
      "train loss 0.4631598923138429 l2 0.666782502391082 mse 0.4631598923138429\n",
      "train loss 0.4628722960245776 l2 0.6670831272436786 mse 0.4628722960245776\n",
      "train loss 0.4616148140504223 l2 0.6673842692826464 mse 0.4616148140504223\n",
      "train loss 0.46267729160716303 l2 0.6676854612231121 mse 0.46267729160716303\n",
      "train loss 0.46281337378317366 l2 0.6679857274385277 mse 0.46281337378317366\n",
      "train loss 0.46246261693426943 l2 0.6682842781680827 mse 0.46246261693426943\n",
      "train loss 0.4627280808244964 l2 0.668580460814827 mse 0.4627280808244964\n",
      "train loss 0.46214581254073384 l2 0.6688735542020741 mse 0.46214581254073384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.46294875122339807 l2 0.6691635582292526 mse 0.46294875122339807\n",
      "train loss 0.46031139954993794 l2 0.6694511196155609 mse 0.46031139954993794\n",
      "train loss 0.46338106291642617 l2 0.6697354623087669 mse 0.46338106291642617\n",
      "train loss 0.4617533922483024 l2 0.6700189348052071 mse 0.4617533922483024\n",
      "train loss 0.46153786810838776 l2 0.6703015593590612 mse 0.46153786810838776\n",
      "train loss 0.46104235984660863 l2 0.6705840319258024 mse 0.46104235984660863\n",
      "val loss 0.5317332868175557\n",
      "val acc 0.7181851733681133 val auc 0.7920820096338309\n",
      "NEW BEST TRAIN LOSS: 0.46104235984660863 @ epoch 110\n",
      "epoch 111\n",
      "training\n",
      "train loss 0.46142430430226866 l2 0.6708687362312504 mse 0.46142430430226866\n",
      "train loss 0.4609180388209818 l2 0.6711576980097561 mse 0.4609180388209818\n",
      "train loss 0.46293553541710536 l2 0.6714512772023707 mse 0.46293553541710536\n",
      "train loss 0.4628856718448971 l2 0.6717483209088407 mse 0.4628856718448971\n",
      "train loss 0.4617712745705737 l2 0.6720490818998828 mse 0.4617712745705737\n",
      "train loss 0.46281765475392556 l2 0.6723532223558675 mse 0.46281765475392556\n",
      "train loss 0.4627712184853535 l2 0.6726599410976591 mse 0.4627712184853535\n",
      "train loss 0.46214157315134197 l2 0.6729673844298377 mse 0.46214157315134197\n",
      "train loss 0.4621496527804212 l2 0.6732751488749501 mse 0.4621496527804212\n",
      "train loss 0.4614278744287541 l2 0.673581383798504 mse 0.4614278744287541\n",
      "train loss 0.46229119850565403 l2 0.6738863144478612 mse 0.46229119850565403\n",
      "train loss 0.45993319010488515 l2 0.6741894116392951 mse 0.45993319010488515\n",
      "train loss 0.46338392917069676 l2 0.6744883199120087 mse 0.46338392917069676\n",
      "train loss 0.462143996065395 l2 0.6747837283603961 mse 0.462143996065395\n",
      "train loss 0.4621213612588035 l2 0.6750734896065111 mse 0.4621213612588035\n",
      "train loss 0.46153447272489656 l2 0.6753560335578528 mse 0.46153447272489656\n",
      "val loss 0.532885409060308\n",
      "val acc 0.716932184047153 val auc 0.791179190398234\n",
      "epoch 112\n",
      "training\n",
      "train loss 0.46159227383260626 l2 0.675633723564241 mse 0.46159227383260626\n",
      "train loss 0.46063184043302074 l2 0.6759076893034043 mse 0.46063184043302074\n",
      "train loss 0.46225621367626546 l2 0.6761809156967356 mse 0.46225621367626546\n",
      "train loss 0.4620190618844683 l2 0.6764555008962713 mse 0.4620190618844683\n",
      "train loss 0.46107245174821654 l2 0.6767357523974336 mse 0.46107245174821654\n",
      "train loss 0.4626195617778304 l2 0.6770241350816012 mse 0.4626195617778304\n",
      "train loss 0.46321462197717367 l2 0.677322262131279 mse 0.46321462197717367\n",
      "train loss 0.4631424240738233 l2 0.6776287606887628 mse 0.4631424240738233\n",
      "train loss 0.4633896728333219 l2 0.6779441946194495 mse 0.4633896728333219\n",
      "train loss 0.4624911302894772 l2 0.6782641719376974 mse 0.4624911302894772\n",
      "train loss 0.4628021041424201 l2 0.6785884485984884 mse 0.4628021041424201\n",
      "train loss 0.45968500823325154 l2 0.6789149296725606 mse 0.45968500823325154\n",
      "train loss 0.46245999325889164 l2 0.6792404050383068 mse 0.46245999325889164\n",
      "train loss 0.46099321565233514 l2 0.67956493039011 mse 0.46099321565233514\n",
      "train loss 0.4613459903861246 l2 0.679886407414275 mse 0.4613459903861246\n",
      "train loss 0.46166794850931125 l2 0.6802016628718992 mse 0.46166794850931125\n",
      "val loss 0.5366309968263934\n",
      "val acc 0.7122822959185204 val auc 0.7897435044013787\n",
      "epoch 113\n",
      "training\n",
      "train loss 0.46285347615045697 l2 0.6805100810663657 mse 0.46285347615045697\n",
      "train loss 0.46277149721500577 l2 0.6808077794845095 mse 0.46277149721500577\n",
      "train loss 0.46465680242363533 l2 0.6810934210252141 mse 0.46465680242363533\n",
      "train loss 0.4639961190155353 l2 0.6813662305691265 mse 0.4639961190155353\n",
      "train loss 0.46194426400162897 l2 0.6816304311841421 mse 0.46194426400162897\n",
      "train loss 0.4621630509942835 l2 0.6818885052152778 mse 0.4621630509942835\n",
      "train loss 0.461817108422047 l2 0.6821466379756345 mse 0.461817108422047\n",
      "train loss 0.461652402315737 l2 0.6824121465971913 mse 0.461652402315737\n",
      "train loss 0.46269085280899885 l2 0.6826920481274414 mse 0.46269085280899885\n",
      "train loss 0.4633341469686768 l2 0.6829880500004254 mse 0.4633341469686768\n",
      "train loss 0.46536423327128773 l2 0.6833032561327802 mse 0.46536423327128773\n",
      "train loss 0.46361030043964685 l2 0.6836359478156978 mse 0.46361030043964685\n",
      "train loss 0.46681359201352035 l2 0.6839801956018575 mse 0.46681359201352035\n",
      "train loss 0.46450971813069236 l2 0.6843309034893452 mse 0.46450971813069236\n",
      "train loss 0.4630260733331911 l2 0.6846832661494671 mse 0.4630260733331911\n",
      "train loss 0.46112330518689476 l2 0.6850332953412358 mse 0.46112330518689476\n",
      "val loss 0.5352442874297804\n",
      "val acc 0.71448671696294 val auc 0.7896052438115343\n",
      "epoch 114\n",
      "training\n",
      "train loss 0.46063247269033963 l2 0.6853794427580364 mse 0.46063247269033963\n",
      "train loss 0.4603014562625803 l2 0.6857227952677315 mse 0.4603014562625803\n",
      "train loss 0.46371198659794016 l2 0.6860622441711482 mse 0.46371198659794016\n",
      "train loss 0.46601633422190686 l2 0.6863944769724816 mse 0.46601633422190686\n",
      "train loss 0.4673276566104989 l2 0.6867178633248218 mse 0.4673276566104989\n",
      "train loss 0.46983901773762393 l2 0.6870225759556867 mse 0.46983901773762393\n",
      "train loss 0.46973829433580344 l2 0.6873014138764203 mse 0.46973829433580344\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c750a4ed07c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m                      y_true: b_ys}\n\u001b[1;32m     59\u001b[0m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_loss_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mce_loss_term\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_lossval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_lossval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;31m#         print(\"pred_ys\", pred_y_val, \"true_ys\", b_ys[:5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"l2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_lossval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_lossval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# running the model\n",
    "\n",
    "try:\n",
    "    sess\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    sess.close()\n",
    "\n",
    "sess = tf.Session() # we don't want to close the session\n",
    "\n",
    "learning_rate=.001\n",
    "epochs=150\n",
    "l2_loss_term = .001 * sum([tf.reduce_sum(tf.reshape(weight*weight, [-1])) for weight in all_weights])\n",
    "mse_loss_term = tf.reduce_mean(tf.squared_difference(pred_y, y_true))\n",
    "ce_loss_term = -(tf.reduce_mean(((y_true+1)/2)*tf.math.log((pred_y+1)/2)+(1-(y_true+1)/2)*tf.math.log(1-(pred_y+1)/2)))\n",
    "# ce_loss_term = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=l3))\n",
    "loss = ce_loss_term\n",
    "# + l2_loss_term\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "flat_val_ys = [(1 if y[0] > y[1] else 0) for y in val_ys]\n",
    "\n",
    "min_acc_auc = (math.inf, math.inf)\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "val_accs = []\n",
    "val_aucs = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# tensorboard doesn't work with tensorflow 1 anymore... \n",
    "# ill fucking switch to pytorch before touching tf2\n",
    "# current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "# test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "# train_file_writer = tf.summary.FileWriter(train_log_dir, sess.graph)\n",
    "# test_file_writer = tf.summary.FileWriter(test_log_dir, sess.graph)\n",
    "\n",
    "with open(\"prev_run_stats.pickle\", \"rb\") as prev_run_file:\n",
    "    _, prev_run_scores, _, prev_best_auc_run = pickle.load(prev_run_file)\n",
    "    _, prev_best_aucs = prev_run_scores[prev_best_auc_run]\n",
    "\n",
    "cur_best_auc = 0\n",
    "cur_best_train_loss = math.inf\n",
    "\n",
    "for epoch in tqdm(range(epochs), leave=False):\n",
    "    print(\"epoch\", epoch)\n",
    "    print(\"training\")\n",
    "    for b_user_Xs, b_movie_Xs, b_genres, b_ys in batchify(user_Xs, movie_Xs, train_genres, ys, batch_size=746661, shuffle=False):\n",
    "        feed_dict = {user_slice_idxs: b_user_Xs, \n",
    "                     movie_slice_idxs: b_movie_Xs, \n",
    "                     movie_genre_embeddings: b_genres,\n",
    "#                      tags: b_tags,\n",
    "                     y_true: b_ys}\n",
    "        outs = (train_step, loss, l2_loss_term, ce_loss_term)\n",
    "        _, lossval, l2_lossval, mse_lossval = sess.run(outs, feed_dict=feed_dict)\n",
    "#         print(\"pred_ys\", pred_y_val, \"true_ys\", b_ys[:5])\n",
    "        print(\"train loss\", lossval, \"l2\", l2_lossval, \"mse\", mse_lossval)\n",
    "\n",
    "#         with train_file_writer as writer:\n",
    "#             writer.add_summary(tf.summary.scalar(\"loss\", loss))\n",
    "\n",
    "    feed_dict = {user_slice_idxs: user_val_Xs,\n",
    "                movie_slice_idxs: movie_val_Xs,\n",
    "                movie_genre_embeddings: val_genres,\n",
    "#                 tags: val_tags,\n",
    "                y_true: val_ys}\n",
    "    val_y_pred, val_loss_val = sess.run((pred_y, loss), feed_dict=feed_dict)\n",
    "    flat_pred_y_floats = [y[0]/(y[0]+y[1]) for y in val_y_pred]\n",
    "    flat_pred_y_bools = [(1 if y[0] > y[1] else 0) for y in val_y_pred]\n",
    "\n",
    "    print(\"val loss\", val_loss_val)\n",
    "    acc = metrics.accuracy_score(flat_val_ys, flat_pred_y_bools)\n",
    "    fpr, tpr, _ = metrics.roc_curve(flat_val_ys, flat_pred_y_floats)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(\"val acc\", acc, \"val auc\", auc)\n",
    "    min_acc_auc = min(min_acc_auc, (acc, auc))\n",
    "    val_accs.append(acc)\n",
    "    val_aucs.append(auc)\n",
    "    val_losses.append(val_loss_val)\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.title(\"val aucs per epoch\")\n",
    "    plt.grid(b=True)\n",
    "    plt.minorticks_on()\n",
    "    plt.plot(prev_best_aucs, marker='o', color=\"grey\")\n",
    "    plt.plot(val_aucs, marker='o', color=\"black\")\n",
    "    plt.savefig(\"cur_val_aucs.png\")\n",
    "    \n",
    "#     train_loss = sess.run((loss), feed_dict={user_slice_idxs: user_Xs, \n",
    "#                      movie_slice_idxs: movie_Xs, \n",
    "#                      movie_genre_embeddings: train_genres,\n",
    "#                      y_true: ys})\n",
    "    train_loss = lossval\n",
    "    train_losses.append(train_loss)\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(\"losses per epoch\")\n",
    "    ax.grid(b=True)\n",
    "    ax.minorticks_on()\n",
    "    train_losses_plot = ax.plot(train_losses, marker='o', color=\"blue\", label=\"train losses\")\n",
    "    val_losses_plot = ax.plot(val_losses, marker='o', color=\"green\", label=\"val losses\")\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.savefig(\"cur_train_loss.png\")\n",
    "\n",
    "    if auc > cur_best_auc:\n",
    "        print(f\"NEW BEST AUC: {auc} @ epoch {epoch}\")\n",
    "    if train_loss < cur_best_train_loss:\n",
    "        print(f\"NEW BEST TRAIN LOSS: {train_loss} @ epoch {epoch}\")\n",
    "    \n",
    "    cur_best_auc = max(cur_best_auc, auc)\n",
    "    cur_best_train_loss = min(train_loss, cur_best_train_loss)\n",
    "    if auc < cur_best_auc * .9 and epoch > 10:\n",
    "        print(\"detecting large amounts of overfitting - stopping to avoid boiling oceans\")\n",
    "        break\n",
    "\n",
    "#     with test_file_writer as writer:\n",
    "#         writer.add_summary(value=[tf.summary.Value(tag=\"loss\", simple_value=val_loss_val)])\n",
    "#         writer.add_summary(tf.summary.scalar(\"loss\", loss))\n",
    "#         writer.add_summary(value=[tf.summary.Value(tag=\"acc\", simple_value=acc)])\n",
    "#         writer.add_summary(value=[tf.summary.Value(tag=\"auc\", simple_value=auc)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-16aa2bfdb624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mmovie_slice_idxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb_movie_Xs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 movie_genre_embeddings: b_genres}\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtest_y_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtest_y_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_y_preds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# save test prediction as file\n",
    "\n",
    "test_y_preds = []\n",
    "\n",
    "for b_user_Xs, b_movie_Xs, b_genres in batchify(user_test_Xs, movie_test_Xs, test_genres, batch_size=497774, shuffle=False):\n",
    "    feed_dict = {user_slice_idxs: b_user_Xs,\n",
    "                movie_slice_idxs: b_movie_Xs,\n",
    "                movie_genre_embeddings: b_genres}\n",
    "    test_y_preds.extend(sess.run(pred_y, feed_dict=feed_dict))\n",
    "    \n",
    "test_y_preds = [pred[0]/(pred[0]+pred[1]) for pred in test_y_preds]\n",
    "\n",
    "df = pandas.DataFrame(test_y_preds, columns=['rating'])\n",
    "\n",
    "with open(\"test_preds.csv\", \"w+\") as test_file:\n",
    "    test_file.write(\"Id\"+df.to_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.95304582]\n",
      " [ 2.          0.38410283]\n",
      " [ 3.          0.9537229 ]\n",
      " [ 4.          0.96523868]\n",
      " [ 5.          0.57257932]\n",
      " [ 6.          0.77299195]\n",
      " [ 7.          0.94740396]\n",
      " [ 8.          0.90670397]\n",
      " [ 9.          0.71593777]\n",
      " [10.          0.81709064]]\n"
     ]
    }
   ],
   "source": [
    "print(test_y_preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best run: 31\n"
     ]
    }
   ],
   "source": [
    "# im reduced to manually tensorboarding like a loser cause tf2 api is being a dick not my fault\n",
    "\n",
    "with open(\"prev_run_stats.pickle\", \"rb\") as prev_run_file:\n",
    "    cur_run, run_scores, best_auc, best_auc_run = pickle.load(prev_run_file)\n",
    "\n",
    "run_scores[cur_run] = (val_accs, val_aucs)\n",
    "if max(val_aucs) > best_auc:\n",
    "    best_auc, best_auc_run = val_aucs[-1], cur_run\n",
    "\n",
    "cur_run += 1\n",
    "best_accs, best_aucs = run_scores[best_auc_run]\n",
    "\n",
    "with open(\"prev_run_stats.pickle\", \"wb+\") as prev_run_file:\n",
    "    pickle.dump((cur_run, run_scores, best_auc, best_auc_run), prev_run_file)\n",
    "\n",
    "plt.title(\"val accs per epoch\")\n",
    "plt.plot(best_accs, marker='o', color=\"grey\")\n",
    "plt.plot(val_accs, marker='o', color=\"black\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"val aucs per epoch\")\n",
    "plt.plot(best_aucs, marker='o', color=\"grey\")\n",
    "plt.plot(val_aucs, marker='o', color=\"black\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"best run: {best_auc_run}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(flat_val_ys, flat_pred_y_floats)\n",
    "\n",
    "plt.title(\"roc curve\")\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction acc by user and by movie\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "user_correct_ct = defaultdict(lambda: 0)\n",
    "user_total_ct = defaultdict(lambda: 0)\n",
    "movie_correct_ct = defaultdict(lambda: 0)\n",
    "movie_total_ct = defaultdict(lambda: 0)\n",
    "for uid, mid, y_true, pred_y in tqdm(zip(user_val_Xs, movie_val_Xs, val_ys, val_y_pred)):\n",
    "    uid = uid[0]\n",
    "    mid = mid[0]\n",
    "    if (y_true[0] > y_true[1]) == (pred_y[0] > pred_y[1]):\n",
    "        user_correct_ct[uid] += 1\n",
    "        movie_correct_ct[mid] += 1\n",
    "    user_total_ct[uid] += 1\n",
    "    movie_total_ct[mid] += 1\n",
    "\n",
    "user_accs = defaultdict(lambda: -1)\n",
    "movie_accs = defaultdict(lambda: -1)\n",
    "for uid, total in tqdm(user_total_ct.items()):\n",
    "    if total > 100:\n",
    "        correct_ct = user_correct_ct[uid]\n",
    "        user_accs[uid] = correct_ct/total\n",
    "\n",
    "for mid, total in tqdm(movie_total_ct.items()):\n",
    "    if total > 100:\n",
    "        correct_ct = movie_correct_ct[mid]\n",
    "        movie_accs[mid] = correct_ct/total\n",
    "\n",
    "plt.title(\"prediction acc histogram by user when number of train examples > 100\")\n",
    "plt.hist(user_accs.values(), bins=100)\n",
    "plt.xlabel(\"acc\")\n",
    "plt.ylabel(\"# of users w that acc\")\n",
    "plt.show()\n",
    "plt.title(\"prediction acc histogram by movie when number of train examples > 100\")\n",
    "plt.hist(movie_accs.values(), bins=100)\n",
    "plt.xlabel(\"acc\")\n",
    "plt.ylabel(\"# of movies w that acc\")\n",
    "plt.show()\n",
    "\n",
    "user_accs = defaultdict(lambda: -1)\n",
    "movie_accs = defaultdict(lambda: -1)\n",
    "for uid, total in tqdm(user_total_ct.items()):\n",
    "    correct_ct = user_correct_ct[uid]\n",
    "    user_accs[uid] = correct_ct/total\n",
    "\n",
    "for mid, total in tqdm(movie_total_ct.items()):\n",
    "    correct_ct = movie_correct_ct[mid]\n",
    "    movie_accs[mid] = correct_ct/total\n",
    "\n",
    "plt.title(\"prediction acc histogram by user\")\n",
    "plt.hist(user_accs.values(), bins=100)\n",
    "plt.xlabel(\"acc\")\n",
    "plt.ylabel(\"# of users w that acc\")\n",
    "plt.show()\n",
    "plt.title(\"prediction acc histogram by movie\")\n",
    "plt.hist(movie_accs.values(), bins=100)\n",
    "plt.xlabel(\"acc\")\n",
    "plt.ylabel(\"# of movies w that acc\")\n",
    "plt.show()\n",
    "\n",
    "user_train_examples_ct = defaultdict(lambda: 0)\n",
    "movie_train_examples_ct = defaultdict(lambda: 0)\n",
    "\n",
    "for uid, mid in zip(user_Xs, movie_Xs):\n",
    "    user_train_examples_ct[uid[0]] += 1\n",
    "    movie_train_examples_ct[mid[0]] += 1\n",
    "\n",
    "user_acc_examples = np.array([(user_accs[uid], ct) for uid, ct in user_train_examples_ct.items()])\n",
    "movie_acc_examples = np.array([(acc, movie_train_examples_ct[mid]) for mid, acc in movie_accs.items()])\n",
    "plt.title(\"user acc against number of examples\")\n",
    "plt.scatter(user_acc_examples[:, 0], user_acc_examples[:, 1], c=[[0, 0, 1, .05]], s=1)\n",
    "plt.xlabel(\"acc\")\n",
    "plt.ylabel(\"number of train examples seen for that user\")\n",
    "plt.show()\n",
    "plt.title(\"movie acc against number of examples\")\n",
    "plt.scatter(movie_acc_examples[:, 0], movie_acc_examples[:, 1], c=[[0, 0, 1, .05]], s=1)\n",
    "plt.xlabel(\"acc\")\n",
    "plt.ylabel(\"number of train examples seen for that movie\")\n",
    "plt.show()\n",
    "\n",
    "# conclusion: highest acc gains can be made by tailoring movie suggestions to specific users with low information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f18727beb24fb98e701a5c14277878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prediction acc by genre\n",
      "0 0.702662007725544\n",
      "2 0.7169578622816033\n",
      "9 0.7200113433857658\n",
      "1 0.717148152534723\n",
      "8 0.7134642711963338\n",
      "11 0.7094245842095201\n",
      "13 0.7104643921159381\n",
      "3 0.7040078741330127\n",
      "4 0.7261100807331442\n",
      "5 0.7160645674624975\n",
      "10 0.7009188973232121\n",
      "17 0.7186280374896733\n",
      "6 0.7302657182568882\n",
      "14 0.6779070352300407\n",
      "15 0.6906036718316583\n",
      "12 0.7149278551944809\n",
      "16 0.7106273123861063\n",
      "19 0.7158106637649619\n",
      "7 0.6750991127053049\n"
     ]
    }
   ],
   "source": [
    "# prediction acc by genre\n",
    "\n",
    "genre_correct_ct = defaultdict(lambda: 0)\n",
    "genre_total_ct = defaultdict(lambda: 0)\n",
    "\n",
    "movie_genres_one_hot = get_movie_genres_one_hot()\n",
    "for uid, mid, y_true, pred_y in tqdm(zip(user_val_Xs, movie_val_Xs, val_ys, val_y_pred)):\n",
    "    genres = movie_genres_one_hot[mid[0]]\n",
    "    correct = (y_true[0] > y_true[1]) == (pred_y[0] > pred_y[1])\n",
    "    for i, genre in enumerate(genres):\n",
    "        if genre == 1:\n",
    "            genre_correct_ct[i] += correct\n",
    "            genre_total_ct[i] += 1\n",
    "\n",
    "print(\"prediction acc by genre\")\n",
    "for genre_id, total in genre_total_ct.items():\n",
    "    correct_ct = genre_correct_ct[genre_id]\n",
    "    print(genre_id, correct_ct/total)\n",
    "    \n",
    "# conclusion: genre can't really be mined for more information i think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rc('figure', figsize=(10, 5))\n",
    "plt.title(\"movie_embedding_rows\")\n",
    "plt.hist(sess.run(movie_embedding_rows, feed_dict).flatten(), log=True, bins=200)\n",
    "plt.show()\n",
    "plt.title(\"user_embedding_columns\")\n",
    "plt.hist(sess.run(user_embedding_columns, feed_dict).flatten(), log=True, bins=200)\n",
    "plt.show()\n",
    "plt.title(\"user_slice_bias\")\n",
    "plt.hist(sess.run(user_slice_bias, feed_dict).flatten(), log=True, bins=400)\n",
    "plt.show()\n",
    "plt.title(\"movie_slice_bias\")\n",
    "plt.hist(sess.run(movie_slice_bias, feed_dict).flatten(), log=True, bins=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode1 = np.random.normal(0, 1, 5000000)\n",
    "mode2 = np.random.normal(-2, 1, 1000000)\n",
    "\n",
    "plt.hist(np.concatenate((mode1, mode2)), bins=100, log=True)\n",
    "plt.show()\n",
    "plt.hist(mode1, bins=100, log=True, color=\"blue\")\n",
    "plt.hist(mode2, bins=100, log=True, color=\"blue\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg pos -0.00021177795315711342 pos ct 76948440\n",
      "avg neg -4.119220633618965e-05 neg ct 83021000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mult_inputs = sess.run(mult_input, feed_dict)\n",
    "plt.cla()\n",
    "plt.title(\"mult_inputs\")\n",
    "plt.hist(mult_inputs.flatten(), log=True, bins=200)\n",
    "plt.show()\n",
    "\n",
    "pred_ys = sess.run(pred_y, feed_dict)\n",
    "\n",
    "mult_input_pos = np.array([mult_input for mult_input, pred in zip(mult_inputs, pred_ys) if pred[0] > pred[1]]).flatten()\n",
    "if len(mult_input_pos) > 0:\n",
    "    print(\"avg pos\", sum(mult_input_pos)/len(mult_input_pos), \"pos ct\", len(mult_input_pos))\n",
    "else:\n",
    "    print(\"mult_input_pos\", mult_input_pos)\n",
    "\n",
    "mult_input_neg = np.array([mult_input for mult_input, pred in zip(mult_inputs, pred_ys) if pred[0] <= pred[1]]).flatten()\n",
    "if len(mult_input_neg) > 0:\n",
    "    print(\"avg neg\", sum(mult_input_neg)/len(mult_input_neg), \"neg ct\", len(mult_input_neg))\n",
    "else:\n",
    "    print(\"mult_input_neg\", mult_input_neg)\n",
    "\n",
    "plt.title(\"pos inputs\")\n",
    "plt.hist(mult_input_pos, bins=200, log=True)\n",
    "plt.show()\n",
    "plt.title(\"neg inputs\")\n",
    "plt.hist(mult_input_neg, bins=200, log=True)\n",
    "plt.show()\n",
    "\n",
    "# makes sense why these are basically chi square - composed of normal distributions multiplied together which is fundamentally what we're predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting experiment G94UT6SBAV\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27a307f251244c6864e61212952acfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "training\n",
      "train loss 0.5652496744476239 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5645683836622711 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5639976734065467 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5634156152095556 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.562884268153996 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5624629478989001 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5620419854397721 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5616974455530761 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5613117671757857 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5610029462080675 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5606925246301309 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5603665157392352 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5600916734623116 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5597812193903371 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5594610569189096 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5591582726808294 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5588368662442036 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5585256445974586 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5581156992466338 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5577236988657451 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5573475138280113 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5569212281352489 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "epoch 1\n",
      "training\n",
      "train loss 0.556382094041375 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5558206833469964 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5552807640983052 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5547726336970172 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5541343055457456 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.553454982432047 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5526585367846429 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5519967338786348 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.551194110776163 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5503299906793867 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5493915840359626 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5484642760561459 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5474837131772019 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5463866590243075 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5453381531557873 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5441169518778542 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5430454421274722 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5417702366774877 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5404598067977336 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5392389777074315 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.537896299107213 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5365935145263971 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "epoch 2\n",
      "training\n",
      "train loss 0.5348134324119377 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5334914411386613 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5321627973282721 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5307535838793241 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5296523464456772 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5281363281240897 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5269809524081664 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5255376926500751 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5245122521873913 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.523258064847023 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5223076851356953 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5211258082043714 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5204479740532044 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5195721883673475 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5188736072101773 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5179893224255597 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5174855268671671 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5166319312856353 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5164051067270659 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5158975267887225 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5156413779526821 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5148966724613775 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "epoch 3\n",
      "training\n",
      "train loss 0.5136548256730052 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5135563313055878 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5133011671560075 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5129161045991032 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5125522511384317 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5125771842480302 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.512448576235478 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5125306017076541 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5117379885888602 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5121037618294532 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5119984037566767 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5117788341428439 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5117344043367146 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5117235630899537 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.511775463993731 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5117171527206767 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5117227205691038 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5117428911428754 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.511511312261437 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5116446855004987 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5113984282448544 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5114207051356198 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "epoch 4\n",
      "training\n",
      "train loss 0.5097899956799322 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5095402817234577 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5099864741538408 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5102050659036648 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5099128784345802 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5096815537911655 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5096955811479116 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5094443241403631 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5099221113390562 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5098332692233164 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5100653270995018 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5095898775563437 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5098914775490199 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5101117668629944 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5102032212768619 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5098312317361997 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5102109820862177 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5101012503648057 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5101884793947378 l2 0.6873014138764203 mse 0.46973829433580344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.5099733154557734 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5102761891195206 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5099971154841529 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "epoch 5\n",
      "training\n",
      "train loss 0.5088965219744471 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5091012060522337 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5089128955257888 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5088783253670154 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.509119952127764 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5092507865657447 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5091232345399823 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5091184741168282 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.508666103407496 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5087633432025266 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5089980254153769 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.509201942924327 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5090462008075779 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5089723167160047 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5093530634086585 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5091075334992797 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5093164427943121 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5093780980098711 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5094897773995163 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5095508649068252 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5096259296641161 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5094529266130453 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "epoch 6\n",
      "training\n",
      "train loss 0.5081472680849336 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5080412465181443 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5079984900899939 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5083674071194277 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5081548674332236 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5081650307818755 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5084021792163611 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5084277798031848 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5083851994111416 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5086497126010281 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5083775280339908 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5086102996216235 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5089154013289447 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5087834268257736 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5088802805780246 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5087655671285051 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5091816305095167 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5088654944863591 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5087406740930802 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5089328412179043 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5089018161717298 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.509169186404763 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "saving candidate submission models/G94UT6SBAV/expr_G94UT6SBAV_epoch_6_loss_0.5076849535388877\n",
      "epoch 7\n",
      "training\n",
      "train loss 0.5077443135887607 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5077132349326542 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5074134932580274 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5077730098482953 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5076942393888638 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5076530698523962 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5076965420568511 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5078147517054767 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5080268910747634 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5078820303844214 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.50809408847956 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5077103222771133 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5079274245669811 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5078332454099482 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5079823966838822 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5079980255878573 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5081928108942886 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5081628114235223 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5085450090131303 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5084869937302214 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5081887048419539 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5080663293047786 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "epoch 8\n",
      "training\n",
      "train loss 0.5069936256143395 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5070536936208896 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5068708778856327 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5069165626811486 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5069779311645287 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5069998114921559 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5068858003209887 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5069684483090438 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5069769375354646 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5069438218007004 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5070975452466607 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5073840188956578 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5074948412954478 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5072258611360156 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5072189874846245 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5072152787240842 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5070436974862259 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5071784884286759 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5074673443505434 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.507216966249199 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5074740118933603 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5070557442960398 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "saving candidate submission models/G94UT6SBAV/expr_G94UT6SBAV_epoch_8_loss_0.5060328698756867\n",
      "epoch 9\n",
      "training\n",
      "train loss 0.5058846432760276 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5061585883848566 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5059708752278295 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5057731728161906 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5061873551693806 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.506049458016582 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5060152747242218 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5059705971672164 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5058846435334897 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5060958928576264 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5058745577957072 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5059787904278656 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5064316256206041 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5062246809186687 l2 0.6873014138764203 mse 0.46973829433580344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.5061988019417627 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5061104061981568 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5065108602642528 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5062505857842489 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5060461886961702 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5060452392094505 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5062024261540282 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5062484283234155 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "epoch 10\n",
      "training\n",
      "train loss 0.5048287423775631 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5045904797438518 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5048276509886602 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5046158987921545 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5048921774791585 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.504536139828211 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.504803745681926 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5046810603705448 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5048853635207571 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.504871572691587 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5044680775901139 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5051148530341127 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5049101330666993 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5051423467012219 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5046968946752797 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5051659834157255 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5051029692428417 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.504781610005864 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5053015542015435 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5053450229096031 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5051790906886936 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5051647229313533 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "saving candidate submission models/G94UT6SBAV/expr_G94UT6SBAV_epoch_10_loss_0.5036118536558655\n",
      "epoch 11\n",
      "training\n",
      "train loss 0.5039811660735426 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5033379588795677 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5038830167608551 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5033001735051206 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.503262774996592 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5034545431872041 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5034678966864733 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5037291965870374 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5034436225290265 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5038211965393465 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.503655619578 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5034521057917729 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.503703868880149 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5039914357665234 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5035138762536946 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5041609208740914 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.503570187710674 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5038717006257897 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5038113170484408 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5041532811751196 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.504021223501517 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5044444119037 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "epoch 12\n",
      "training\n",
      "train loss 0.5021515931655899 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5024648840356435 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5022181640599974 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5022246712884962 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5020016449546152 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5022983262590747 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5026208620543302 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.502534195802729 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5029113970505821 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5024159457256667 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5025461408046731 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5028697700272061 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5023833285299266 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5030575849484115 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5025761276573809 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5027837176574971 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5028285987891014 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5029800426638549 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5029120857837812 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5031174409090713 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5029294223243123 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5033414636257194 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "saving candidate submission models/G94UT6SBAV/expr_G94UT6SBAV_epoch_12_loss_0.5013563950523876\n",
      "epoch 13\n",
      "training\n",
      "train loss 0.5011370225680818 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5012499772175862 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5010469386829409 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5012678264312419 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5013676115874166 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5013499738975681 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5015005221260671 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5015157073827243 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5016172132675748 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5015086539763486 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5015196026143274 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5015790656222983 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.501564781625653 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5017974535314611 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5019506381924463 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5018034502563162 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5013791068426328 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5017459769112449 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.501683646292397 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5021156188749194 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5017200270292118 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5024673721366998 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "epoch 14\n",
      "training\n",
      "train loss 0.5003080048410301 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5003491884315757 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4998891718823501 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5002970613660865 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49973848460283465 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5002182969594984 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5002608705230649 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5005720575575335 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.500419117331063 l2 0.6873014138764203 mse 0.46973829433580344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.5002768072320964 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5004758000716356 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5001123461581831 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5004395769561903 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5005620860594073 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5001717322483924 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5002633128266549 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5004821438981447 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5006914569459706 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5005743904244184 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5009638752093297 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5007665468851618 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.5003548384201457 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "saving candidate submission models/G94UT6SBAV/expr_G94UT6SBAV_epoch_14_loss_0.4988543883873585\n",
      "epoch 15\n",
      "training\n",
      "train loss 0.4988302284693181 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49878947077003827 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49868505318690887 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49847552227460823 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4989386762056199 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49866456325007846 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49914277497208237 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4988905685637609 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49902531653100807 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49880210608737746 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4987884069049608 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4992297745769065 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4985900067221675 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4991085946617076 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.499263525615611 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49934484135717094 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.499166076055403 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4993477145275925 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4990953717855934 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4993194834316485 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4994647160763349 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49908163313292847 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "epoch 16\n",
      "training\n",
      "train loss 0.4970918658025064 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49717699040619145 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49729080583265767 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4970681593447567 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49700745545161396 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49732737254079507 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4971718925951739 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.497364441565901 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4973496157440099 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4975241733202001 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4974500151769243 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49755526988978616 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4973511808899766 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4978601102272123 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4977033316491442 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49771801247167996 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49770287999840146 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4978846507279103 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49781398258342946 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49837172632121884 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49796826015932516 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4982357233731612 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "saving candidate submission models/G94UT6SBAV/expr_G94UT6SBAV_epoch_16_loss_0.49573730880910716\n",
      "epoch 17\n",
      "training\n",
      "train loss 0.49564291657329423 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4957787134775602 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4958238887755365 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.495697326916006 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.495504478852414 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4953760253399036 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4955397224324444 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4956382072998548 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49570037301937575 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49606622071048995 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4958664902572763 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49590755012086 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4962799485432634 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4962284399848524 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4963287670932832 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49639942762829464 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4965877516423021 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4964426489562271 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4965230996596003 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4966386136515536 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49663270292188594 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49649341225989974 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "epoch 18\n",
      "training\n",
      "train loss 0.49427708272247023 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49430721065357996 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4942767076132678 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49383923013519926 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4941960498129813 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4941872607524089 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49393118712471756 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49451303554572756 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49423939647310144 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49393205429245934 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4943191936184444 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49459596262930267 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4945437240067326 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49492191095187466 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49503860623841317 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49488891224909093 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4951087170206533 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49509303205871164 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.494965883037243 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49498770319472485 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4954030819961233 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49546582374157866 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "saving candidate submission models/G94UT6SBAV/expr_G94UT6SBAV_epoch_18_loss_0.4926051790588196\n",
      "epoch 19\n",
      "training\n",
      "train loss 0.4927356598770672 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4924138486835047 l2 0.6873014138764203 mse 0.46973829433580344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.4923863834665383 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49266329680010673 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4924967684503267 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49261928604561045 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49289384685744686 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4925060158135342 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49277731387709917 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49274431840934824 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4931087834035087 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4932377708959901 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49318215358081213 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4932751312357793 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4933480159076786 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4935710160792167 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49362879012841254 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4938306512922868 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49361725539198076 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4937640339281155 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4942928756895755 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4939635989231381 l2 0.6873014138764203 mse 0.46973829433580344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucastong/.local/lib/python3.6/site-packages/ipykernel_launcher.py:66: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20\n",
      "training\n",
      "train loss 0.4909236478615265 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49100017374321353 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4909031857889557 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4910086307999917 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4912545516413849 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4909897844055323 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49103928189992135 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.491049829748903 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.491528153467418 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49143725000298993 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4916967065683325 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49157199674892227 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4914173848336199 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4917020792636721 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4919154208179011 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4922360886567206 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4924794664375289 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4926033040490476 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49256484545736134 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49302879249078563 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49285959439333715 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49284276570532587 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "saving candidate submission models/G94UT6SBAV/expr_G94UT6SBAV_epoch_20_loss_0.48962632053966704\n",
      "epoch 21\n",
      "training\n",
      "train loss 0.4897813931454053 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.48965238900666097 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4896694974185051 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4896325092224946 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.48944663992133297 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.48942504676574466 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4895029283100604 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.48988139321825647 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4897438806813638 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49030806668800614 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49047271673735743 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49052635778395476 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49031148751322784 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49037597680649464 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49057140579539954 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4910757282462098 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4908326047399488 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4912255967998204 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49124857464231253 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49145857847354163 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4914648617269593 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.491569073352568 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "epoch 22\n",
      "training\n",
      "train loss 0.4882235974478427 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4881931441856254 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.48816020192463816 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4878470487478012 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.48836829973693335 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4882920893273898 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4885340951894984 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4884915335844535 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.48856207038321137 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.488876672815638 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.48851142127846475 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4886099412477493 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4895032699894514 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4894310738324906 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.48939861650807487 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.48948016860412263 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.48951388603045676 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4898596442640073 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.4902731079843683 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49009386884027334 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.490344220075089 l2 0.6873014138764203 mse 0.46973829433580344\n",
      "train loss 0.49010300376348576 l2 0.6873014138764203 mse 0.46973829433580344\n"
     ]
    }
   ],
   "source": [
    "# script to run the model on training and validation data to submit\n",
    "# running the model\n",
    "import string\n",
    "\n",
    "try:\n",
    "    sess\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    sess.close()\n",
    "\n",
    "sess = tf.Session() # we don't want to close the session\n",
    "\n",
    "expr_num = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(10))\n",
    "\n",
    "print(f\"starting experiment {expr_num}\")\n",
    "expr_path = Path(\"models\")/expr_num\n",
    "os.makedirs(expr_path, exist_ok=False)\n",
    "\n",
    "learning_rate=.001\n",
    "epochs=50\n",
    "# save_epochs = {70, 72, 74, 76, 78, 90, 100, 110}\n",
    "\n",
    "l2_loss_term = .001 * sum([tf.reduce_sum(tf.reshape(weight*weight, [-1])) for weight in all_weights])\n",
    "mse_loss_term = tf.reduce_mean(tf.squared_difference(pred_y, y_true))\n",
    "ce_loss_term = -(tf.reduce_mean(((y_true+1)/2)*tf.math.log((pred_y+1)/2)+(1-(y_true+1)/2)*tf.math.log(1-(pred_y+1)/2)))\n",
    "# ce_loss_term = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=l3))\n",
    "loss = ce_loss_term\n",
    "# + l2_loss_term\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "cur_best_auc = 0\n",
    "\n",
    "min_loss = math.inf\n",
    "train_losses = []\n",
    "for epoch in tqdm(range(epochs), leave=False):\n",
    "    print(\"epoch\", epoch)\n",
    "    print(\"training\")\n",
    "    arg_len = len(user_val_Xs), len(user_Xs)\n",
    "    \n",
    "    \n",
    "    train_val_user_Xs = np.concatenate([user_val_Xs, user_Xs], axis=0)\n",
    "    train_val_movie_Xs = np.concatenate([movie_val_Xs, movie_Xs], axis=0)\n",
    "    train_val_genres = np.concatenate([val_genres, train_genres], axis=0)\n",
    "    train_val_ys = np.concatenate([val_ys, ys], axis=0)\n",
    "    for b_user_Xs, b_movie_Xs, b_genres, b_ys in batchify(train_val_user_Xs, train_val_movie_Xs, train_val_genres, train_val_ys, batch_size=746661, shuffle=True):\n",
    "        feed_dict = {user_slice_idxs: b_user_Xs, \n",
    "                     movie_slice_idxs: b_movie_Xs, \n",
    "                     movie_genre_embeddings: b_genres,\n",
    "                     y_true: b_ys}\n",
    "        outs = (train_step, loss)\n",
    "        _, lossval = sess.run(outs, feed_dict=feed_dict)\n",
    "        print(\"train loss\", lossval, \"l2\", l2_lossval, \"mse\", mse_lossval)\n",
    "\n",
    "    train_loss = sess.run(loss, feed_dict={user_slice_idxs: train_val_user_Xs, \n",
    "                     movie_slice_idxs: train_val_movie_Xs, \n",
    "                     movie_genre_embeddings: train_val_genres,\n",
    "                     y_true: train_val_ys})\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(\"losses per epoch\")\n",
    "    ax.grid(b=True)\n",
    "    ax.minorticks_on()\n",
    "    train_losses_plot = ax.plot(train_losses, marker='o', color=\"blue\", label=\"train losses\")\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.savefig(\"cur_train_loss.png\")\n",
    "\n",
    "    \n",
    "    if epoch > 5 and epoch % 2 == 0:\n",
    "        cur_submission_num += 1\n",
    "        save_path = expr_path/f\"expr_{expr_num}_epoch_{epoch}_loss_{train_loss}\"\n",
    "        print(f\"saving candidate submission {save_path}\")\n",
    "\n",
    "        os.makedirs(save_path/\"model\", exist_ok=True)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, str(save_path/\"model\"/\"model.ckpt\"))\n",
    "\n",
    "        test_y_preds = []\n",
    "\n",
    "        for b_user_Xs, b_movie_Xs, b_genres in batchify(user_test_Xs, movie_test_Xs, test_genres, batch_size=497774, shuffle=False):\n",
    "            feed_dict = {user_slice_idxs: b_user_Xs,\n",
    "                        movie_slice_idxs: b_movie_Xs,\n",
    "                        movie_genre_embeddings: b_genres}\n",
    "            test_y_preds.extend(sess.run(pred_y, feed_dict=feed_dict))\n",
    "\n",
    "        test_y_preds = [pred[0]/(pred[0]+pred[1]) for pred in test_y_preds]\n",
    "\n",
    "        df = pandas.DataFrame(test_y_preds, columns=['rating'])\n",
    "\n",
    "        with open(save_path/\"test_preds.csv\", \"w+\") as test_file:\n",
    "            test_file.write(\"Id\"+df.to_csv())\n",
    "\n",
    "        os.system(f\"cp basic_nn.ipynb {str(save_path)}\")\n",
    "\n",
    "#         with open(\"submission_info.pickle\", \"wb\") as submission_info_file:\n",
    "#             pickle.dump(cur_submission_num, submission_info_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}